Jorda has no life notes:
https://docs.google.com/document/d/1KfyhNgojjp-0fyBFqqV7jaGErPGFfo1BgnuN9_yDIkk/edit

----------------------------------------------------------------

A POP CDN can drop the latency from 300 ms to origin server down to 10 ms to access a resource btw. 

    mongodb also has ttl so you can use that with Redis ttl?



    Is 10k writes per second really huge. Most database can do these easily with 10-12 nodes. Some database write throughput numbers

    MySQL: ~1,000 writes/sec (single node, optimized for writes)
    PostgreSQL: ~500 writes/sec (single node, typical workload)
    MongoDB: ~10,000 - 100,000 writes/sec (depending on configuration)
    Cassandra: ~100,000 - 1,000,000 writes/sec (depending on configuration)
    Redis: ~1,000,000+ writes/sec (in-memory)

    Do we really need to talk about write scaling for this scale?

    tiME SERIES influx db can handle 250k writes per second with 8 cores and 32 gig ram. and 25 queries per second. and 1 million uniqe series. 


Back of the envelope calculations (BOTEC)

You expected something else? Sit down... I thought hard and I think this is something many people secretly 
fear and feel weak on. So since my goal in this effort is to genuinely help people, I think this is where my contribution is most valuable.

You might think why do we even need BOTEC? Short version is to size tiers. What? Size? Tiers? Sizing as a 
verb here means to estimate how many machines/disks/etc. you will need. Tiers in this context means the typical 
tier in any system development. For example, a logging/counting system can have three tiers: 1. the collection tier, 2. 
the aggregation tier and 3. the storage tier.

Another way you can use BOTEC is to figure out if something can fit in one machine or not, mostly memory wise.
 Most of the time it is self-evident, but 10 seconds of BOTEC should confirm it.

Example: How do you serve the timelines of 1 Million people efficiently? Well how many posts do you expect 
to have readily available per person? Let's say 10. Ok. If we store the IDs only (say 64 bit) we can use a redis native list to store list of posts:

8 bytes/post * 10 post/timeline=80 bytes/timeline (post goes away)

1 Million timelines * 80 bytes/timeline = 10^6*10^1*8=8*10^(6+1) bytes (timeline goes away)

so 80 million bytes means 80 MB - easily goes into one machine's RAM.

This was an easy example however note two things: always drag the units with you: bytes/post, post/timeline and reduce when possible. It's extremely helpful to not get lost. Second, always use powers of 10 to not miss a zero here and a zero there.

Here's the real life version of that.

- 64 bit ID per post - 8 bytes/post
- 800 posts/timeline
- 500 million timelines
- replication factor of 3

800 posts / timeline * 8 bytes/post = (post goes away) 8*10^2 * 8 bytes/timeline= 64 * 10^2 bytes/timeline

500 million timelines* 64 * 10^2 bytes/timeline = (timeline goes away) 5 * 10^2 * 10^6 * 64 * 10 ^2 bytes= 5*64 * 10 ^(2+6+2) bytes= 300 * 10^10 = 3 *10^12 bytes = 3 TBs

3TBs * 3 (unitless replication factor) = ±10 TB. Considering 64 GB ram/machine out of which 50 can be considered usable you have 10 TB/(50GB/machine) = 10* 10^12 Bytes /(5*10^9)Bytes/machine= 2 *10^3 =2000 Machines (Bytes goes away, 1 over 1 over machine becomes machine).

The temptation here is to stop being pedantic with units but I suggest you don't. These can get messy so stick to it.

This tier can be considered to be memory-bound.

Look up Raffi Krikorian's Timelines at Scale on infoQ to see him talk about this at more length.

Usually you have different tiers with different scaling mechanisms. Do your calculations early to get a feeling of what is going to be the bounding factor, then plan ahead to scale that accordingly.

Here's an example of a QPS-bound tier. You're told you'll have to log 100 billion events per day. Ok. That sounds like a lot. Let's turn that into per second. Is that a calculator you took out? *takes it and throws out the window*

100 * 10^9 events/day [divided by] 86400 seconds/day

round to a convenient number

100 * 10^9 events/day [divided by] 80000 seconds/day=

10^11/(8*10^4) events/second (1 over day goes away)=1/8 *(10^7)Events/sec= 10/8 Million QPS=~1.2 Million QPS

This looks to be a QPS bound tier. To size it, divide it by some (numerically convenient) number that can be handled by one machine and you get the number of machines.

Same goes for storage sizing. Keep in mind the replication factor in storage and the amount of time you will be storing for. Look up datastax capacity planning for cassandra numbers, they're super useful.

And last but not least here are some numbers I used to reference quite often. They're taken from various sources and I used them without causing dropped jaws, so it should be safe.

- Compress 1KB with Zippy - 0.003 ms
- Send 1KB over 1Gbps 0.01 ms
- Read 4MB of sequential memory is 1 ms
- Read 4MB of sequential SSD is 20 ms
- Read 4MB of sequential HDD is 80 ms
- One single disk seek is 10 ms
- Inter-datacenter roundtrip 150ms
- Inter-datacenter bandwidth ±100 Gbps
- Video is roughly 10 MB/minute
- A server can have 100-150GB of usable RAM
- Cassandra is best used with 1-2TB storage / node
- A 1Gbps link can handle max of 125 MB /s
- Cassandra cluster can handle 10-15000 reqs/s/node and grows linearly with number of nodes
- 720p video - 1.5Mbps
- 340 video - 150Kbps


        TRANSACTIONS PER SECOND:

            For RDBMS, you can suppose the TPS is <1k.
            For the NoSQL DB, it could be more than 10k for single node. And much more for a cluster since most NoSQLs are horizontal scaleble.

            If the query is simple select and you have enough memory allocated to Oracle then your throughput per node can be near 100-1000 QPS.


        MB/s is Megabytes per second. Mbps is Megabits per second. There's 8 bits in a byte.

        1 MB/s = 8Mbps


        To run at 4K, Google advises that you need a connection of 35 Mbps - that's just under 4.5 MB/s.




        1 million requests/day = 12 requests/second

        L1 and L2 caches: 1 ns, 10 ns
        E.g.: They are usually built onto the microprocessor chip. Unless you work with hardware directly, you probably don’t need to worry about them.

        RAM access: 100 ns
        E.g.: It takes around 100 ns to read data from memory. Redis is an in-memory data store, so it takes about 100 ns to read data from Redis.

        Send 1K bytes over 1 Gbps network: 10 us
        E.g.: It takes around 10 us to send 1KB of data from Memcached through the network.

        Read from SSD: 100 us
        E.g.: RocksDB is a disk-based K/V store, so the read latency is around 100 us on SSD.

        Database insert operation: 1 ms.
        E.g.: Postgresql commit might take 1ms. The database needs to store the data, create the index, and flush logs. 
        All these actions take time.

        Send packet CA->Netherlands->CA: 100 ms
        E.g.: If we have a long-distance Zoom call, the latency might be around 100 ms.

        Retry/refresh internal: 1-10s
        E.g: In a monitoring system, the refresh interval is usually set to 5~10 seconds (default value on Grafana).

        The average time for a human to blink is 100 ms, and the average reaction time for a human is around 250 ms. 
        Therefore, the actions performed within 250 ms are perceived as real-time or live 4, 5. An event-driven 
        architecture can be used to build a real-time data platform. The general subscription models for an API are the following 


        • Fast response time: As a user types a search query, autocomplete suggestions must show up fast enough. 
        An article about Facebook’s autocomplete system [1] reveals that the system needs to return results within 100 milliseconds. Otherwise it will cause stuttering.


        latency to calculate load on cpu and qps

Redis, an open source in-memory NoSQL database, can handle a large number of transactions per second (TPS) depending on several factors, including:
        Hardware: The speed of the CPU, amount of available memory, and I/O performance
        Configuration: Optimizing Redis configuration, such as using pipelining to group multiple commands or disabling persistence 
        
        Here are some examples of how many TPS Redis can handle: 
        
        1,200,000 TPS
        Redis Labs was able to achieve 1,200,000 TPS at less than 1 millisecond latency using a single AWS EC2 instance 
        
        Over 1 million RPS per node
        With ElastiCache for Redis v7.1, large enough nodes can achieve over 1 million requests per second (RPS) per node 
        
        500 million RPS per cluster
        With ElastiCache for Redis v7.1, a cluster can achieve over 500 million RPS 
        
        Redis also supports pipelining, which allows multiple commands to be sent at once. This feature can significantly increase the number of operations per second a server can deliver.



        Approach
        We can address all the limitation of the previous solutions by using an in-memory data store like Redis, which supports geospatial data types and commands. 
        This allows us to handle real-time driver location updates and proximity searches with high throughput and 
        low latency while minimizing storage costs with automatic data expiration.

        Redis is an in-memory data store that supports geospatial data types and commands. It uses geohashing to encode 
        latitude and longitude coordinates into a single string key, which is then indexed using a sorted set. This allows 
        for efficient storage and querying of geospatial data.

        Redis provides geospatial commands like GEOADD for adding location data and GEORADIUS or GEORADIUSBYMEMBER for querying nearby 
        locations within a given radius. These commands are highly optimized for geospatial data and can be used to efficiently handle 
        real-time driver location updates and proximity searches.

        We no longer have a need for batch processing since Redis can handle the high volume of location updates in real-time. 
        Additionally, Redis automatically expires data based on a specified time-to-live (TTL), which allows us to retain only 
        the most recent location updates and avoid unnecessary storage costs.

        Challenges
        The main challenge with this approach would be durability. Since Redis is an in-memory data store, there's a 
        risk of data loss in case of a system crash or power failure. However, this risk can be mitigated in a few ways:

        Redis persistence: We could enable Redis persistence mechanisms like RDB (Redis Database) or AOF (Append-Only File) to periodically save the in-memory data to disk.
        Redis Sentinel: We could use Redis Sentinel for high availability. Sentinel provides automatic failover if the master node goes down, ensuring that a replica is promoted to master.




QPS AND LATENCY FORMULA:
    How many requests per second are considered 'Heavy Load'?
    I'm trying to quantify this in numbers even it is hard to come up with numbers that works for everything. any guide/tutorial would be appreciated!

    it vary's inversely with service time. for instance, if you had a server that did something purely cpu-bound, and it took 100ms to return the result, 
    you could do (theoretical max) 10 requests, per second, per cpu core. the formal math for this is called "little's law".

    in reality you want to capacity plan for running somewhere around 25 - 40% of the max capacity, more than that and tail latency starts to get bad.

    note that this is for a perfectly simplified cpu bound service that replies all on its own. in reality your service will very likely spend some time memory bound, some time cpu bound, and the vast majority of its time i/o bound, so things get really complex from there. its still the same math, buy instead of having *a* queue you have a graph of queues, the limiting factor (slowest path) for the end response being a journey through a queue of queues.

    for example, if you look at fastly's live stats at the bottom of their network page:https://www.fastly.com/network-map/

    they show a response time of about 0.166ms. so thats 1000/0.166 = ~6000 req/s/cpu. put that on a dual socket 16 (real) core/socket server and they can do about 192k req/s/server, but again in order to keep the tail end of that latency distribution from getting bad you want to run around a third of that, so 64k req/s/server. also, since that time was just for the hits and they say thats 88% of the requests, we have to assume the real number is much less than that as the 12% of misses are probably an order of magnitude slower, so right off the bat you can assume you cut the number in half (32k req/s/server).

    conversely, as a much more common example, a standard wordpress install with a small handful of popular and not even 'that bad' plugins will typically return the home page in 300 - 400ms. that means your single-core VPS running your wordpress site can handle about 1 request per second



        For the platform I work on, we target 50% to maybe 60% CPU usage on easily scalable things like API servers and 20% to 33% on systems that take more time to scale.

        Any higher on the API servers and latency becomes a concern. We can tolerate it briefly during a traffic spike, but in general you never want to see anything interactive hit 80% or more. Not only does latency go up, but resource occupancy for things TCP sockets spikes. Not just from the volume, but also the duration as each request is taking longer to fulfill. If you have antique software that spawns a thread per request (like Apache or Jetty), this can also lead to spending more time context swapping than doing actual work.

        Remember when you're looking at a CPU usage stat and see 50%, that means the CPU is being used at 100% half the time. The higher you go on usage, 
        the more likely a second request will be stalled waiting for cycles.


        Here’s a cheat sheet for the typical sizes of common data types that are useful for system design interviews:

        1. Primitive Data Types (in C, C++, Java, etc.)
        Data Type	Size (in bytes)
        Boolean	1
        Char	1
        Int	4
        Long	8
        Float	4
        Double	8
        Short	2
        Byte	1
        2. Database-Related Data Types
        Data Type	Size (in bytes)
        UUID	16
        String	1 byte per character + metadata
        Date/Time	8 (depends on format and precision)
        Timestamp	4 or 8 (depends on precision)
        Blob	Variable
        3. Networking and Storage Units
        Data Unit	Size (in bytes)
        IP Address (IPv4)	4
        IP Address (IPv6)	16
        MAC Address	6
        Port Number	2
        Block (in file system)	Typically 4096 (4 KB)
        4. Common Data Sizes (High-Level)
        Entity	Typical Size
        User Profile	1 KB to 10 KB
        Text Message	160 bytes (SMS)
        Image	100 KB to several MB (varies)
        Video (1 minute)	1 MB to 10 MB (compressed)
        Document (PDF)	50 KB to 5 MB (varies)
        Database Row	100 bytes to several KB (varies)
        5. Memory Sizes
        Memory Size	Equivalent
        1 KB	1,024 bytes
        1 MB	1,024 KB
        1 GB	1,024 MB
        1 TB	1,024 GB
        6. Additional Considerations
        Index Sizes (e.g., B-Tree, Hash Index): Typically in the range of 10-20% of the original data size.
        Pointers: Typically 4 bytes (32-bit systems) or 8 bytes (64-bit systems).
        Overhead for Data Structures (e.g., Array, Linked List): Includes space for metadata like length, pointers, etc.
        This cheat sheet should provide a good reference point during system design discussions.

        Here are more sizes of common entities often referenced in system design:

        1. Social Media Entities
        Entity	Typical Size
        Tweet (text)	~300 bytes (including metadata)
        Facebook Post	2 KB to 4 KB (including comments/metadata)
        Instagram Photo	500 KB to 5 MB (depends on resolution)
        YouTube Video (1 min)	10 MB to 50 MB (compressed)
        2. Web Entities
        Entity	Typical Size
        Web Page (HTML/CSS/JS)	500 KB to 2 MB (uncompressed)
        Cookies	~4 KB (maximum size per domain)
        HTTP Header	500 bytes to 1 KB
        API Request/Response	1 KB to 10 KB (varies with payload)
        3. File Types
        File Type	Typical Size
        Text File (1 page)	~4 KB
        Word Document	50 KB to 500 KB
        Spreadsheet (Excel)	100 KB to 1 MB (varies with complexity)
        Audio File (MP3, 1 min)	1 MB to 3 MB (compressed)
        Compressed File (ZIP)	40% to 70% of original size
        4. Metadata
        Metadata Type	Typical Size
        File Metadata (e.g., creation date, permissions)	512 bytes to 4 KB
        User Metadata (e.g., name, email, preferences)	1 KB to 5 KB
        Geo-Location (lat/long, altitude, etc.)	~100 bytes
        5. Search and Indexing
        Search/Index Data Type	Typical Size
        Text Search Index	~30% of original data size
        Inverted Index	10% to 20% of text data
        Autocompletion Trie	Varies widely based on dictionary size
        Bloom Filter	~1% of data size
        6. Communication and Messaging
        Entity	Typical Size
        Email (text)	10 KB to 75 KB (including headers)
        SMS	160 bytes
        WhatsApp Message (text)	~1 KB (including metadata)
        VoIP Call (1 minute)	0.5 MB to 1 MB (compressed audio)
        These additional sizes should give a broader view of data storage and transfer considerations when designing systems at scale.



        Here are additional sizes and details relevant to HTTP types:

        Ideally, an HTTP request should not go beyond 1 packet. The most widely used networks limit packets to approximately 1500 bytes, so if you can constrain each request to fewer than 1500 bytes, you can reduce the overhead of the request stream.



        1. HTTP Requests
        HTTP Entity	Typical Size
        GET Request	200 bytes to 2 KB
        POST Request (without body)	500 bytes to 2 KB
        POST Request (with JSON body)	1 KB to 10 KB (depending on payload)
        PUT Request	1 KB to 10 KB (varies based on payload)
        DELETE Request	200 bytes to 2 KB
        2. HTTP Headers
        Header Type	Typical Size
        Basic Headers (Host, User-Agent, etc.)	500 bytes to 1 KB
        Authorization Header	300 bytes to 1 KB (depends on token type)
        Custom Headers	Variable, typically a few hundred bytes
        Cookies in Header	200 bytes to 4 KB (max size per cookie)
        Content-Type Header	~50 bytes
        3. HTTP Responses
        Response Entity	Typical Size
        Basic 200 OK Response	200 bytes to 1 KB
        Error Response (e.g., 404)	200 bytes to 2 KB
        JSON Response	1 KB to 10 KB (depends on content)
        HTML Response	2 KB to 500 KB (depends on page complexity)
        File Download Response (e.g., PDF, image)	Varies widely based on file size
        4. HTTP Response Codes
        Status Code	Typical Size
        1xx Informational	~100 bytes
        2xx Success	~100 bytes
        3xx Redirection	~200 bytes
        4xx Client Error	~200 bytes to 1 KB
        5xx Server Error	~200 bytes to 1 KB
        5. HTTP/2 and HTTP/3 Overheads
        HTTP Version	Additional Overhead
        HTTP/2 Headers Compression (HPACK)	Reduces header size by ~40-70%
        HTTP/3 (QUIC-based)	Lower latency but slightly larger overhead due to connection setup
        These sizes can vary depending on the complexity of the request, headers, and payload, but this gives a good estimation for planning and designing efficient HTTP communication in systems.


        
        Review this:
        https://leetcode.com/discuss/interview-question/system-design/357656/Experience-with-back-of-the-envelope-calculations


        One part of system design interview is the back-of-the-envelope calculations. You have to estimate how many servers with which specs you need. I tried to find any manuals with calculations, bit didn't find anything better than https://servebolt.com/articles/calculate-how-many-simultaneous-website-visitors/
        I would appreciate if you share your experience with such calculations.
        Below I publish my humble effort to estimate Twitter (processing servers).

        Preconditions and assumptions:

        330 millions MAU;
        5700 tweets per second in average;
        uniform distribution around the world;
        feed update requests are more frequent (say 10 times);
        we'd like to respond 95% of users for 100ms.
        So we have about 60000 ( = 5700 * 11) QPS. According to the magic formula from the article 32 cores / 0.1s = 320 queries per second for 1 server. So we need 60000 * 0.95 / 320 = 178 servers

        We also know that Twitter uses Redis for improve response speed. Feed cache for each user is stored there. Say each user has 500 tweets in this cache and each tweet is about 200 bytes. So we have 100Kb for each user. We don't know DAU value, but assume 50% of monthly users use Twitter every day. So we have 165 millions users per day, and it's 16Tb RAM (165000000 * 100Kb). So we have about 90Gb RAM for Redis if we have 178 servers. If we choose a server with 128Gb RAM Redis takes about 70% of memory, it's on the verge of critical value. 256Gb RAM will be better.
        In the sake of service durability we have to replicate Redis. Say we need 2 replicas, it's 178 * 3 = 534 servers.

        In the end we estimate storage. 5700 tweets per second, each tweet is 200 bytes. It's about 4Gb per hour, or about 100Gb per day. Let our servers have two weeks history in the fast access, after that we move data to archieve servers (we don't estimate them here). So we need 1.5Tb (of course in RAID).

        Summary:
        about 500 servers for real time processing, 32 cores, 256Gb RAM, 1.5Tb HDD RAID

        P.S. We also need CDN for video, images and so on. They are also out-of-scope here.



        I have seen 2 approaches taken when calculating the back of the envelope calculations.

        The first approach as you have listed out in the bullet points starts with an overall picture of the system and calculations move to a single server and memory requirements. That is if there are 330 million active users and 5700 tweets a second, how do I get to what specs will be required for a single server and thereby calculating how many servers/DBs are needed, etc.
        Under the interview pressure, I always felt this process to be a bit difficult when performing larger divisions. To quote your example "... So we need 60000 * 0.95 / 320 = 178 servers". There is no way I can do this calculation on the whiteboard in live interview without sweating myself.

        The second approach, which I always preferred is to start small and grow bigger with quite a few approximations. After all, the back of the envelope calculation is supposed to be a T-shift level "estimation". I also often start with a small number of variables preferably one.

        For example, instead of managing 2 variables like "Number of active users" and "Number of tweets", I start at the server level and ask myself a question, what factor affects my server the most? a number of active users coming to the server or number of tweets coming to the server. If my server gets 10 tweets per second, does it matter in terms of memory and threads requirements if 10 active users send 1 tweet/sec or 1 active user sends 10 tweets/sec? If it does not matter then I, for now, I will ignore the number of active users and focus on how many tweets the server receives per second. My 2 variables are down to 1.

        I also make sure, I never talk on the specifics of the functionality and instead talk/focus on the raw/common server requirements. That is instead of saying, the server receives 10 tweets per second, I will say the server receives 10 "requests" per second. Converting tweets to requests helps me memorize the same logic across twitter design where the server receives tweets and facebook design where the server receives photo upload and comments requests. Everything is incoming request no differentiation.

        Ok so focusing on the twitter calculations, I would start something like this.

        I will start saying, I will at minimum calculate servers, memory, and storage requirements

        Starting small, I will say, assuming, the application gets 1000 requests per second, (1000 is an easy nice number for any calculation and we can scale up or down easily depending on the requirement. The real twitter number would be much higher)

        1000 requests/sec
        3600 seconds per hour, it will be 1000 * 4000 (approximating 3600 to nearest whole number 4000 as multiplication by 4K is much easier orally than 3.6K) => we get 4 million requests/hr
        4M requests/hr translates to 4M * 30 hours (instead of 24 hours in a day as its much easier to multiply by 30 than 24) => 120 million requests/day
        120 million requests/day translates to 120M * 400 days (instead of 365) = 50 billion requests/year (instead of 48B)
        Assuming the capacity planning estimates are for 5 years, we get 50B * 5 = 250 Billion request data is what we may end up storing in our system.
        Now to calculate the number of servers, From the experience of running applications in production, I know say a tomcat/jetty server running Spring boot application at a minimum will have 100 worker thread (200 default) to handle HTTP requests

        The server will handle tweets, photos, video uploads
        handling 1000 requests with 100 threads I would use 10 + 20% more = 12 servers.
        If 1000 requests change to 10000 requests, 12 servers would more or less convert to 120 servers.
        For server memory requirements of the server:
        Now for server memory requirements, the capacity required to handle requests with video and images would be much higher compared to tweet,

        Assuming photos are 1MB in upload size (Usually a UI side compression library will reduce a photo image size to be around 500KB, but 1 MB is easy for calculation) and videos to be 50 MB in size
        To handle 100 requests/sec for video uploads, 100 * 50MB = 5GB of memory for each commodity server.
        For Storage requirement, assuming we need to store data for 5 years

        As previously calculated, 250 Billion request data to store for 5 years, assuming 10% to be for videos (50MB avg), 20% for photos (1MB avg) and 70% for tweets (200KB avg) we need
        -- Note, usual conversions are (1000 translates to KB storage, 1 million translates to 1 MB of storage, 1 billion translates to 1 GB of stroage)
        10% Video: 250 Billion request data (that is 250 GB) * 10% => 25 GB * 50MB ~~ 25000MB * 50MB ~~ 1250000 MB => 1250 GB => 1.2TB
        20% Photos: 250 Billion request data * 20 % => 50 GB * 1MB => 50000 MB * 1MB => 50000 MB => 50GB
        70% Tweets: 250 Billion request data * 70% ~~ 200 GB * 200 bytes => 200000MB * 0.002MB => 400MB
        Total (1.2TB + 50 GB + 400 MB) ~~ 1.2TB (in reality this capacity will be much higher as video/photo storage size requirements will be much higher but I hope reader gets the point)
        Summary

        Start with a single variable and translate specific design requirement into raw server requirements like requests/sec (instead of tweets/sec or photos/server)
        Start from a single server requirement instead of trying to divide total tweets or total storage by servers.
        Remember to get all the calculations done in 5 mins. Unless the interviewer wants to focus on the specifics calcuations. Remember these contents are high-level estimates.







        Latency calcuation
        Lets say we want 95% 500 ms. 

        18k requests / second are needed for our daily active users. 

        
        1 second = 2(500 ms)

        So 1 server can server 
        
        1 second you need to serve 2 requests -> because 2 * 500ms 
        
        
        1 second -> 2 reqs need to be served. 

        1 server has 50 threads.   

        So in 1 second for a thread you can do 2 requsts... 
        so 1 server can do 100 req/second


        So you need 180 application servers because 18k/100



         -> Therefore -> 1 server can server 100 req/s if we want 500ms latency per request. .. -> 

----------------------------------------------------------------
DATABASE TECHNOLOGYS AND TPS SUPPORT;

    requirement is not clear.

    if tps is high, no-sql may not support it since it has limit also (1k write TPS at dynamodb) we can use redis.
    if cardinality of emailID or name is high, time series DB will not work well
    redis key is: emailID-timeBucket, value is set of {name: cnt}.
    so the data in redis could be:
    abc@gmail.com-day:2/21/2024: {tom: 10, John:5}
    abc@gmail.com-week:2/18/2024-2/25/2024: {tom: 100, John:50}
    abc@gmail.com-month: Feb, 2024: {tom: 200, John:100}
    abc@gmail.com-year: 2024: {tom: 300, John:150}
    redis can support at least 100k TPS per node. If more TPS is needed, use cluster mode and more partition

        IMO, as someone who has been on the interviewer side of system design interviews, always ask questions. As for Databases, just default to SQL until you have a reason not to. OSS SQL databases are insanely efficient and modern OLAP SQL databases (Snowflake, Clickhouse, etc.) can chug through 10s-100s of GBs(possibly TBs) of data in seconds.

        If they give you a reason not to, then use noSQL or streaming tech.


EALSTIC SEARCH LATENCY TIMES: 
    The average search latency in Elasticsearch can vary widely based on the size of the index, hardware, query complexity, and cluster configuration. However, here are some general benchmarks:

    Small Index (under 1 million documents):

    Simple queries: 10-30 milliseconds (ms)
    Complex queries (aggregations, sorting): 50-100 ms
    Medium Index (1 million to 100 million documents):

    Simple queries: 50-100 ms
    Complex queries: 100-300 ms
    Large Index (over 100 million documents):

    Simple queries: 100-300 ms
    Complex queries: 300-500+ ms

        An Elasticsearch index is a logical namespace that stores documents that share similar characteristics. It's the fundamental unit of storage in Elasticsearch and is similar to a database in a relational database schema. 
        
        Here are some things to know about Elasticsearch indices: 
        
        Documents
        Each document in an index contains a collection of fields, which are key-value pairs that contain data. 
        
        Shards
        Each index is divided into one or more shards, which are smaller subsets of the index. 
        
        Data types
        Elasticsearch can index many types of data, including text, numeric data, geolocational data, and dense vectors. 
        
        Data structures
        Different data types are stored in dedicated data structures, such as inverted indexes for text fields and BKD trees for numeric and geo fields. 
        
        Querying
        An index is the highest level entity that you can query against in Elasticsearch. 
        
        Examples
        In an e-commerce website, you can have an index for customers, one for products, one for orders, and so on. 




    SCALING ES:

        2. System Design for Query TPS
        When thinking about the system design to achieve or support high query TPS, there are several key architectural and infrastructure considerations.

        a. Cluster Scaling
        Shard Count: Elasticsearch splits data into shards for distribution. More shards mean more parallelism, which can help support higher query TPS. However, too many shards can cause overhead.
        Node Count: Adding more nodes to the cluster increases the overall query throughput. More nodes can handle more queries simultaneously.
        Replication: Using replica shards allows queries to be distributed across more nodes, balancing the load and increasing read throughput (QPS).
        b. Index and Data Structure Design
        Efficient Mapping: Using efficient data types and mappings reduces the size of the index and speeds up search operations.
        Inverted Index: Elasticsearch uses an inverted index structure for search. Optimizing this structure through field normalization (such as not analyzing certain fields) can reduce query time.
        Doc Values: Storing fields as doc values allows for faster aggregation queries, which improves TPS for analytical searches.
        c. Caching and Memory
        Query Cache: Elasticsearch has a query cache to store frequently queried results. A well-tuned query cache can dramatically increase TPS by serving repeated queries from memory.
        Field Data Cache: If you're running heavy aggregations or sorting, field data cache needs to be well-managed to prevent out-of-memory errors and to improve query speed.
        Heap Size: Proper memory allocation (heap size) on each Elasticsearch node is crucial. Keeping frequently queried data in memory reduces the time to serve requests.
        d. Query Optimization
        Query Complexity: Simpler queries (e.g., exact matches vs. fuzzy searches) will result in a higher TPS. Complex queries that involve aggregations, filters, or fuzzy matching will reduce TPS.
        Batching Requests: Grouping queries or using multi-search (_msearch) can reduce overhead, increasing the overall TPS for the system.
        e. Load Balancing
        Coordinator Nodes: Elasticsearch can have dedicated coordinator nodes that handle query routing. These nodes distribute queries to the appropriate data nodes and return the result to the client.
        External Load Balancer: Using a load balancer (like HAProxy, Nginx) can help distribute query traffic evenly across Elasticsearch nodes to avoid overloading any one node.
        f. Infrastructure
        IO Performance: SSDs are recommended for fast index and search operations. IO performance has a significant impact on how quickly Elasticsearch can serve queries.
        Network Latency: Reducing network latency between client applications and Elasticsearch nodes, as well as between Elasticsearch nodes themselves, will contribute to a higher query rate.
        g. Benchmarking
        Tools like Rally (Elasticsearch benchmarking tool) can be used to simulate workloads and measure the TPS under different conditions.



            1. Optimizing Geospatial Queries (Geohash Tuning)
            Elasticsearch supports geospatial queries using geohash indexing. When querying for stores based on geographic locations, you need to ensure that these queries are as efficient as possible.

            a. Use the geo_point Field Type
            Define the store locations using the geo_point field type in your mapping. Geohashes are used to index these points, allowing for fast geographic search.

            Example mapping:

            json
            Copy code
            PUT /stores
            {
            "mappings": {
                "properties": {
                "location": {
                    "type": "geo_point"
                }
                }
            }
            }
            b. Adjust Geohash Precision
            Elasticsearch allows you to control geohash precision, which determines how detailed the location data is stored. Higher precision means more accurate locations but at the cost of performance. Since you have a small dataset, you should experiment with lower precision values to speed up queries without sacrificing too much accuracy.

            Example:

            json
            Copy code
            "location": {
            "type": "geo_point",
            "geohash": true,
            "geohash_precision": 5  // Experiment with precision values like 3, 4, 5
            }

            Lower precision (e.g., 3-5 characters) will group stores into larger buckets, making range queries faster.
            Higher precision (e.g., 8-12 characters) gives finer accuracy but could lead to slower queries.
            c. Use the geo_distance Query for Store Lookup
            For querying stores within a specific radius from a location, use geo_distance. To optimize TPS, ensure your queries are not overly complex in terms of distance or range, keeping the bounding box as small as possible.

            Example query:

            json
            Copy code
            GET /stores/_search
            {
            "query": {
                "geo_distance": {
                "distance": "5km",
                "location": {
                    "lat": 40.73,
                    "lon": -73.93
                }
                }
            }
            }
            2. Small Data-Sized Index Optimizations
            Given that your data size is small, the following general optimizations will help reduce query latency and increase TPS:

            a. Sharding Strategy
            Few Shards: Since the dataset is small, avoid over-sharding. One or two primary shards should be sufficient.
            Replicas for Query Throughput: If you expect many simultaneous read queries, consider having one or two replica shards. This allows Elasticsearch to distribute read queries across multiple nodes.
            json
            Copy code
            PUT /stores/_settings
            {
            "index": {
                "number_of_shards": 1,
                "number_of_replicas": 1  // Add replicas for read scalability
            }
            }
            b. Optimize for Search Performance (No Indexing Overhead)
            If the data does not change often, you can disable features like dynamic mapping and ensure that no unnecessary indexing or background processes (such as merging) are running.
            json
            Copy code
            PUT /stores/_settings
            {
            "index": {
                "refresh_interval": "30s",  // Reducing refresh interval if frequent updates are not necessary
                "number_of_replicas": 1
            }
            }
            c. Cache Utilization
            Query Cache: Enable and tune query caching. If you expect many repeated queries (e.g., same location radius or store lookup), Elasticsearch’s query cache can store frequently queried results.
            json
            Copy code
            PUT /stores/_settings
            {
            "index": {
                "requests.cache.enable": true
            }
            }
            Field Data Cache: If your queries involve sorting or aggregating based on numeric fields, optimize the field data cache to reduce expensive disk lookups.
            3. Query Optimization
            a. Use Simple and Selective Queries
            Avoid using complex queries with multiple filters or aggregations. Keep your queries simple, especially if the goal is to maximize query throughput.

            b. Avoid Full-Text Search
            Since you're working with geospatial data (store locations), ensure that fields like store name or store details are not unnecessarily analyzed as full-text. Use keyword fields instead to store non-analyzed data.

            c. Use Filtered Queries:
            Use filtered queries when possible. Filters are cached, and using them instead of scoring queries can improve performance.

            Example:

            json
            Copy code
            GET /stores/_search
            {
            "query": {
                "bool": {
                "filter": [
                    {
                    "geo_bounding_box": {
                        "location": {
                        "top_left": {
                            "lat": 40.917577,
                            "lon": -74.259090
                        },
                        "bottom_right": {
                            "lat": 40.477399,
                            "lon": -73.700272
                        }
                        }
                    }
                    }
                ]
                }
            }
            }
            4. Memory and Hardware Considerations
            a. Heap Size
            Since the index is small, you should allocate enough heap size to ensure that frequently accessed data can stay in memory. The rule of thumb is to keep the heap size below 32GB to allow for pointer compression and efficient memory use.
            b. SSD Storage
            Even for small data sets, using SSDs for your Elasticsearch nodes is critical to achieving high TPS, as SSDs provide fast random read/write access.
            c. Reduce Network Latency
            Ensure your nodes are deployed in low-latency environments to minimize communication delays between nodes and with the querying applications.

    SCALE FOR ES: SMALL INDEX VS LARGE INDEX 

            To tune an Elasticsearch index for high query throughput (TPS) for a small data-sized index with a list of stores, you need to focus on optimizing both the cluster configuration and the index settings. Let's break down the key components:

            1. Index Sharding and Replicas
            Sharding: By default, Elasticsearch creates 5 primary shards. For a small dataset, having too many shards can be inefficient, as each shard incurs overhead for resource management. Since your index is small, you should reduce the number of shards to 1 (or a few depending on query parallelism needs). This will help reduce the overhead and improve performance.

            Why: Elasticsearch parallelizes queries across shards, and with a small dataset, fewer shards mean less internal coordination.
            How: When creating the index, specify:
            json
            Copy code
            {
            "settings": {
                "index.number_of_shards": 1
            }
            }
            Replicas: For high throughput, adding replicas can help because replicas allow queries to be load-balanced across nodes. If you have multiple nodes, increasing the number of replicas can improve read TPS by distributing the query load.

            Set replicas based on the number of nodes (e.g., if you have 2 nodes, set index.number_of_replicas: 1).
            Balancing: Replicas can also help with fault tolerance, but if you have a single-node deployment, replicas may not matter for performance.
            2. Mapping and Index Settings
            Optimize Field Mappings: If your data consists of a list of stores (e.g., store name, geolocation), ensure that only necessary fields are indexed. Disable indexing for fields that don’t need to be searchable.


            1. Sharding and Replication Strategy
            Sharding for Large Data: Unlike a small index, a large index benefits from having multiple shards. This distributes both indexing and search load across nodes. However, over-sharding can result in increased coordination overhead. You need to balance the number of shards to ensure they are neither too large nor too small.

            Aim for shard sizes between 20GB and 50GB.
            Monitor shard size to avoid unbalanced shard distribution. Use the _cat/shards API to ensure shards are spread evenly across nodes.
            You can split shards later if needed, but it’s better to estimate shard count upfront based on data size.
            Example of setting 10 shards:

            json
            Copy code
            {
            "settings": {
                "index.number_of_shards": 10
            }
            }
            Replica Shards: Like small indexes, replica shards distribute read load across nodes. For large indexes, replicas become more important for scaling read queries and ensuring fault tolerance. The number of replicas should be at least number_of_nodes - 1.

            Ensure that replicas are spread across different nodes for availability.
            Dynamic Load Balancing: Elasticsearch will automatically distribute queries across the replica shards, so scaling read throughput becomes more efficient.



            Keyword fields: Use keyword data type for exact matches like store names.
            Geo fields: Use geo_point for geolocation data to enable efficient geo-based searches.
            Example mapping:

            json
            Copy code
            {
            "mappings": {
                "properties": {
                "store_name": { "type": "keyword" },
                "location": { "type": "geo_point" }
                }
            }
            }
            Doc Values: Disable doc values for fields that are not used for sorting or aggregations. This reduces disk usage and speeds up retrieval.

            json
            Copy code
            {
            "store_name": {
                "type": "keyword",
                "doc_values": false
            }
            }
            3. Query Tuning
            Use Filter Queries: Filters (e.g., for store attributes like store type or region) are cached and therefore much faster to execute than regular search queries. Since filter queries do not need scoring, they are computationally cheaper.

            Use filters instead of queries when you do not need relevance scoring.
            Cached filter queries:
            json
            Copy code
            {
            "query": {
                "bool": {
                "filter": [
                    { "term": { "store_name": "example-store" } }
                ]
                }
            }
            }
            Geo Queries: If you are querying based on location (e.g., searching for stores within a radius), use Elasticsearch’s geo-distance queries with geohashes or geo_point optimizations.

            Geohash indexing enables efficient location-based searches at various zoom levels by partitioning the space into smaller boxes.
            Example:

            json
            Copy code
            {
            "query": {
                "geo_distance": {
                "distance": "10km",
                "location": {
                    "lat": 40.7128,
                    "lon": -74.0060
                }
                }
            }
            }
            4. Caching
            Query Cache: Elasticsearch caches the results of filters automatically. Make sure your queries utilize filter contexts to benefit from query caching.

            The cache size can be adjusted in Elasticsearch’s elasticsearch.yml file.
            To enable/disable the query cache for specific queries:
            json
            Copy code
            {
            "query": {
                "bool": {
                "filter": [
                    { "term": { "store_name": "example-store" } }
                ]
                }
            },
            "settings": {
                "request_cache": true
            }
            }
            Field Data Cache: If you are using keyword fields for sorting or aggregations, ensure that field data is well-optimized. Field data caching helps in speeding up these queries but consumes a lot of heap memory.

            5. Memory and Resource Management
            Heap Size: Ensure that your Elasticsearch nodes have enough heap memory allocated. For small indexes, you may want to allocate more memory to the filesystem cache (OS-level caching) than Elasticsearch's heap. A good rule of thumb is to set the heap size to no more than 50% of the available memory.
            Example: For a 16GB system, set the heap size to 8GB.
            ES_JAVA_OPTS="-Xms8g -Xmx8g"
            Avoid Swapping: Set bootstrap.memory_lock: true in your elasticsearch.yml file to avoid performance hits from swapping.
            6. Index Refresh and Segment Merging
            Refresh Intervals: By default, Elasticsearch refreshes the index every second to make new documents searchable. If real-time indexing is not critical, you can increase the refresh interval to reduce I/O overhead.

            Example:
            json
            Copy code
            {
            "settings": {
                "index.refresh_interval": "30s"
            }
            }
            Segment Merging: Elasticsearch periodically merges segments in the background, which can cause temporary spikes in resource usage. Tune the merge policy for small indexes by lowering index.merge.scheduler.max_thread_count to reduce the impact of large merges.





    SCALE ES FOR ALOT OF WRITES:



        To optimize Elasticsearch for high write throughput (TPS) on a large index, you need to focus on improving indexing performance, reducing I/O overhead, and efficiently managing resource consumption. Here's a detailed breakdown of how to tune Elasticsearch for high write performance in a large data-sized index:

        1. Sharding and Replication Strategy
        Sharding: Large datasets require more shards to distribute the write load across nodes. However, having too many shards can lead to inefficient resource use, and too few shards may cause nodes to become overwhelmed. A good target is around 20-50GB per shard, but you can test based on your data model and indexing patterns.

        Pre-shard estimation: Estimate the total data size and pre-allocate an appropriate number of shards upfront. Adjust shard allocation as data grows using the _split API.
        Dynamic sharding can also be used, but for high write TPS, it's better to configure this upfront rather than adding shards dynamically.
        Replicas: Replicas slow down writes because every write operation must be replicated. If you prioritize write throughput over availability, reduce the number of replicas to 0 during the bulk indexing process and increase it once the data is ingested.

        For indexing: Set replicas to 0.
        After indexing: Increase replicas back for fault tolerance and read throughput.
        json
        Copy code
        {
        "settings": {
            "index.number_of_replicas": 0
        }
        }
        2. Refresh Interval Optimization
        Increase the Refresh Interval: Elasticsearch makes newly indexed documents searchable after each refresh operation, which happens by default every second. This can be very costly for write-heavy workloads. Increase the refresh interval (e.g., to 30 seconds or more) to reduce the frequency of refreshes.
        For high write TPS, you can even temporarily disable automatic refresh and manually refresh the index when needed:
        json
        Copy code
        {
        "settings": {
            "index.refresh_interval": "-1"
        }
        }
        After the bulk operation, you can set a reasonable refresh interval (e.g., 30s or 1m).
        3. Bulk Indexing
        Use Bulk API: Elasticsearch is highly optimized for batch processing, and sending documents in bulk can significantly improve write TPS compared to individual indexing operations.

        Batch size: A typical bulk size is between 5,000 to 15,000 documents or a bulk size of 5MB to 15MB. Experiment with the bulk size to determine the ideal throughput.
        Using too large of a bulk size can result in slower indexing due to the overhead of handling larger requests and can increase the memory pressure on Elasticsearch nodes.
        Example of bulk indexing:

        bash
        Copy code
        POST _bulk
        { "index": { "_index": "my-index", "_id": "1" } }
        { "store_name": "Example Store", "location": { "lat": 40.7128, "lon": -74.0060 } }
        { "index": { "_index": "my-index", "_id": "2" } }
        { "store_name": "Another Store", "location": { "lat": 34.0522, "lon": -118.2437 } }
        4. Index Buffer Size and Translog Settings
        Increase Index Buffer Size: Elasticsearch uses an indexing buffer that holds documents before they are written to disk. The default buffer is 10% of the JVM heap. For large indexing jobs, you can temporarily increase this buffer size to allow more documents to be held in memory before flushing to disk.

        Example: Increase the buffer size to 20% of heap (use with caution on production clusters):
        json
        Copy code
        {
        "settings": {
            "indices.memory.index_buffer_size": "20%"
        }
        }
        Translog Durability: The translog is responsible for ensuring write durability (i.e., keeping track of changes in case of a failure). By default, Elasticsearch writes each document to the translog synchronously, which can slow down write performance. For high TPS, consider switching to asynchronous durability during bulk indexing.

        Set the translog to async for high write performance:
        json
        Copy code
        {
        "settings": {
            "index.translog.durability": "async",
            "index.translog.flush_threshold_size": "512mb"
        }
        }
        Flushing: You can also adjust the translog flushing behavior. By default, Elasticsearch flushes the translog once it reaches 512MB. You can increase this threshold for larger indexing jobs to reduce the frequency of flush operations.

        5. Segment Merging and Throttling
        Segment Merging: Elasticsearch uses Lucene to manage the underlying data storage, which involves merging smaller segments of indexed data into larger ones. While merging improves query performance, it can be resource-intensive, especially during write-heavy operations.

        Use the index.merge.scheduler.max_thread_count setting to limit the number of threads used for merging and avoid resource contention during heavy indexing.
        Example:
        json
        Copy code
        {
        "settings": {
            "index.merge.scheduler.max_thread_count": 1
        }
        }
        Throttle Merges: By default, Elasticsearch limits the rate of merges to ensure the system isn’t overwhelmed. However, for high write TPS, you may want to tune this by allowing more aggressive merging after bulk indexing.

        Example:
        json
        Copy code
        {
        "settings": {
            "index.merge.policy.max_merge_at_once": 10,
            "index.merge.policy.segments_per_tier": 10
        }
        }
        Index Throttling: Elasticsearch can throttle indexing to prevent too many write requests from overwhelming the system. By default, it allows for unthrottled bulk indexing, but you can fine-tune this based on cluster health:

        json
        Copy code
        {
        "settings": {
            "index.store.throttle.type": "none"
        }
        }
        6. Disk I/O and File System Tuning
        Use SSDs: Write-heavy workloads benefit from high IOPS, and using SSDs instead of HDDs is critical to maximizing write performance. SSDs handle the high random write throughput more efficiently than traditional spinning disks.
        Optimize Filesystem Cache: Elasticsearch relies heavily on the operating system’s filesystem cache for efficient write operations. Ensure that enough memory is allocated outside of the JVM heap for OS-level caching.
        Disk Watermarks: Ensure that Elasticsearch’s disk watermark settings are properly configured to avoid sudden write slowdowns. Lowering the watermark thresholds (e.g., cluster.routing.allocation.disk.watermark.high) helps prevent the cluster from becoming overwhelmed as disks fill up.
        7. Memory and JVM Tuning
        JVM Heap Size: For high write TPS, ensure that the JVM heap size is optimally configured. Too small a heap can lead to frequent garbage collection, while too large a heap can lead to inefficient GC pauses.

        The general recommendation is to allocate 50% of the available system memory to the JVM heap (but no more than 32GB to avoid pointer compression issues). Example:
        bash
        Copy code
        ES_JAVA_OPTS="-Xms32g -Xmx32g"
        Garbage Collection (GC): Use G1GC (Garbage-First Garbage Collector) to minimize GC pauses during indexing. G1GC is better suited for larger heaps and more predictable pause times, which is critical during large write operations.

        8. Node and Cluster Scaling
        Dedicated Ingest Nodes: In large clusters, consider setting up dedicated ingest nodes that handle preprocessing of documents (e.g., enrichment, pipelines) separately from data nodes. This allows you to offload resource-intensive tasks from the data nodes.
        Dedicated Master Nodes: For large-scale clusters, having dedicated master nodes ensures that master election and cluster coordination are separated from the indexing process, reducing the chance of master node bottlenecks during high write operations.
        9. Index Lifecycle Management (ILM)
        Rollover Indexes: If your index is growing too large, rollover indexes allow Elasticsearch to manage large data sets by splitting them into smaller indexes over time. This helps in keeping shards manageable and avoiding performance degradation.
        Example ILM policy:
        json
        Copy code
        {
        "policy": {
            "phases": {
            "hot": {
                "actions": {
                "rollover": {
                    "max_size": "50gb",
                    "max_age": "30d"
                }
                }
            }
            }
        }
        }
        10. Monitoring and Benchmarking
        Monitor Hot Threads: Use the hot_threads API to identify bottlenecks in the system during bulk indexing operations.
        bash
        Copy code
        GET /_nodes/hot_threads
        Rally for Benchmarking: Use Elasticsearch Rally to simulate bulk indexing operations and identify the optimal configurations for your write-heavy workloads.
        Summary:
        Shards and Replicas: Pre-configure the number of shards based on expected data size. Set replicas to 0 during bulk indexing.
        Bulk Indexing: Use the Bulk API to batch write operations for efficiency.
        Refresh and Translog: Disable automatic refreshes and use asynchronous translog durability for high write throughput






    SCALING ELASTIC SEARCH:



        Yes, you are correct. When setting up your mapping, you can set the number of shards (primary) and replicas (copies). Replica shards are basically clones of your primary shards, that are there for resiliency, but also benefit the read performance (they can serve read operations). They can harm write performance though, since elastic needs to replicate the data across the nodes in order to keep them up to date. Depending on the number of nodes, you can decide the number of primary and replica shards, with resiliency in mind (what happens if a node goes down)
        
        Yes if you have one shard with zero replicas, as per the documentation, it will be a single thread. That is not necessarily bad or good. Keep in mind that in the case of one request, that request is served by multiple threads (multiple shards containing parts of the data) in the end these records need to be accumulated in order to be served to the client. This can harm performance. Moreover, even if you have replicas, if you have only one primary shard, that means that all the data of your index are in a shard (primary or replica). This means that different requests can be served by any shard (thus any thread), but each request will be served by one thread (no accumulation needs to happen, which for MB of data, is not a bad thing)
        Since the data size is small, and you need a very high throughput, I would opt to have 1 primary and as many replicas as the number of nodes - 1 (which will hold the primary). Now the number of nodes depends. You'll have to test, but you could go with 3 nodes (which is a common resilient/performant first setup). So 1 primary and 2 replicas in total. Check with that setup and try stress testing it.



        https://datadome.co/bot-management-protection/store-50-million-event-per-second-in-elasticsearch/


        Our cluster stores more than 150TB of data, 15 trillion events in 60 billion documents, spread across 3,000 indexes and 15,000 shards over 80 nodes. 
        Each document stores 250 events.

        During peak charge each day, our Elasticsearch cluster writes more than 200,000 documents per second and has a search rate of more than 20,000 requests per second.


        Our Sharding Policy Challenge
        We provide our customers with up to 30 days of data retention in our Elasticsearch cluster. Initially, our cluster was composed of 30 dedicated servers split into a hot-warm architecture. Our hot layer consisted of 15 servers with CPUs (20 threads and 5 SSD disks in RAID0), while our warm layer consisted of 15 servers with lower CPUs in order to reduce the cost because the data inside the warm layer are less requested.

        The last seven days of data are stored in the hot layer, and the rest in the warm layer.

        A common best practice is to keep a shard size around 50GB. As described above, we have dedicated indexes for each customer, but our customers do not all have the same workload. Our biggest customers write tens of thousands of documents per second, while our smallest write a few hundred. Furthermore, we had to constantly adapt the number of shards to the evolution of our customers’ traffic, in order to respect this best practice of 50GB per shard.

        To resolve this issue, we ran a job each day to update the mapping template and create the index for the next day, with the right number of shards according to the number of hits our customer received the previous day. Since we know that one document indexed weighs around 1,000 bytes, we can predict the number of shards we need for each index to keep the shard size at or below 50 GB.

        If we put a document inside a nonexistent index, an index will be created for it. Due to the design of our daily index, this could cause some trouble for our cluster (yellow status, unassigned shards, etc.), because we have a lot of throughput and the primary nodes need to allocate the shards to the nodes at the same time (around midnight). However, a job that creates a new index a day in advance helped us avoid these issues.

        In the example below, our index pattern is “index_clientID_date”


    ELASTIC SEARCH PAGINATION:


            Pagination in Elasticsearch is a crucial feature for handling large sets of results efficiently. It allows you to break down search results into manageable chunks and retrieve them incrementally. Elasticsearch primarily supports two types of pagination methods:

            1. Search After (Cursor Pagination)
            Search After is a cursor-based pagination method designed for efficient deep paging with large result sets. It is particularly useful when you need to handle large numbers of documents and want to avoid the inefficiencies of traditional offset-based pagination.

            How It Works:
            Cursor-Based Pagination: Instead of using an offset, which requires Elasticsearch to skip through documents to reach the desired page (leading to performance degradation for deep pages), Search After uses the sort values of the last document from the previous page as a cursor to fetch the next page.
            Sort Values: Search After relies on the sort values of documents to determine the position in the result set. This means you need to specify a sort order in your query.
            Example Usage:
            Initial Request: Perform a search request and sort the results by a unique field or fields. The search_after parameter will be empty for the first request.

            json
            Copy code
            GET /my-index/_search
            {
            "sort": [
                {"timestamp": "asc"},
                {"_id": "asc"}
            ],
            "size": 10
            }
            Subsequent Requests: Use the sort values from the last document of the previous page as the search_after parameter to get the next set of results.

            json
            Copy code
            GET /my-index/_search
            {
            "sort": [
                {"timestamp": "asc"},
                {"_id": "asc"}
            ],
            "size": 10,
            "search_after": ["2024-01-01T00:00:00", "1"]
            }
            Here, "2024-01-01T00:00:00" and "1" are the sort values from the last document on the previous page.

            Benefits:
            Efficient Deep Paging: More efficient than using offsets for deep pagination as it does not require Elasticsearch to skip through large numbers of documents.
            Consistent Results: Provides a stable cursor-based navigation through results, avoiding issues with results changing between page requests.
            Considerations:
            Requires Sorting: You must specify a sort order to use Search After.
            Stability: The sort field(s) used should have unique values or be stable enough to avoid issues with duplicate or missing documents.
            2. From/Size Pagination (Offset-Based Pagination)
            From/Size Pagination is the traditional method of pagination in Elasticsearch. It involves specifying an offset (from) and a limit (size) to control which subset of results to return.

            How It Works:
            Offset-Based: Uses the from parameter to skip a certain number of results and the size parameter to limit the number of results returned.
            Example Usage:
            json
            Copy code
            GET /my-index/_search
            {
            "from": 20,
            "size": 10,
            "query": {
                "match_all": {}
            }
            }
            This request skips the first 20 documents and returns the next 10 documents.

            Benefits:
            Simple to Use: Easy to understand and implement for straightforward pagination needs.
            Considerations:
            Performance Issues: Can be inefficient for deep pagination as Elasticsearch has to sort and skip through the results, which can impact performance and memory usage.
            Consistency: Results may change between page requests if the index is updated, leading to potential inconsistencies.
            Summary:
            Search After (Cursor Pagination): Suitable for deep pagination and large result sets. It is more efficient for large datasets and provides consistent results. Requires a stable sort order.
            From/Size (Offset-Based Pagination): Simpler and easier to use but less efficient for deep pagination and can lead to performance issues and inconsistencies in large datasets.
            For large-scale applications where deep pagination is required, Search After is generally recommended due to its efficiency and stability.

----------------------------------------------------------------

Amazon Onsite | System Design Pickup Delivery System For Groceries
        30
        codingsomething's avatar
        codingsomething
        34
        January 1, 2020 10:03 PM

        12.5K VIEWS

        The question was to design a system for a delivery pick up system (think whole foods devliery).

        Things they asked me to think about are:

        How do grocery workers pick up the task
        How are the devlieries scheduled with each devliery truck/worker
        How are payments adjusted as per what the items are available





















----------------------------------------------------------------

    Design "How Many people currently viewing the property"  for a E Commerce Hotel Booking Site (todo)

    PRESCENSE PLATFORM?



    First we should define and clarify currently viewing. Say currently means within 5 mins, viewing means the client side (e.g. a web browser) is connected and in current active status.

    We can implement a client side application (e.g. javascript) which periodically calls back web client current status. We keep counters in 
    shared memory e.g. redis, for all the properties we want to track. Assume the interval we defined for current is 5 mins, then the callbacks 
    frequency should be at least once per 5 mins. Also we purge the callbacks older than 5 mins ago, to make sure all the records in our counters is no older than 5 mins.


        We can store a HashMap<clientid, int[300] Hit > in Redis/Memcache .
            Each item in 300 length Circular array corresponds to the number of hits in that second. We store hits for the past 300 secs (5 mins).
            Additionally we need a Circular array of int[300] times as well. Each hit bucket is associated with times[] bucket . Times bucket can store timestamp and hit bucket will store no. of visitors for a particular second. We can just combine Hit and Times bucket into another Object of size 300 to simplify the implementation.

            We need to store session info as well . a clientId can open multiple sessions . I am sure we do not want to consider that as a hit count.
            This information needs to be persisted for each Hotel , client will request the hits for a particular hotel at any point in time.



            I think the solution isn't far off from using a "pixel", although it would be JS that sends updates that the client is still viewing the property. The first call would happen after the page is rendered, to insure that a view actually occurred. The, I'd use some type of event from the client that tells the server that it's still being viewed by that particular client - monitor for scroll events, mouse move, etc at some reasonable interval - similar to how ad tech monitor engagement or "time on page". This prevents some page that is just left open for a long time from being counted, as it is not likely a real user viewing the property.

            This would allow for a very short TTL for that person being counted. Instead of a 5 minute TTL, as others have suggested, you can use something more accurate like 30 seconds.



----------------------------------------------------------------

        E-commerce (Amazon)Website looking into other competitor Website products prices and update (TODO)

        E-commerce (Amazon)Website looking into other competitor Website products prices and update the website item prices accordingly. Main idea is to make sure that items are listed less than price of competetor website.



----------------------------------------------------------------


        Coupon Validation Service

        Last Edit: February 2, 2021 11:30 PM

        2.2K VIEWS

        Please design a Coupon Service ( Low Level and High Level ) for online food delivery system like Zomato. Following are the properties of the coupon service :

        A coupon can be specific to some restaurants, payment gateways or coupon can be generic.
        A coupon can be flat / percentage / cashback coupon. A coupon will have min. Cart value and max. discount.
        Coupon can be single time use or multiple time use.
        A coupon can be specific to one customer.
        A coupon can have time limit meaning it can only be applied once in t minutes.
        If a coupon is one time usable coupon and the order gets cancelled, coupon should be active again

        Low LD - DB Schema, Api
        High LD - Components or Services, Caching Algorithm

        Any ideas on how to implement such a system.
        Thanks


----------------------------------------------------------------    

    system design batch delete and recover system

        Recently preparted for system design and came this design question with Tt on how to design a system to batch delete and undo-delete within some centains times.

        I feel like this is similar to design dropbox/youtube on the uploading part. but cound't get too much details on how to do delete and 'undo' delete. Would appreciate any insights.



        Conceptually, we don't delete the data; instead, we retain it for a while to help with recovery and undo.
        This concept is called Soft-delete . You do this by adding a field to the persistent object, such as "deleted_ts" (timestamp). The timestamp is set to the current timestamp when delete is performed, while undo merely unsets the field. Additionally, you can have a policy to garbage collect/completely delete an object if deleted_ts is older than a predetermined amount of time (for example, 30 days). This can be done by a background cron type of process. In the event of erroneous or bug-related deletions, this functionality also enables data recovery.


        You could tag the resource as soft deleted, you then don't count these as valid, downloadable, viewable or anything (aside from viewable in trash, which picks items by soft_deleted tag)

        You can also add it as a timestamp "soft_deleted_at", this then helps you clean the deleted items at intervals (let's say you got a cron job running every noon hard deleting items that have soft_deleted_at > 30.days)




----------------------------------------------------------------

    System Design: Design a system to process data in different formats from different sources

    Question: We are getting data in different formats such as JSON, plain text, CSV files, XML etc. The file size can vary from 1 MB coming in every 15 mins to 1 GB files coming in every month. Design a system that can process this data coming in from various sources in various formats, parse it and store in in JSON format only. You are free to use any type of database.

    Follow up: When the data files are very huge like 1 GB each, how will you process them fast.

    Please suggest solutions.


    i THINK s3 should be used here, and data should be chunked and maintained in metadata db. 


    As per my understanding I can think of below design.
    To save different formats data in JSON format, I would prefer to save it in a document oriented database like mongoDB or DynamoDb because they're typically schemaless, so each document can contain any data that we want them to have and we can change it on the fly. For key I assume to take ID like Document Id or so. (I assume file is coming in frequency of 1MB/15 min or 1GB/ month)

    Follow up: To avoid any data loss of large data, add queue before writing it to db and call async threads to process those data conversion in parallel(to process fast). Also in this case I will break the data in small files and save it in Cassandra DB (instead of mongoDB) with column like document ID, partition, count , JSON to improve read performance.

    For scaling add replica of DB, Load Balancer, Separate API's for read and write.

    Please feel free to add any comments or idea you have.


    Requirements?
    do we need to process these files in real-time?
    do all documents have proper field name defined? for json and xm lits fine, but for csv and text we have to make sure.

    Approach

    save the file to s3 or any location and then submit the location along with file type to a kafka topic.
    run a stream processor or a simple consumer which will consume those messages
    take msg, and download the file on local disk and read line by line.
    based on file type convert the line to a json document
    store to database. I will use document database like mongo for storing these data for obvious reasons.
    Size of the file will not affect our approach as we are not copying file to the memory unless we encounter a line big enough to not fit into the memory


    This should be solved using spark/hive HDFS

----------------------------------------------------------------
Design amazon fresh: (TODO:)


For my Amazon SD interview, I was asked to design Amazon Fresh. It took up an entire whiteboard and even then we intentionally didn't dive into the delivery/employee side. It was almost exclusively focused on the customer side and the B2B integration. Even more narrowly we targeted searching, inventory management and content serving all with a heavy emphasis on scalability. Remember, Amazon is expecting millions of users spread across the world and you should design for that. We went pretty deep into specifics but it was by far my favorite interview of the day.

Now I was going in with 15+ yoe and I wasn't new to SD so YMMV on how deep they expect you to go. Let the interviewer guide you and when in doubt ask them if they want you to expand on an area (such as how you'd load balance or what messaging system you'd use) instead of volunteering that information.
----------------------------------------------------------------
amazon (todo):

I would like to know how system is design for like amazon order syste, how do they deliver prime order so fast, how the cart is designed ? etc .. where can i find some blogs or good resources for system design like above scenarios ?


----------------------------------------------------------------

Job listing storage and search

    Lets say we are storing a large number of job listings. We want to search a job at a location,, for example location 'dallas, tx' and by search term 'java develoer' and it should return all the list of job postings at that particular location. How do you index and store the job postings for such a requirement? The search results should return the recent ones first. The premium job listings should show up on the top. how would you design it on high level?




    
    yOU CAN query more results then you need it and keep it in redis cache for pagination purposes. 





    @zeus1985 I think this use-case can be easily handled by having denormalised data in our backend search engine(ElasticSearch or Solr). I am considering Elasticsearch(ES) as our search engine going forward
    Lets say we have the following jobs in Dallas, TX area:
    Location || Language Preferred || Min YoE || Premium
    Dallas Java 5 Y
    Dallas Java 10 N
    SF Java 3 Y

    Our ES will have 3 documents for these 3 job openings & we can have index on all 4 fields.
    We will have a REST layer sitting on top of this ES which exposes data to customers. It would intercept incoming query from user sanitise inputs(throw out errors if any) and then proceed to query ES.

    I am assuming the response from REST layer is paginated. Assuming REST layer is responsible to send down a max of 10 results to the clients.
    Whenever REST layer gets an incoming query, it would first query ES with a soft filter(Premium = Y). Now there are 2 scenarios:

    ES returns back all the Premium jobs for the location & there are >10 Premium jobs(enough to display a page). REST layer would send the response back to client
    ES returns back all the Premium jobs for the location but the total number of premium jobs for this location is < 10. REST layer would make a follow up query to ES with Premium = N to get the rest of job openings & returns 10 results to the client.

    @xram Can you explain what do you mean by location range queries ? Are you saying if user searches for jobs in Dallas, the system should show them every job which is say within 10/50 miles radius of Dallas ? Is that the requirement ?

    If we want to have location range queries then we can store the lat/long co-ordinates of a city as a geo-point type in ES. During query time we can perform a geo-distance query to fetch all the listings within x mile radius of Dallas.

    
    
----------------------------------------------------------------
    Design a UPS style mail delivery system (todo)


    These are the problems that I can come up with:

    What kind of data do we have?
    UPS Stores with Addresses, UPS Delivery Vans, UPS Freight Flights.
    Addresses of all homes and destinations
    Types of Shipping
    Shipping Costs by Vehicle and Distance (This is a big problem in itself. How to keep the costs low and competitive)
    Tracking and updating the package info at each facility
    Email information of the clients to update the status of package
    
    What is the main problem at hand for UPS delivery?
    
    Finding the Shortest path from Source to Destination Address
    Package sorting based on the number of Vehicles, Flights and Urgency of the package
    Sorting the packages at facility based on the tracking number, which saves lot of time for the employees
    For Drivers, we need to identify the closest address based on the current location of the vehicle to minimize the distance travelled. So, once a package is dropped, he can immediately go to the closest package next.
    Shipping Time Estimation based on the data we have at hand (Working Hours, Holidays, Weekends etc.,)
    Getting signature from the Client, if Required
    Take the picture of the package, if it is left at the porch (Amazon is doing this to some extent)


    Was this a sorting system in a delivery center? So, packages are offloaded from trucks and the system would use cameras to read the addresses and sort the packages to the brownie truck for delivery? Or was the system more for drivers on the road to direct them to their destination? Please don't disclose any information you're not comfortable disclosing. I'm just trying to understand the problem.



    this is a very interesting question, depends on the level of detail, it could be really hard.
    I would thinks

    the source and destination address first need to to translated to a UPS distribution center id, this can be based on hash of zip code..
    finding the shortest path between two centers, some sort of BFS, also consider shipping priority.
    creating a timeline for this shipment, everytime stop in the middle, update the time line, this is for tracking ..
    when reach destination center, based on address, put it on a deliver route, deliver, then update the timeline.


--------------------------------------

UNIQUE ADDRESSES (TODO)
    Round 2: [Bar Raiser]

    Design a system to capture unique addresses in the entire world
    Answer: Discussed the overall system design, APIs, techniques for cleaning address data, 
    finding a unique address amongst a set of duplicates, processing, storing and retrieving it in an effecient manner.

    TODO REVIEW THIS SOLN:
    https://leetcode.com/discuss/interview-question/system-design/341980/Amazon-or-System-Design-or-System-to-capture-unique-addresses-in-the-entire-world



    This can be solved by using a QuadTree group a bunch of addresses in a grid. Let say we have a bunch of grids like a 1M. The problem is that in some areas like San Francisco, Tokyo, etc.. we will have a bunch of addresses in one grid. Like 3000 address in a radius of 100 miles for example. So this comes in handy the QuadTree per grid we can locate a max of 500 addresses if we reach our limit we divide our tree by four area and keep dividing until the grid has less than 500 addresses. The leaves of tree will be the the addresses located.

    Then we can hash every lat long to avoid duplication. Using our phone GPS and laptop, desktop geolocation we can get an approximation on those addresses and find them.
    
    If we search by keyboard we need an index table of words where it points to a list of lats and longs.



    My first question would be whether the system is supposed to generate these addresses or is this system supposed to source this data from external parties? Like could you query delivery companies like FedEx, DHL, UPS etc. and use their data to create one central repo of all the (explored) addresses in the world. The design you have mentioned above is generating this and keeping it, but what if the address changes like the name of the street changes or street is permanently shut down?

    Just a different pov.

        
    sOLNT 3:

        The interview could go a different way too. It might be the feature which Amazon.com has, which checks your address when you put in your address, it corrects/ changes it to something, and then asks, "is this the address". If that is the case, below is my attempt ( ).

        Once the address comes, try to map it to either an existing address, or create a new address

        FR:

        User enters their address, system stores their address uniquely.
        NFR:
        System needs to store all the addresses of the world. there are 7B people in the world,
        1B addresses.
        System should be highly available.
        Some latency is permitted.
        Follow up:
        System should suggest addresses after user puts in some part of the address.

        So how to do that?
        How to capture the address?
        Split up the address.
        Apt Type: Apt/Floor
        Apt Number: Can be Floor number, Apt Number

        House Number: Will be a number.
        Street : Will have the street Name
        Locality: Will have the locality name/County name, etc.
        City: Will have the city Name
        ZIP: ZIPCode
        State: State name
        Country Name: Country Name

        Now how to clean the data?
        What needs to be cleaned?

        Street name
        Locality name
        So, Street name and Locality name can have duplicates based on their actual locations.
         There can be the same named locality in
        n number of places.

        So we need to take care of that.
        The easiest way to deal with that is ZIPCode. It's unique, and uniquely defines a place in the world.
        Now we need to see if there is a similar locality in the same zipcode,
        if yes, then we add it to the same group. otherwise create new.
        For this we need to store groups of localities which are actually the same locality.
        For each ZipCode, all localities.

        {

        Zipcode1 :
        [
        Locality1: [Locality1, local1, loc1],
        Locality2: [l2, loc2]
        ]

        }
        When we get a new locality, we try to map it to any of the existing localities.
        if not getting mapped, we create a new entry under the zipcode.

        Same goes for street name and City name too.
        How to Store this data?
        Shard based on Country first, then Zipcode.
        We can even cache some of the popular zipcodes in the app server, so that we can quickly get the locality and street data names.

        Once we clean the data, we store it with the cleaned value, for eg: loc1, local1, Locality1 all would have "Locality1" as the address.
        We can save it with No SQL with sharding based on Country/ZIPCode.
        Let me know what you guys think?





----------------------------------------------------------------

Snapchat | System Design | Instagram Story Feature

This is a design question, not sure if I can post it here. But I would love to have discussion on this as this sounds an interesting design 
question given the feature that stories only need to be visible for 24 hours, which makes it different from desigining twitter/instagram etc.

I'd like to hear how you would approach the question if you are given this question.



Honestly I don't expect too much of a difference in the server backend for the Instagram stories feature, especially with how much tech they likely 
share with Facebook these days, although it does mean they get to aggressively move content from hot to cold storage.

Most of Instagram's uniqueness comes from their client-side features — they have those cool filters and stickers and typography features.












----------------------------------------------------------------
Facebook | System Design | E-commerce - Apply discount on every nth order


Design a system for a high-traffic e-commerce (e.g. Amazon, Walmart) so that on every N-th order, a predetermined promotional discount (e.g. 25%) would be applied to the total price in real-time (i.e. not done asynchronously such as applying the discount retroactively after purchase).

It’s across all users. It’s similar to how grocery stores sometimes have those promotions where the 100th customer wins something.


how about we precompute orderIds and then each time we pick the next unassigned one %100 ==0 apply the discount?
This sounds good in my opinion. But then there has to be a global order Id generator. How would you design that?
    -> does twitter have one its like tenant_id, rack_id, server_id + timestamp? UUID with 16 bytes

Few Thoughts :

What if instead of forcing N-th order site wide, we force it to be N-th order served by local server
[Method 1]

If we go strictly N-th order site wide, all server needs to check with some centralized service for discount or not. This centralized service can become bottleneck with heavy traffic so this wont scale.
advantage : strictly following N-th order site wide
disadvantage : can be a bottleneck since all servers are looking at centralized counter
[Method 2]

Alternative is to maintain counters per local server, so local server doesn't need to check with any other service to apply discount or not. It decides on its own. When counter reached certain value it should give apply discount and reset counter( mod operation).
advantage: all servers can independently decide to apply discount or not. No communication is needed with centralized discount counter service. Better scaling.
disadvantage: uneven load could cause discount to be not applied correctly. For example, lets say 2 servers and 100th order discount policy : If server 1 has served 99 orders and server 2 has served 99 orders, Ideally at least 1 customer should have received a discount. With this policy it wont happen. But with a busy website this should be fine. Overall number of discounts doesn't go down with this policy.

----------------------------------------------------------------

Design a coupon/voucher management system | DellEMC


The requirement is as below :-

10000 users are using the application at a moment.
There are 200 employees generating coupon at a moment.
When a user calls GetCoupon( ), a coupon is assigned to the user for 30 seconds.
Till the timespan of 30 seconds, this coupon will not be assigned to any other user.
If the coupon is not used by a user within 30 seconds, the coupon is marked as expired for the user and moved back to the pool so that it is available for other users for consumption.
If a coupon is used by a user, the coupon is marked as used and is not available again for sharing.
If all the coupons are exhausted or not available in pool for use, appropriate error messages has to be generated. This should disappear when :
a) If a new coupon is generated by an employee.
b) If an already existing coupon is not used by a user in 30 seconds.
There should be a failover option so that if one machine crashes, the system should still work.
The design should be horizontally scalable.
Any suggestions are appreciated.

TTL Cache With concurrency

----------------------------------------------------------------

    Design a monitoring or analytics service like Datadog or SignalFx


        Let's say there is a client which needs to monitor how many active users are there on his website at any point of time. He wants to monitor this and for that he uses our monitoring service. So, how would we design such a service?

        Assume there is an agent on every client node/machine which submits data to your analytics service.

        Functional Requirements

        The service would receive different types of events from clients and should be able to process each differently, and finally show it in separate graphs on the UI.
        The service should store data for the last 6 months, at least.
        Non-Functional Requirements

        The service should be highly available.
        There are 10000 nodes with 500M DAU. This will give you an idea of the volume of event data.
        Note:

        Do not concentrate on the UI side. Assume you have the UI (web client) where the user can select a particular metric and the type of graph to show the metric.
        The example of event data described here is just one example. There can various other kinds of data and can be considered out of scope of this question.
        A detailed design describing the different libraries or frameworks to use like kafka, cassandra, hadoop, etc. would be great.




    This is a perfect scenario to use kafka plus ELK stack.

    no the node site, light weighted agent to streaming events type of our interest, could be log in. log out etc, this can easily be get out from monitor the logs, then each event combine with node id, user id etc to streaming to a kafka topic based on event type.
    kafka bus shoulde be designed partition data based on user_id(can be discussed).

    on the other side of kafka, use logstash to process events based on topic and send it to elasticsearch, in a way the index should be created daily. also if we only want to store short period of data like 0.5 year, elastic search cluster should be fine, give that we should make replicas of index. if for a long term storage, we should use object storage like S3 or ?HDFS based storage to archive data. this can be done by a separate service directly feed of kafka.
    UI side, can directly use kibana to do some display, also using elasticsearch aPI, it is easy to do additional analytic work.







    each agent will report data to service, we choose rpc instead of http to reduce network overhead and improve communication speed.
    we need to save metric data of 10000 nodes for last 6 months, we need multiple graph instances to save data, use consistent hash for data distribution.
    we save metric data in rrdtool which is designed for time series data.

    soln 3:
        There are two pieces that I would use to build this. One is the event ingestion and the other is the aggregation piece.

        Event Ingestion
        Create an ingestion service which will be lightweight and will accept any type of event. The payload would have just a few fields (i.e. event type, datetime, userId). Additionally, you can specify an arbitrary number of "key, value" pairs that are associated with this event. After the event is been send to the event service it will be published in raw form into GCP Pub/sub.

        Event Processing and Storage
        Create a DataFlow Streaming job in GCP which subscribes to the raw topic which has all of the raw events from your Event Ingestion service. Using Spark or Hadoop combine all events by minute and by user. What will be generated is a time series of (datetime, user, num events). You can write the job to create by the second, minute, hour, date stats depending on how realtime you would like your dashboard for knowing how many active users you have. Write the contents of the rollups in Big Table or Hosted Cassandra or (Sylla).

        UI
        Create a service which can serve up a list of users and their events at a given time and granularity. Let's say that you want to show the number of users who triggered events at 11:00am and you want 59 "ticks" which will be a snapshot of (datetime, num users who submitted an event during the duration, time duration (i.e. 1 min)). The client will receive this and use it to build their dashboard.


----------------------------------------------------------------

    Design a file download application, handling all negative cases.
    Ex: How to efficiently restart download when network goes down, without redownloading whole file again
    Follow up to above question, How do you make sure data integrity is preserved of the downloded file.
    Give Low level and High Level design of different components.


    Use checksums, corruption handling.. 

    application/octet streams 

        Approach 2: Using Streams & Pipes
        Chunks and synchronize data transfer between client & server.

        // Node Js Code
        app.get("/download", async (req, res) => {

            const filePath = "test-file.txt";

            // around 200 MB
            const fileSize = fs.statSync(filePath).size; 

            res.header('Content-Type', 'application/octet-stream');
            res.header('Content-Length', fileSize.toString());
            res.header('Content-Disposition', `attachment; filename="test-file.txt"`);

            // create the stream
            const fileStream = fs.createReadStream(filePath);
            // pipe the data in between the file and response with 64 KB chunk size
            fileStream.pipe(res);

        });


    I got asked the same question but to do the downloads in parallel to efficiently use the bandwidth.

    Okay so start multiple download streams!
    different servers?

    @olponom The thing is, when you are using TCP, you don't need any hash computations. You can be sure that the data you received from the server is not corrupted.

    But by looking at your explanation about the Merkle tree, I think it is a good data structure when using UDP, but again, Nobody should use UDP for file transfers unless you have a strong reason and know what you are doing.

    And based on my experience, complicating things in the system design interview will make you lose points. But mentioning Merkle Tree with proper constraints and when it can be used can really earn us points.

    Maybe the Merkle tree is an interesting and powerful data structure, but there is no need to use it in this use case as TCP takes care of the integrity of the data for you(even for large scale of files), provided you have a secure connection.

    While reading chunks of data on the server side, read Nth chunk and calculate a hash on it (assuming it can not be done ahead of time). While sending the bytes to the app, the server places a header: hash=xxxxxxxx on each chunk returned. The app receives the bytes (chunk) and verifies the hash. This is needed because of potential network hiccup. Number of bytes received does not guarantee the correctness. Also the chunks could be requested in parallel and Merkle tree is used once in the end to verify integrity (checksum on chunk)

    @olponom I might be wrong, but TCP can handle the correctness of data based on what I know. But what you are suggesting can be used for UDP transfers.

    I never heard of Merkle trees, so I may have to go through it. But for requesting parallel chunks, I maintain multiple Hash Tables to keep track of "requested" and "received" chunks. This keeps 
    the implementation very simple, and I'm sure Hash Tables can handle huge numbers before the performance suffers.

    But if Merkle Trees are industry standard for performing these types of operations, then I may have to look at it and put it in my toolbox.

    @leetcode__id In nutshell comparing the hashes is O(N) operations (for each chunk). If a Merkle tree is given to you (precomputed on the storage server) you could find corrupted chunk in O(logN) 
    time traversing the tree. The root node of the tree is a hash for entire file, the child nodes are hashes for smaller chunks. This is very cool data structure, definitely worths mentioning on the interview, 
    I am guessing only practical if you deal with a large scale of files







    I have implemented a similar system previously.

    The request has the parameters like chunk number and max chunk size(optional) for the file. Initially, the chunk number would be 0.
    The server splits a file based on the max chunk size and uses file offset to skip the max chunk size in bytes * chunk number bytes in the file and returns the next max chunk size to the client.
    When the client receives the chunk, the client sends another request by incrementing the chunk number.
    Continue this till you get the confirmation from the server that no data is left to send. Similar to the pagination logic.
    Once all the data is received on the client-side, combine the chunks into one single file.
    If there is a problem with the ith chunk number due to network failure, try the download again after x seconds or by the click of a retry button.
    The high-level design would be the above explanation with some flow charts and UML diagrams.

    The low-level design would be Interfaces, Classes, and its methods if you are going OOPs way or appropriate function declarations if you are going functional way. I guess you can 
    come up with the classes and the methods which are capable of storing the state in properties and a method that can request the server and a method to combine everything.

    Hope it helps.

----------------------------------------------------------------    


A bloom filter can only answer with certainty whether something doesn't exist. If the product is not in the filter (miss), then it is definitely not flagged. But if a product is in the filter (hit), it may be either flagged or unflagged - you don't know for sure without checking with the underlying storage system.

A bloom filter is just a bunch of bits. You can encode them as e.g. Big/Little endian uints to a binary file and load it from there. Many languages also have built-in support for generalized binary encoding of an in-memory data structure (e.g. pickle in Python, or gob in Go), but it is usually less efficient than packing the bits yourself.

---------------
design analytics system:

Design an analytics system.

The input to the system is fed from another service and contains Personally Identifiable Information(PII) such as email,name etc..
The input comes in the form of an API request -
eg. { "email" : "abc@gmail.com",
"phone": 9888,
"name":John
}


The service should return the following metrics for the last 1 week, 1 month, 1 day and 2 yrs -
A). No. of requests with the given email-id ..
B). No. of requests with unique names for a given email-id. Some requests may contain same email-id but many different names and phone numbers. That is likely to come from a fraudster. These metrics help us in fraud detection.
C). Percentage of request with a given name of the total number of entries for the given email id.
eg, For email id - abc@gmail.com, there may be a total of 100 records out of which 50 come with the name John,20 with the name Robert, 30 with the name Daniel. So, for John, it will be 50%, Robert - 20% and Daniel - 30%

Note: You can assume that the data older than 2 yrs will be automatically deleted from our datastore.
**Questions : **
**1. What are the services ?do you need just one service - analytics service ?
Time Series DB ?
Can Oracle be used? NoSQL?
Can Kafka fit in somewhere?
Performing aggregation in the DB vs code. eg...
No. of requests with unique names for a given email-id. This query filters by email id and then does aggregation on it using the name(count(*)). Doing aggregation in the query vs offloading it to the application code. Pros and cons

sOL1: 
requirement is not clear.

if tps is high, no-sql may not support it since it has limit also (1k write TPS at dynamodb) we can use redis.
if cardinality of emailID or name is high, time series DB will not work well
redis key is: emailID-timeBucket, value is set of {name: cnt}.
so the data in redis could be:
abc@gmail.com-day:2/21/2024: {tom: 10, John:5}
abc@gmail.com-week:2/18/2024-2/25/2024: {tom: 100, John:50}
abc@gmail.com-month: Feb, 2024: {tom: 200, John:100}
abc@gmail.com-year: 2024: {tom: 300, John:150}
redis can support at least 100k TPS per node. If more TPS is needed, use cluster mode and more partition

SOL2:

    Design an analytical system
    Use Case
    input comes in as json, with fields for email, phone, name
    service needs to calculate
    number of requests for given email
    number of unique names for given email 
    same email but different phone numbers indicates fraud
    % of a given name, over all names
    service needs to show metrics for last 1 week, 1 month, 1 day and 2 yrs
    data older than 2 years can be truncated
    Questions
    What is the expected QPS 
    How many unique emails ? 
    This contains PII data, do we need to scrub data later (for GDPR) ? 
    Design Goals
    We need to filter data and detect fraud
    We need to support following queries
    select count(*) from T where email = E and date in D
    select countDistinct(name) from T where email = E and date in D
    select sum(if(email=E, count, 0)) / sum(count) as p from T and date in D

    This means we need an index on email and partition by date. if the database is slow (like postgres), 
    we need to pre-compute, so rollup old partitions by weeks or months. 

    Note: this question is more on building an analytics platform, and not a distributed systems question. 
    So focus in more on design the data ingestion > processing > query layers, and not much on scale

    High Level Design
    Pipeline
    Events would be ingested via kafka 
    Realtime processing would happen via spark-streaming
    Batch processing can happen via spark
    Storage could be postgres (relationalDB) or clickhouse (OLAP)

    Link to diagram: https://app.diagrams.net/?mode=google#G19bD6ABGFp1iGUkVkBVVW6DoLldi_vK1o


    Tables
    Raw Tables
    raw_events_realtime: same as raw_events but stores data only for the data in realtime

    Derived Tables
    filtered_raw_events: email, phone, name, request_id, filters
    filters could be [fraud, duplicates] etc
    processed_events: email, phone, name, count
    Implementation
    Option 1
    Stack: 
    Streaming: kafka, spark_streaming / flink, postgres, redis
    Batch: spark, postgres, s3

    Data comes into the system in realtime and lands into raw_events_realtime. Spark streaming / flink pushes data into postgres table every 5 min (can be tuned). This cache can not be for much longer as streaming memory is limited. Final correction would happen with batch pipeline

    At EOD, 
    data from raw_events_realtime is processed and gets landed into filtered_raw_events
    a downstream job removes filtered events and creates a partition into processed_events

    Detecting fraud 
    Realtime Path: have a cache of email <> [phone_number] for last N hours. spark-streaming can store 
    long watermarks using rocksDB (a persistent key value store with high fault tolerance)
    // Without watermark using guid column
    streamingDf.dropDuplicates("email", "phone")

    // With watermark using guid and eventTime columns
    streamingDf
    .withWatermark("eventTime", "10 seconds")
    .dropDuplicates("email", "phone", "eventTime")


    Batch: have a cache for the last N days for better accuracy. This can either be stored in S3 or loaded into a fast KV store (like redis, dynamoDB, etc)
    Handling scale
    Kafka topics needs to be properly tuned and sharded. Older daily partitions should be rolled up, the querying layer should accordingly query from the database. Doing aggregation like SUM is okay, but countDistinct is added work
    Disaster Recovery
    Raw events should be backed up into S3, while kafka should replay last N days
    Option 2
    Same as option 1, but use a better database like Clickhouse / Druid / Pinot that support real time ingestions and are crazy fast for OLAP use cases

    Realtime path stays same, events get added to latest partition
    At EOD, the partition is post processed using a batch pipeline, and corrected partition is swapped out

    Clickhouse table should be partitioned and sharded by hash(email) % N  if required
    Option 1 vs Option 2
    Option 2 is simpler. No need to do pre-computation or extra work to tie results across different partitions. Clickhouse would do the work for us. if we have to honor data scrubbing for PII fields, deletes gets complicated with Clickhouse. Although it supports deletes, but we need to set expectation on the number of deletes and their frequency
    Summary
    All design goals met. 
    Things can be further improved to honor data scrubbing

----------------------------------------------------------------

Hotstar || LLD || Staff Engineer (TODO)


design a recurring subscription service for hotstar.



----------------------------------------------------------------





----------------------------------------------------------------

Design a Payment System like Google Pay


https://github.com/savitansh/SystemDesignInterview/blob/master/GPay3.pdf

THIS REPOSTIORY IS GOOD IT HAS GOOGLE PAY, GOOGLE DRIVE, IMAGE EDITOR, NETOFICATIONS, RECOMEENDED PRODUCTS, TOP K HITTERS. 



----------------------------------------------------------------

Load balancing a migration: 
https://leetcode.com/discuss/interview-question/system-design/943352/Facebook-or-E5-System-Design-Interview-Question-or-Menlo-Park


Given a set of processes running on a cluster of hosts, design a system that load balances the hosts 
through live migrations of the processes. The goal of load balancing is to minimize or prevent resource 
starvation, a situation in which processes are not allocated the amount of resources they want to consume. 
In cases where all hosts in the cluster are overloaded, we want to distribute resources evenly across demanding 
processes. Given an imbalanced cluster, we want to bring it to a balanced state as soon as possible while minimizing the cost.

Assumptions:

To simplify the problem, each migration is the same cost.
Each process is long running with fluctiuating workloads.
Live process migration is carried out by an external service.

Here are my thoughts. Let me know what you guys make of it.

Core idea

Main things to identify

Determine the degree of busyness of a node
Migrate a queued/never started process (non preemptive)
Migrate a running process (preemptive)
Calculating CPU load

Sysinfo.h to find load
Read the /proc/stat file to know about jiffles, interrupts etc. to estimate load
Classify based on these signals the degree of busyness
Process migration policy
When to migrate:

Too many enqueued processes on a busy node
Too few running process on an underloaded node would want to acquire running processes from busy nodes
Which process to select?

Has least dependencies on other processes
Files opened
Basically, the one which has the least overhead
Migrating non-started processes is straightforward. Don’t start them and directly move them to underloaded node
For running processes, you need to save their state in a checkpoint table and move that

Process’s task_struct
Process’s mm_struct
Process address space (stack, data, code, heap etc)
Open files (regular files, pipes, sockets etc.) Signals
Current working directory and current root.
HLD

Central load balancer accepts request to run a processes and selects the best node for delegating the process
Each node in the cluster, send signals related to how busy they are, based on which LB decides where to forward the current process. This is done via a background thread
Node id, busyness => schema
Another background thread keeps a state of all the running processes within the node’s memory itself.
Process_id, files_opened, struct’s sizes, etc.
This can help in quickly identifying which process should be migrated
There could be another background service (running on individual nodes) which keeps monitoring if that node has been busy for too long. If yes, it contacts the LB to give it the best suited node to migrate a process. It should first select non preemptive processes, and then preemptive ones.
For better fault tolerance, each process can also keep updating its state in a persistent db. Keeping info like on which node are they running on, for how long have they been running, etc.
A background service can scan this table periodically for identifying processes that failed/stuck due to any number of issues, one of them being the node crashing (on which the process was running). Upon identifying them, it can retrigger them by calling LB.

----------------------------------------------------------------

API GATWAY:

I have to design an API gateway server with GraphQL API, I will be divide my server to 3 layer:

GraphQL API: Receive all request, handling authentication, handling loging
Bussiness logic: Handle all logic of our system as calling to external API, database (have many types of database, so need to abtract class to handle), caching query
Database: Handling insert/update/delete/query on database
With this system design above, do you think I can handle 1 million request per second?


scaling, metering, rate limiting, caching, authentication... lost of missing items :)


load balancer + Server Replication/RabbitMQ

you should also talk about scaling. For instance, what happens when you still receiving large amounts of request per second? Although 
you have a cache, A single cache server can only handle so much.

-----------------------------------------

Design Task Executor as Saas

https://leetcode.com/discuss/interview-question/system-design/385363/Design-Task-Executor-as-Saas


You are given a VM and tomcat server. Design a optimized Task executor system. Feel free to add or change anything. Question was completely open ended and interviewer were mostly looking for what different design solution(s).

Please share any thoughts on how to approach, happy to learn.

Pay close attention to the words -

VM (a resource limited server)
Tomcat Server (Concurrent Thread limited app server)
Optimize - Given the above mentioned restrictions you have to get the best out of this system.
Task - A single unit of work. Generally expressed as Threads.
Executor - System or class that takes up tasks and manage their lifecycle.
SaaS - One software many consumers.
What you should clarify with the interviewer -

Can we add more VMs? Could be yes or no.
How many tasks are we expecting per sec? could be a very large number. Design interviews are generally asked for large scale application.
How this system receive tasks? Likely through an API because it is a SaaS.
How would users submit tasks? Likely via API or UI.
How would consumer track their tasks? Likely through a centralized UI
Would work done in tasks involve interaction with other systems (API, DBs, etc)? If yes, tasks would be dependent on other systems for its execution. Adds latency.
Design of this kind of SaaS is very simple actually. You could simply build an API and have it running on Tomcat server. This API will accept tasks and pass it over to a Queue (Like RabbitMQ). Have separate instances of Task Executors that take up tasks from the queue and run them asynchronously. As tasks go through various stages you can attach a state to your task and store it somewhere in a DB exclusive to your system. You can also store all Task related information in the DB. If you enforce a protocol to building tasks then you can have more control over tasks and eventually you will be able to support actions like pause and resume tasks. This can be done simply by allowing users to divide a tasks in multiple stages. Whenever user pause a task you would complete the current stage and stop the execution. On resume, simply load up the task with details stored in the DB and start from where you left off.

You can do tonnes of other things in this system. These questions are often left open ended on purpose. Try to pay close attention to the interviewers words and constantly get feedback to make sure you are on right track. Interviewers feedback will also help you to understand which direction you want to take your system to. For instance, if your interviewer stress on Task Executor more then you can start explaining more on your ideas as to how task executor will handle all these tasks. How would you pick up right thread pool size. What happens when your system is overwhelmed by tasks and you need to put some kind of back-pressure on the queue system/task publishers.

Hope this helps.
---------------------------------------------------------------- COLD STORAGE:
 
 In system design for cold storage, particularly for database data, your goal is to offload rarely accessed data in a cost-effective way. Here’s a breakdown of how you could approach this:

1. Cold Storage Architecture
Separate Cold Storage Tiers: Typically, cold data is moved from high-performance databases to cheaper, slower storage solutions like object storage (e.g., Amazon S3 Glacier, Azure Blob Archive).
Hybrid Database Models: Some systems use a combination of hot and cold storage within a single architecture (e.g., Amazon DynamoDB’s tiered storage).

2. Data Formats and Compression
Serialized Formats: Common formats for storing database data in cold storage include:
Parquet/ORC: Highly optimized columnar storage formats for analytics.
Avro: A row-based format well-suited for long-term archival.
JSON/CSV: For simpler, more human-readable storage but not as efficient for large datasets.
Compression: Use formats like Gzip, LZ4, Zstandard, or Snappy to compress data before storage to reduce costs and improve retrieval times.

3. Database Systems and Cold Storage Compatibility
SQL Databases (e.g., MySQL, PostgreSQL): Export infrequently accessed tables or partitions as flat files (CSV, Parquet) for archival. You can also leverage external tools like pg_dump and mysqldump.
NoSQL Databases (e.g., MongoDB, Cassandra): Export collections as BSON or JSON for cold storage. You can also use tools like mongodump or Apache Spark to convert to more space-efficient formats like Parquet.

4. Data Partitioning and Tiering
Time-Based Partitioning: Partition your database by time (e.g., monthly/quarterly), then periodically migrate older partitions to cold storage.
Automatic Tiering: Some managed database services (like AWS RDS with S3) offer automatic tiering based on data access patterns.

5. Metadata Management
Track metadata for cold-stored data, ensuring quick access when needed. This includes timestamps, storage location, and indexing for fast retrieval.

6. Retrieval Considerations
Retrieval from cold storage can be slow and should be designed for batch processing (e.g., restore large datasets from S3 Glacier).
Use staging areas for rehydrating cold data into your database or analytic pipelines.
Example System Design Flow
Identify Cold Data: Use access patterns to identify data not queried in the last year.
Export Data: Convert the identified data into a compressed, space-efficient format like Parquet.
Move to Cold Storage: Store the data in a cold storage service (e.g., S3 Glacier).
Index and Metadata: Maintain a lightweight metadata store in your main database for quick access.
Example Use Case
For a time-series database like InfluxDB or a log management system (e.g., Elasticsearch), you might migrate old shards or indices to a cold storage layer using tiered storage.

This approach not only reduces storage costs but also ensures you can scale without sacrificing query performance on hot data.


Parquet is designed for efficient storage and retrieval, especially for analytics workloads, but querying it directly isn’t as straightforward as querying a traditional database. Here’s a breakdown:

1. Is Parquet Easy to Query?
    Optimized for Analytics: Parquet is a columnar storage format optimized for read-heavy operations, such as aggregations and filtering, which makes it well-suited for analytical queries.
    Tools Required for Querying: To query Parquet files, you typically need to use tools or frameworks such as:
    Apache Spark: Widely used for distributed querying of Parquet files.
    Apache Drill: Allows querying Parquet files using SQL without requiring a separate ETL process.
    Presto/Trino: Distributed SQL engines that can query Parquet data in place.
    Hive/Impala: Can be used to query Parquet data stored in HDFS.
    Python/Pandas: For smaller datasets, Python with Pandas or PyArrow can load and query Parquet files.

2. Can You Query Multiple Parquet Files at Once?
Yes, You Can: Parquet supports splitting data across multiple files (e.g., partitioned by date or other criteria), and most query engines (like Spark, Hive, or Presto) can scan and query multiple Parquet files in parallel as if they were a single dataset.
Partition Pruning: Tools like Hive or Spark can automatically prune partitions based on the query, limiting the files scanned for faster execution.

3. How It Works in Practice
Distributed Query Engines: When querying multiple Parquet files, these engines split the workload across nodes in a cluster, reading different Parquet files in parallel. The columnar format ensures that only relevant columns are loaded into memory, making queries efficient.
Example Scenario: Imagine you have a dataset partitioned by date and stored as multiple Parquet files (e.g., one file per month). A 
query engine like Presto can run a SQL query that targets specific months, automatically filtering and combining relevant files.

4. Best Use Cases for Parquet
Big Data Analytics: Parquet is ideal for large-scale analytics where you need to perform aggregations, filtering, or slicing on columns rather than rows.
Data Lakes: In data lakes (e.g., on AWS S3), Parquet files are commonly used for storing massive datasets in a way that’s still queryable using tools like Athena or Redshift Spectrum.

Limitations
Parquet is not ideal for OLTP (transactional) workloads or scenarios needing frequent updates.
You generally need external tools or engines to interact with Parquet data, unlike directly querying a traditional database.
In summary, Parquet is excellent for querying large datasets efficiently, especially when using the right tools that can handle multiple files.


----------------------------------------------------------------

Airbnb | System Design Round | Airbnb Wallet (todo)

    Design airbnb wallet.
    Features -

    User can see wallet balanceON
    user can see transaction history.
    -User can transfer money between wallet and his bank account
    APIs -
    vendor_transfer_id = create_transfer(account_number, routing_number, transfer_type, amount)

    get_transfer_status(vendor_transfer_id)

    Explained the design, he was more interested in what all services will be there, scalability, caching, reducing latency, Atomic transactions etc.

I got this in my airbnb interview recently. I discussed about system having high availability 
and discussed consistency in terms of the ACID properties of SQL. 

The interviewer asked me to do estimates of QPS and storage based on 10M DAU. I estimated 3M transactions per 
day(30% of the Airbnb DAU) which brings to about 12*3 = 36 transactions per second for the make payment api. 
Then I estimated the storage 1Kb per record * 3M records per day=3GB/day. This is a ever growing data which will be costly to 
store in a SQL database so I proposed to use a NoSQL database for the archival(storing records older
than a fixed time, say 6 months). Interviewer seemed happy with the choices made.

They mentioned that the vendor api which we are calling to process a payment might take hours, so I mentioned a async polling based architecture, where we return a url to the client to poll to find the status of a transaction.

The intreviewer also discussed about the failure cases, what happens if any of the services in the system went down. I discussed about using Kubernetes which can restart the pods or using a Active-Passive failover model.

----------------------------------------------------------------

AD AGGREGATIONs:

Particular ad created on an ad network are shown on mutliple clients with a traffic of 200 Billion impressions per day.
Design a system that can store ad impressions per ad id and can aggregate information over a day per ad id.

The information should be accurate within 30 sec, meaning when a query is made, the data 30 sec ago should be accurate. This data needs to be kept for 2 years.


Harman: 

    Store in C* 
    Use kafka to store impressions and write to C*
    PARTITION BY ad id 

    Have a daily job that aggregates and stores in per day ad id C* table .
    
    leverage apache pinot for the 30 second accruacy. 
    Also store impressions in Redis cache for 30 second accuracy with ttl of 30 seconds? 

    Or use flink to aggregate in a window for every 30 seconds, then write these counts per ad_id to Redis.. 
    Redis uses redis file storage and checkpoints for resilliancy? 
    
    Rebuild redis view by querying all data for ad ids historically, and then combine with flink? 
    or.. 
    save the events aggregated by 30 seconds to kafka -> and have redis get rebuilt... when it fails from 
    a service that writes history data and then combines with events in kafka or something... 




    
    Apache pinot creates segments that you can then query with apache pinot for a particular ad id. 




    Question is not clear. You want only day-wise aggregate or real-time reporting also. Real-time reporting is extremely difficult to be accurate especially because of netword/data delays and spikes in traffic, spam, distributed logging over different regions.

    push all events to kafka, move all the data to hadoop partitioned on some timestamp(hourly, daily), run spark jobs to produce materialized views and also ingest the data to druid like OLAP db.

    for providing realtime reporting you can run structured streaming on kafka and temorarily generate the materialized view and insert in database indexed on ad id.




---------------------------------------------------------------- LEVERAGEING KUBERNETES IN SYSTM DESIGN OF IG


Using Kubernetes (K8s) in a system design problem like designing Instagram involves leveraging its container orchestration capabilities to ensure scalability, availability, and reliability. Here’s how Kubernetes can be applied, along with handling failover:

1. Microservices Architecture
Instagram consists of multiple microservices (e.g., user management, posts, feed generation, notifications). Kubernetes allows you to deploy and manage each of these microservices as individual pods, enabling easy scaling, deployment, and monitoring.

2. Scaling
Horizontal Pod Autoscaling (HPA): K8s can automatically scale microservices based on CPU, memory usage, or custom metrics (like request latency). This ensures that services can handle varying loads (e.g., spikes when a post goes viral).

Load Balancing: K8s Service resources provide built-in load balancing to distribute traffic across pods, improving performance and fault tolerance.
3. Failover and High Availability
Kubernetes handles failover using the following features:

Replication Controllers and StatefulSets: These ensure a desired number of pod replicas are always running. If a pod fails, Kubernetes automatically reschedules it on a healthy node.

Multi-Region Deployments: K8s can be deployed across multiple clusters in different regions, ensuring that even if one region goes down, the application remains available.

Leader Election: For services that require a leader (like a primary instance in a database), Kubernetes can handle leader election using patterns like ConfigMaps or leveraging tools like etcd.

4. Service Discovery and Routing
Kubernetes makes it easy to discover services using built-in DNS. This allows microservices to communicate without worrying about underlying infrastructure changes.

5. CI/CD Integration
Kubernetes supports rolling updates and canaries, allowing seamless deployment of new features without downtime. This is key in a dynamic platform like Instagram, where frequent updates are made.

6. Handling Data Consistency and Failover
For components like databases, Instagram’s architecture requires careful consideration:

Stateful Workloads: Use StatefulSets with persistent volumes for databases (e.g., PostgreSQL, Cassandra) that maintain data consistency.
Data Redundancy and Backup: Utilize multi-region setups with tools like Velero for backup and recovery. Kubernetes operators can manage complex databases, ensuring data consistency across failovers.

7. Monitoring and Observability
Tools like Prometheus and Grafana, integrated with Kubernetes, help monitor service health, track metrics, and alert in case of failures. This enables proactive resolution of issues before they impact users.

Conclusion
Kubernetes provides the necessary orchestration, scaling, and failover capabilities needed to design a complex, high-traffic system like Instagram. Handling failover through replication, multi-region deployments, automated leader election, and self-healing mechanisms ensures high availability and resilience even in the face of failures.




--------------------------------


Ticketmaster: 

https://www.hellointerview.com/learn/system-design/answer-keys/ticketmaster
(Review this video asap.)


Approach
Add Elasticsearch or a similar full-text search engine. Elasticsearch is a powerful search engine that excels in full-text search, complex query execution, and handling high-volume traffic efficiently. At its core, Elasticsearch operates using inverted indexes, a key feature that makes it highly efficient for search operations. Inverted indexes allow Elasticsearch to quickly locate and retrieve data by mapping each unique word to the documents or records it appears in, significantly speeding up search queries.

To make sure the data in Elasticsearch is always in sync with the data in our SQL DB, we can use change data capture (CDC) for real-time or near-real-time data synchronization from PostgreSQL to Elasticsearch. This setup captures changes in the PostgreSQL database, such as inserts, updates, and deletes, and replicates them to the Elasticsearch index.
We can enable fuzzy search functionality with Elasticsearch, which allows for error tolerance in search queries. This is way we can handle typos and slight variations in spellings such as "Taylor Swift" vs "Tayler Swift". This is something that would be very difficult to do with SQL alone.
Challenges
Keeping the Elasticsearch index synchronized with PostgreSQL can be complex and requires a reliable mechanism to ensure data consistency.
Maintaining an Elasticsearch cluster adds additional infrastructure complexity and cost.

----------------------------------------------------------------

Design a system that can handle hourly backups for mobile phones

    Question Description:

    You are in charge of designing a system that can handle regular backups for mobile devices. For the purposes of this problem, you should assume that users are storing a relatively large amount of data, though a lot of this data is not changed between backups. You should also assume that your users travel frequently, and your system should be able to deal with users moving between regions

    Solution:

    The problem is fairly open ended, but you should be able to touch on a few key points here. First, since sending data over cellular networks is expensive and slow, some form of deduplication should be used to reduce the amount of data that needs to be sent over the network and stored on your servers. Second, you should discuss how you would handle users who travel between regions. Will you send all of their data to their "home server", or use some sort of consistency protocol to maintain a global set of backups? Do you need strong consistency here, or can your system handle some amount of inconsistency? You should discuss read/write trade-offs here too. Users will not need to restore a backup very often, so sacrificing read performance is probably a reasonable design choice.
    
    chatgpt
        Problem Analysis and Requirements
        Data Characteristics: Users store large amounts of data, most of which is unchanged between backups.
        User Mobility: Users frequently travel, requiring backups across geographically distributed regions.
        Network Constraints: Cellular networks are slow and costly, so minimizing data transfer is crucial.
        Consistency Requirements: The system should balance consistency, latency, and network efficiency while considering trade-offs in read/write operations.
        2. Key Design Considerations
        Data Deduplication: Deduplication can significantly reduce the amount of data transferred and stored. Options include:

        Block-level Deduplication: Divides data into blocks and only transmits changed blocks (content-based chunking). This can be implemented using hash-based approaches like SHA-256 to identify identical blocks.
        Client-side Deduplication: The client calculates hashes for data blocks and only sends the differences.
        Compression: Apply compression algorithms to further reduce data size before transmission.
        Efficient Data Transfer Protocols:

        Delta Encoding: Send only the changes (deltas) between the current backup and the last backup, reducing network load.
        Protocol Selection: Use a protocol like rsync (or a custom variation) for differential syncing.
        3. Regional Consistency and Data Distribution
        Geo-distributed Data Storage:

        Multi-Region Storage Strategy: Store data in regional datacenters close to the user’s primary region. This minimizes latency and supports faster incremental backups.
        Home Region Concept: Each user has a designated "home region" where their full backup is stored. When they move regions, data is synchronized back to the home region.
        Consistency Models:

        Eventual Consistency: For backups, eventual consistency is sufficient since read operations (restoring backups) are infrequent. The system can tolerate small delays in syncing backup data across regions.
        Write Path Optimization: Optimize for write performance by storing incremental backups in the nearest region and asynchronously syncing to the home region.
        Metadata and Indexing:

        Store metadata (e.g., hashes, timestamps) globally accessible to track versions of the data.
        Use a distributed metadata service to ensure that any region can quickly determine what data needs to be uploaded or synchronized.
        4. Handling User Mobility
        Cross-Region Data Syncing: Implement mechanisms to detect user mobility and decide when to sync data across regions:

        When users connect to a new region, the backup system should check for changes and either pull or push data incrementally.
        For efficiency, this syncing can be prioritized during idle times, reducing impact on user experience.
        Caching and Latency Reduction:

        Use local caching to store frequently accessed data and reduce cross-region requests.
        Employ CDNs or edge caches for backup retrieval in cases of data restoration.
        5. Scaling Considerations
        Distributed Systems Architecture:

        Use a sharded and replicated database (like Cassandra or DynamoDB) for metadata storage, allowing the system to scale horizontally.
        Leverage object storage (e.g., S3-compatible systems) for cost-effective, scalable data storage.
        Fault Tolerance and Reliability:

        Implement redundancy and data replication across multiple availability zones to ensure durability.
        Incorporate checksums and error-detection mechanisms to handle data integrity issues.
        6. Trade-offs and Performance Tuning
        Read vs. Write Optimization:
        Prioritize write performance since backup writes will be frequent. Sacrifice read performance since restores are less common and can tolerate higher latency.
        Compressing data can add compute overhead but reduces network usage, a critical trade-off in mobile contexts.
        7. Security and Privacy
        Data Encryption: Encrypt both in-transit and at-rest data to protect user information.
        Access Controls: Implement user authentication and fine-grained access controls to manage backup operations.
        8. Monitoring and Observability
        Implement detailed logging and monitoring to track backup success rates, data integrity, and regional sync performance.
        Use metrics like data deduplication ratios, latency in syncing between regions, and storage utilization for capacity planning.
        Conclusion
        This design addresses the core challenges by leveraging deduplication, delta syncing, and eventual consistency across a multi-region setup. It prioritizes write operations for efficient backups while managing regional data consistency asynchronously. The system scales horizontally, ensuring reliability and fault tolerance, while minimizing the impact on cellular network usage.



--------------------------------

System design to backup datacenter1 to datacenter 2

    Hi All,

    Today I had an Interview with one of the product companies. Below is the scenario they have asked.

    There is a DataCenter in HongKnog, Due to current protests in the country they want to backup/copy the entire servers data in the data center to a different location in FrankFurt.

    For SImplicity he asked me to design a solution to backup one Server in HongKong datacenter to other server in FrankFurt DC.

    The server will have Peta Bytes of data
    It could contain Databases, files, Images etc.,
    While transferring Data transactions should not be lost like there could be data related to CreditCards, banking etc


    My Solution:
    I said we could have different services that will read data from the server and send data to a HashMap in between to prevent loss of data since we are sending in HTTP Traffic.

    The interviewer asked more detail design, all the component, services involved in this design which I could not answer.

    Could Someone point to a right thoughts.


Harmans Solutin:

    Will we have to make sure that while we are transferinng data, that DC2 stays in sync with DC1 in terms of new writes while we are copying?

    
    NFR:
        durability and no loss of writes 
        Data is available at all times


    HLD:

        We can take a snapshot of all the data, compress it, and save it in S3. 
        Then we can setup kafka and ensure all writes get written to KAFKA. 

        We will replay all the writes from kafka to our database, files, images, etc. .

    CHAT GPT SOLN:
        Problem Breakdown
        Data Consistency: Ensuring that no transactions are lost during the transfer.
        Data Volume: Handling petabytes of data across a large geographic distance.
        Data Types: Support for various data types, including databases, files, and images.
        Latency and Bandwidth Constraints: Managing the network challenges inherent in cross-continental data transfers.
        Data Integrity and Redundancy: Ensuring data integrity while considering redundancy.
        Fault Tolerance and Recovery: Designing for high availability, fault tolerance, and disaster recovery.
        High-Level Architecture
        Data Categorization and Prioritization:

        Break down data into categories based on sensitivity, frequency of access, and type (e.g., transactional databases, static files, or logs).
        Prioritize transactional and critical data (like credit card details) for synchronous replication.
        Synchronous vs. Asynchronous Replication:

        Synchronous Replication: For critical and transactional data that requires consistency (e.g., database records), ensure synchronous replication between the data centers. In this approach, every write operation in Hong Kong must be acknowledged by Frankfurt before it is considered successful.
        Implementation: Use distributed consensus algorithms (e.g., Paxos or Raft) to maintain consistency across data centers.
        Challenges: High latency can cause performance degradation, so limit this approach to high-priority data only.
        Asynchronous Replication: For non-critical data (e.g., static files, images), use asynchronous replication, where data is transferred in batches without waiting for acknowledgments from Frankfurt.
        Implementation: Tools like rsync, distributed object storage systems (e.g., Ceph, MinIO), or change-data-capture (CDC) solutions like Debezium can be used.
        Data Transfer Mechanism:

        Bulk Data Transfer: For initial seeding (petabytes of existing data), consider bulk transfer solutions:
        Physical Transfer: Ship physical storage devices (e.g., AWS Snowball) from Hong Kong to Frankfurt to avoid network constraints.
        Network Transfer: For incremental changes after the initial seed, use WAN-optimized data transfer protocols (e.g., Rsync over WAN, SCP with compression).
        Incremental Backups: Implement incremental backups after the initial full backup to reduce data transfer overhead.
        Data Pipeline Design:

        Implement a pipeline with:
        CDC (Change Data Capture) for real-time data streams, capturing only changes (insert/update/delete).
        Queueing Systems (e.g., Kafka, RabbitMQ) for buffering and ensuring ordered delivery.
        Stream Processing (e.g., Apache Flink or Kafka Streams) to filter, transform, and validate data before replication.
        Data Consistency and Conflict Resolution:

        Implement mechanisms like two-phase commits (2PC) or eventual consistency strategies for handling conflicts or inconsistencies during asynchronous replication.
        Network Optimization and Compression:

        Compression: Apply data compression techniques (e.g., gzip, LZ4) to reduce transfer size.
        Deduplication: Use deduplication to avoid sending redundant data.
        CDNs and Edge Caches: Leverage edge servers or CDNs to cache static content closer to Frankfurt if applicable.
        Fault Tolerance and Monitoring:

        Set up active monitoring and alerting (e.g., Prometheus, Grafana) to monitor replication health and network issues.
        Implement retry logic with exponential backoff for failed transfers and ensure automatic failover between replication channels.
        Disaster Recovery Strategy:

        Implement a hot standby in Frankfurt that is ready to take over in case of a failure in Hong Kong.
        Regularly test failover procedures to ensure they work as expected.



        LLD Breakdown
        1. Data Categorization
        Critical Data: Database transactions, financial records, etc. Requires strong consistency.
        Non-Critical Data: Images, logs, archives, etc. Can be replicated asynchronously.
        2. Components and Services
        The system will have several key components:

        Change Data Capture (CDC) Service: Captures database changes in real time.
        Data Processor/Filter: Filters and transforms CDC streams before replication.
        File Sync Service: Handles large-scale file replication asynchronously.
        Message Queue (MQ): Acts as a buffer for transactional data (e.g., Kafka).
        Data Transfer Orchestrator: Manages bandwidth, prioritization, and error handling.
        Data Storage: Distributed file storage systems like Ceph, or cloud-based storage like S3.
        Monitoring and Alerts: Tools like Prometheus/Grafana.
        3. Database Replication (Critical Data)
        Critical transactional data needs to be synchronized with minimal latency.

        Design Flow:

        MySQL/PostgreSQL Streaming Replication:

        Use semi-synchronous replication to ensure that a transaction is acknowledged by the Frankfurt data center before being committed.
        This is achieved through write-ahead logs (WAL) in PostgreSQL or binlog replication in MySQL.
        The database writes are propagated in real time using an enhanced replication protocol.
        Debezium (CDC Layer):

        Debezium reads changes from the WAL/binlog and pushes them to Kafka.
        Kafka guarantees message ordering and fault tolerance with configurable retries.
        Stream Processing (Kafka Streams/Apache Flink):

        Stream processors filter, transform, and validate data in the Kafka topic before it’s sent to the backup data center.
        Use data serialization formats like Avro or Protobuf for efficient data transport.
        Replication Channel:

        The stream processor pushes validated data to the Frankfurt data center over a WAN-optimized channel.
        Use gRPC with TLS for secure data transport.
        Error Handling:

        If Frankfurt’s acknowledgment isn’t received within a timeout, the system retries the transmission with exponential backoff.
        Implement conflict resolution strategies (e.g., last-write-wins, vector clocks) for concurrent updates.
        4. File and Asset Replication (Non-Critical Data)
        This data can be transferred asynchronously to reduce the load on the network.

        Design Flow:

        Rsync-based Replication:

        Implement Rsync (or a similar tool) with compression and deduplication enabled for efficient file transfer.
        Batch operations at regular intervals (e.g., hourly or daily) to transfer file deltas.
        Distributed Object Storage (e.g., MinIO, Ceph):

        Store non-critical files in an object storage system with erasure coding for redundancy.
        Use cross-region replication features provided by these storage systems to automatically sync data to Frankfurt.
        Event-Driven Sync:

        For frequently changing files, use an event-driven model where file updates trigger a sync event (e.g., via a message queue).
        5. Bulk Data Transfer for Initial Seeding
        For the initial migration of large data volumes:

        Option 1: Physical Transfer:

        Use AWS Snowball or similar appliances to physically ship storage devices to Frankfurt.
        Once seeded, initiate incremental updates via the network.
        Option 2: High Bandwidth Transfer:

        If physical transfer isn’t feasible, use WAN accelerators and high-bandwidth links to perform the initial transfer.
        6. Data Transfer Orchestrator
        Handles synchronization of different data types while optimizing for network constraints.

        Orchestrator Flow:

        Bandwidth Management:

        Dynamically allocate bandwidth based on data priority (e.g., databases get higher priority).
        Use traffic shaping and QoS policies to minimize latency for critical data.
        Multi-Channel Transfer:

        Implement multi-channel transfer with dedicated channels for different data types (e.g., one for databases, one for files).
        Each channel has its own fault tolerance and retry logic.
        Failure Handling and Retry:

        If a sync operation fails, log the error, trigger an alert, and retry with exponential backoff.
        Use checkpoints to resume large file transfers from the last successful chunk.
        7. Monitoring, Alerts, and Observability
        Monitoring:

        Prometheus collects metrics on replication health, latency, and bandwidth usage.
        Grafana dashboards display real-time metrics and alert thresholds.
        Alerting:

        Set up alerting for replication failures, high latency, or network saturation.
        Audit Logs:

        Maintain detailed logs for all replication actions (e.g., successful syncs, retries, failures) for audit and debugging purposes.
        8. Disaster Recovery (DR) Strategy
        Frankfurt acts as a hot standby. If Hong Kong goes down:
        Frankfurt takes over as the primary data center with minimal switchover time.
        Implement automatic failover using tools like Pacemaker or custom scripts.
        9. Edge Cases and Performance Tuning
        Network Partition: Implement split-brain detection and recovery mechanisms (e.g., fencing or quorum-based decision making).
        Data Validation: Periodically validate replicated data (e.g., hash comparison) to detect corruption.
        Compression and Deduplication: Enable adaptive compression and deduplication to optimize network usage for large files.
        10. Security
        Use TLS encryption for all in-transit data.
        Enable data-at-rest encryption using solutions like LUKS or cloud-native encryption.
        Regularly rotate encryption keys and ensure compliance with data protection regulations (e.g., GDPR).
        Diagram (Conceptual)
        A diagram would show:

        Data Centers in Hong Kong and Frankfurt.
        The CDC pipeline from the database to Kafka, and then to the stream processor.
        Rsync services for file transfer.
        The Data Transfer Orchestrator coordinating different channels.
        Monitoring and alerting layers.
        
        ALTERNATIVE LLD SOLUTION:

        How rsync Works
        rsync is a powerful and widely used utility for syncing files and directories between two locations while minimizing data transfer by only sending the differences (deltas) between source and destination. It’s especially effective for handling large datasets with relatively small changes.

        Key Features of rsync:

        Delta Transfer Algorithm: rsync compares file chunks between the source and destination, sending only the differences (not entire files) to minimize bandwidth.
        File Comparison: It compares files using checksums, file size, and timestamps.
        Compression: Data can be compressed during transfer to further reduce bandwidth.
        Bandwidth Control: The --bwlimit option allows limiting the transfer rate to avoid saturating the network.
        Efficient Handling of Large Datasets: It can handle large files, directories, and deep directory structures with millions of files.
        Basic Workflow:

        File Scan: The source system scans the files and directories to be synced.
        Comparison and Chunking: rsync splits files into blocks and compares these blocks using checksums (MD5, rolling checksum) to detect differences.
        Transfer Only Deltas: Only the changed chunks or files are sent over the network.
        Destination Sync: The destination system reconstructs the updated files using the received deltas.
        Use Cases:

        Periodic backups across data centers.
        Syncing static content (e.g., images, videos).
        Syncing directories containing large datasets with minimal changes.
        Limitations of rsync
        Inefficient for Real-time Data: It’s designed more for periodic bulk syncs than for real-time, event-driven data replication.
        Performance: For extremely large datasets (petabytes), or millions of small files, rsync can be slow during initial syncs.
        High Latency and Long Distance: Network latency impacts performance during checksum verification and transfer.
        Alternative Solution: Distributed Object Storage with Cross-Region Replication
        A more scalable and modern alternative is to use distributed object storage systems like MinIO, Ceph, or cloud services like Amazon S3. These systems support built-in cross-region replication, ensuring that data is automatically and asynchronously replicated across data centers.

        LLD for Cross-Region Replication using Object Storage
        Components:

        Object Storage Service: MinIO or Ceph deployed in both data centers.
        Replication Manager: Controls data replication policies and schedules.
        Metadata Service: Handles object metadata (e.g., versioning, timestamps) and keeps both data centers in sync.
        Data Transport Layer: Handles secure, compressed, and optimized data transfer between regions.
        Monitoring and Alerts: Tracks the health of replication, transfer status, and issues alerts if there’s a lag.
        Architecture:

        Both the Hong Kong and Frankfurt data centers host MinIO or Ceph clusters.
        Data objects (e.g., files, images, logs) are stored as key-value pairs within these clusters.
        Cross-region replication is configured to replicate all objects asynchronously. Only changed objects or parts of objects are transferred.
        Replication Workflow:

        Data Ingestion: Data is uploaded or written to the object storage system in the Hong Kong DC.
        Metadata Tracking: The replication service detects changes (new objects, updates, deletions) based on metadata like version IDs and timestamps.
        Data Sync: The replication engine sends changed objects (or object deltas) to the Frankfurt DC using an optimized protocol (e.g., gRPC with compression).
        Consistency Check: A periodic consistency check verifies that both data centers are synchronized.
        Advantages:

        Scalability: Designed to handle petabytes of data with millions of objects.
        Built-in Redundancy: Data is automatically distributed across multiple nodes, reducing single points of failure.
        Event-driven Sync: Changes are detected in near real-time and pushed to the backup DC.
        Fault Tolerance: Supports versioning, erasure coding, and automatic failover.
        Challenges:

        Network Latency: Asynchronous replication may introduce lag, though eventual consistency is typically acceptable for non-critical data.
        Initial Data Seeding: The first full sync can still be time-consuming, similar to rsync.
        LLD Breakdown for Cross-Region Replication
        1. Data Ingestion Layer:

        Services: S3-compatible APIs (e.g., MinIO Gateway) for uploading files.
        Configuration: Enable versioning and lifecycle policies to manage object states and deletions.
        2. Metadata Service:

        Responsibilities: Track object updates, version IDs, timestamps, and ensure both clusters have consistent metadata.
        Tools: Use Redis or etcd for fast, consistent metadata storage and lookup.
        3. Replication Manager:

        Policies: Define what data to replicate, how frequently (e.g., near real-time or in batches), and with what priority.
        Event Triggers: Integrate with MinIO’s event notification system or Ceph’s radosgw to trigger replication when an object changes.
        Transport: Use gRPC or HTTP/2 with TLS for secure transfer.
        4. Data Transfer Layer:

        Compression: Enable LZ4 or zlib compression to optimize bandwidth usage.
        Deduplication: Only transfer object deltas using differential algorithms like rsync’s block checksums.
        5. Monitoring and Health Checks:

        Tools: Prometheus for monitoring, Grafana for visualization.
        Alerts: Set alerts for replication lag, network issues, or storage failures.
        6. Fault Tolerance and Disaster Recovery:

        Use erasure coding for high redundancy and fault tolerance.
        Implement regular validation of data integrity (hash comparison) across regions.
        Conclusion
        For large-scale, cross-datacenter replication, using a modern distributed object storage system with built-in replication capabilities is often more effective than rsync. It provides better scalability, redundancy, and can handle complex data types more efficiently. This approach is well-suited for petabyte-scale environments and offers improved reliability and fault tolerance in large-scale distributed systems.


--------------------------------

Dropbox

https://www.hellointerview.com/learn/system-design/answer-keys/dropbox

file upload
multipart

chunking on client
Caclualte fingerprint, and checksum of files for upload and confirm they are uploaded. 

status of chucks uploading in file metadata db. 

Blob storage ->> s3 notifications -> chunk uploaded -> update file metadata service. 

S3 has multipart upload. 

Resumable uploads by uploading metadata for entire upload first and save in db, before client uploads each chunk. 

Compression, gzip, gzip on media and photos isnt that great, but compressing text files can compress very well. -> 5gb to 1Gb

Syncing remote change and local change. (if oyu change file in laptop and phone)
Sync fast.
Changes are remote -> refelected in local folder, and local folder to repote 

Client polls to see if changes have been made in remote folder vslocal folder ()

1. Remote changed -> pull for changes, download the new file and replace
2. locat chnaged -> upload the changed file to remote. 

 Sync has to be fast
 sync has to be consistent. 
 
ONLY SYNC locally the changed chunks not all the chunks. 

include updatedat field for every chunk for the file 

This is called delta sync. -> client app stiches it together. 

Adaptive polling. 


Dropbox has event bus with cursor. -> any change to files -> file change event on bus -> Folder has sync cursor. -> put changes in event bus. 
Replay all changes to update local folder. -> audit trail, rollback changes using event bus. 

Event bus you can consolitate events and create snapshot points to recover things faster and update local state. 


Sync service handles syncs. 

File service. 
----------------------------------------------------------------

Design a system similar to the Google's feature "People also searched for". (TODO)

    How this featuer work:

    Google any celebrity's name.
    In the search result, on right hand corner there will a list of profile which would be similar to the one which was googled.


    Its an open ended questions. First thing is you will have to give suggestion based on geo sharding. In that particular shard, 
    you can build a cache on it which will keep count of hot topics/searches.


    ChatGPT solution: 

            To design a system similar to Google’s "People also searched for" feature using Elasticsearch, the process involves leveraging Elasticsearch's capabilities for information retrieval, text analysis, and query expansion. Below is a step-by-step outline of how Elasticsearch can be integrated into such a system:

            1. Data Collection
            Source Data: Gather data on celebrities, such as names, professions, relationships (e.g., co-stars, collaborations), and other relevant metadata (movies, awards, etc.).
            Profile Representation: Each celebrity's profile can be represented as a document with various fields, such as name, occupation, and associated entities (e.g., movies, related celebrities).
            2. Data Indexing in Elasticsearch
            Store each celebrity profile as a document in Elasticsearch. Each document might include fields like:
            name
            occupation
            associated_entities (related celebrities, works, etc.)
            popularity_score
            tags (keywords or entities relevant to the person)
            Elasticsearch’s inverted index structure will allow for fast retrieval and efficient search operations.
            
            3. Query Processing
            When a user searches for a specific celebrity:

            Text Matching: The search query can be used to match the name field in the documents.
            Similarity Matching: Elasticsearch can compute similarity scores based on field analysis. Fields like associated_entities and tags 
            are especially useful for finding related profiles.
            
            4. Building the “People Also Searched For” Logic
            Collaborative Filtering Approach: Track user behavior to understand patterns (e.g., when users search for "Celebrity A," they often proceed to search for "Celebrity B"). This pattern can be stored in Elasticsearch and used to suggest related profiles.
            Semantic Similarity with Elasticsearch:
            Use the more_like_this query in Elasticsearch to find documents similar to the one being searched for. The query analyzes fields like associated_entities and tags to suggest profiles that share common attributes.
            Boosting and Scoring: Boost certain fields like popularity_score or tags to influence which results appear more prominently.
            5. Result Ranking and Presentation
            Custom Scoring: Use custom scoring scripts in Elasticsearch to rank the results based on relevance, popularity, and other signals.
            Filtering and Fine-Tuning: Implement additional filters to ensure only the most relevant profiles are displayed. You can use must, should, and must_not clauses in Elasticsearch queries to refine the results.
            6. Continuous Feedback and Improvement
            Feedback Loop: Continuously collect user interaction data (e.g., clicks, time spent) to refine the recommendation logic. This data can be indexed in Elasticsearch for real-time query adjustment.
            Machine Learning Integration: Consider integrating machine learning models to enhance the recommendations by analyzing deeper relationships between entities. This can be coupled with Elasticsearch’s vector search and ranking algorithms.
            Key Elasticsearch Features Utilized:
            Inverted Indexing: Fast text-based search.
            More Like This (MLT) Query: Finding similar documents based on fields.
            Nested Queries and Filters: For fine-grained control over query logic.
            Custom Scoring: Adjusting scores based on specific business rules or signals.
            Aggregations: To analyze and group related entities for better recommendations.
            Final Workflow Example:
            User searches for "Celebrity X."
            Elasticsearch retrieves the profile for "Celebrity X" and runs a more_like_this query on key fields (e.g., associated_entities, tags) to find related profiles.
            The system ranks and boosts results based on additional factors (e.g., popularity, co-search behavior).
            The top results are displayed as “People also searched for.”


            1. Problem Statement and Requirements
            The goal is to design a system that, when given a celebrity name as input, suggests a list of other related celebrities that people might search for. 
            The relationships can be based on factors like profession, co-appearances, popularity, or user behavior patterns.

            Key requirements:

            Scalability: The system should handle large volumes of celebrity data and high query traffic.
            Relevance: Suggestions should be contextually relevant to the queried celebrity.
            Real-time Response: Query results should be returned quickly for a seamless user experience.
            Adaptability: The system should learn and evolve based on user behavior and feedback.
            2. Data Ingestion and Preprocessing
            2.1. Data Sources
            Structured Data: Public databases (e.g., IMDb, Wikipedia) providing celebrity information like names, professions, movies, etc.
            Unstructured Data: Scraped articles, news, social media mentions for additional context (e.g., trending relationships).
            2.2. Data Modeling
            Each celebrity is represented as a document in Elasticsearch. A simplified JSON document might look like this:

            json
            Copy code
            {
            "name": "Celebrity X",
            "occupation": "Actor",
            "associated_entities": ["Celebrity Y", "Celebrity Z"],
            "movies": ["Movie A", "Movie B"],
            "tags": ["Hollywood", "Action"],
            "popularity_score": 87,
            "related_searches": ["Celebrity Y", "Celebrity Z"]
            }
            Fields Explanation:

            associated_entities: Direct associations like co-stars, collaborators, or people often mentioned together.
            tags: Keywords derived from content or context (e.g., genres, domains).
            popularity_score: A precomputed score based on metrics like social media mentions, search trends, etc.
            related_searches: Derived from user behavior data—actual searches users made after querying “Celebrity X.”
            2.3. Indexing Data in Elasticsearch
            Each document is indexed with multiple analyzers for flexible querying (e.g., standard, ngram, and synonym analyzers).
            Field types are carefully chosen (text, keyword, float, etc.) to optimize query performance and relevance.
            
            3. Core Query Logic and Recommendation Strategy
            When a user searches for a celebrity, the system needs to retrieve related profiles based on a combination 
            of content similarity and user behavior. Here’s how it works:

            3.1. Primary Search and Document Retrieval

            Query Execution: The query first retrieves the document for “Celebrity X” using a standard match query.
            Field-Level Boosting: Fields like name, occupation, and tags can be boosted to prioritize certain types of matches. For instance, 
                if two celebrities share common tags or have many overlapping associated_entities, they should be ranked higher.
            Example ES query:

            json
            Copy code
            {
            "query": {
                "bool": {
                "must": [
                    { "match": { "name": "Celebrity X" } }
                ],
                "should": [
                    { "more_like_this": {
                    "fields": ["associated_entities", "tags"],
                    "like": ["Celebrity X"],
                    "min_term_freq": 1,
                    "min_doc_freq": 1
                    }},
                    { "term": { "related_searches": "Celebrity X" }}
                ]
                }
            }
            }
            3.2. Similarity Matching with more_like_this Query
            The more_like_this query analyzes text fields (e.g., tags, associated_entities) to find documents that share similar content. This is useful for semantic recommendations where relatedness is driven by shared attributes.
            3.3. User Behavior Data Integration
            The related_searches field is populated based on user activity data. For example, if many users who searched for “Celebrity X” later searched for “Celebrity Y,” this association strengthens over time.
            3.4. Custom Scoring and Boosting
            Using Elasticsearch’s custom score scripts, you can create complex scoring logic. For example:
            Boost documents with high popularity_score.
            Adjust scores based on recency if you want to prioritize trending searches.
            Apply decaying functions so that older associations are weighted less over time.
            4. Advanced Techniques: Collaborative Filtering and Aggregations
            Collaborative Filtering: This technique is used to find patterns based on user behavior. Elasticsearch can track these behaviors using related_searches and aggregation queries. For instance:
            json
            Copy code
            
            {
            "aggs": {
                "common_searches": {
                "terms": {
                    "field": "related_searches",
                    "size": 10
                }
                }
            }
            }


            The aggregation identifies which profiles are frequently searched together, which can be incorporated into the recommendation logic.
            5. System Architecture
            Data Pipeline: Regularly ingest and preprocess celebrity data from sources. Update indices based on fresh data or evolving trends.
            Elasticsearch Cluster: Consists of multiple nodes with dedicated roles (master, data, coordinating) to ensure scalability and fault tolerance.
            Caching Layer: Use Elasticsearch’s built-in caching mechanisms or an external cache (e.g., Redis) to store frequently queried data, improving response times.
            6. Handling Scale and Performance Optimization
            Shard Strategy: Split the index into multiple shards based on expected query load and document size.
            Replica Settings: Adjust the number of replicas to ensure high availability and quick failover.
            Query Caching: Use query caching for frequently accessed results, especially for popular queries (e.g., “Celebrity X”).
            Result Windowing: Limit pagination depth (using from and size) to avoid performance degradation when dealing with large result sets.
            7. Continuous Learning and Feedback Loop
            Track user clicks and refine suggestions based on this feedback. This can be fed back into the related_searches field or used to update tags and associations dynamically.
            Machine learning models can be integrated to enhance the recommendation engine, especially for deeper relationships not captured by traditional search methods.
            8. Monitoring and Maintenance
            Set up monitoring dashboards (e.g., Kibana) to track query performance, latency, and errors.
            Implement automated reindexing pipelines to handle data updates and schema changes.
            Conclusion
            This design leverages Elasticsearch’s capabilities for full-text search, relevance scoring, and real-time analytics to create a system that efficiently recommends related celebrities. The combination of content-based and behavior-based approaches ensures that suggestions remain relevant, timely, and personalized for the end user.


        how would you get related celeberities initally? What if we dont want that in es index, can we still do it 

            3/3

            ChatGPT said:
            ChatGPT
            Great question! Getting the initial set of related celebrities is crucial, and there are various strategies for this depending on how much data you have, the complexity you want, and whether you prefer to keep this information outside of Elasticsearch.

            Let’s break down different ways to identify and compute related celebrities initially and discuss how it can be done without storing it directly in the Elasticsearch index.

            1. Generating Initial Relationships (Without Using Elasticsearch)
            If you prefer not to store related celebrities in Elasticsearch initially, you can compute these relationships using external tools, algorithms, and data sources. Some common approaches include:

            1.1 Co-occurrence Analysis (Graph-Based Approach)
            Data Source: Scrape or obtain structured data about movies, events, or co-appearances (e.g., movie credits, award nominations, social media mentions).
            Graph Building: Build a graph where nodes represent celebrities and edges represent co-occurrences or relationships (e.g., acting in the same movie, co-starring at events).
            Community Detection and Clustering: Use algorithms like Louvain, k-means, or spectral clustering to group celebrities into clusters. Celebrities within the same cluster are considered related.
            External Processing: You can use a tool like NetworkX (Python) or Neo4j (graph database) to manage this relationship data and query it externally.
            Advantages:

            You maintain full control over relationship definitions.
            Data remains outside Elasticsearch, which can simplify index design.
            Graph algorithms allow deeper, more complex relationship insights.
            How to Use This Data in Elasticsearch:

            When a user searches for "Celebrity X", you query your external system to fetch related celebrities in real-time.
            You then send these related names to Elasticsearch as part of your query.
            Example workflow:

            User searches "Celebrity X."
            Query an external graph database to get related names (e.g., "Celebrity Y" and "Celebrity Z").
            Send a multi-match query to Elasticsearch with these names to retrieve documents.
            json
            Copy code
            {
            "query": {
                "multi_match": {
                "query": "Celebrity Y, Celebrity Z",
                "fields": ["name", "tags", "associated_entities"]
                }
            }
            }
            1.2 Collaborative Filtering Based on User Behavior (Matrix Factorization)
            If you have access to user behavior data (e.g., search logs), you can use collaborative filtering techniques to identify related celebrities. For example:

            Data Source: Collect search logs or interaction data (e.g., which celebrities users search for in sequence).
            Matrix Factorization: Build a user-item matrix where rows are users and columns are celebrities. The entries are counts of interactions (e.g., searches). Use techniques like Singular Value Decomposition (SVD) to reduce dimensionality and find latent factors that connect related celebrities.
            External System: Compute these relationships offline and store them in a database (e.g., PostgreSQL, Redis).
            Advantages:

            Captures dynamic relationships based on real user behavior.
            Allows personalization and contextual recommendations.
            How to Use This Data in Elasticsearch:

            Query your behavior-based recommendation engine in real-time when a search occurs.
            Use the returned list of related celebrities to perform Elasticsearch queries.
            2. Hybrid Approach: Combining Precomputed Data with Elasticsearch Queries
            Even if you prefer not to store precomputed relationships in the Elasticsearch index, you can still use a hybrid approach where:

            Precompute relationships externally (as described above) and store them in a fast-access store (e.g., Redis, a relational database).
            Use these precomputed relationships to refine and filter Elasticsearch results.
            Example Workflow (Hybrid):
            User searches for "Celebrity X."
            Your system queries an external graph database or recommendation engine to fetch related celebrities (e.g., "Celebrity Y," "Celebrity Z").
            You then enhance your Elasticsearch query with this additional information:
            Use it to boost the relevance score of related celebrities.
            Apply filters to narrow down results based on these names.
            Sample Elasticsearch Query (Hybrid):
            json
            Copy code
            {
            "query": {
                "bool": {
                "must": {
                    "match": { "name": "Celebrity X" }
                },
                "should": [
                    { "match": { "associated_entities": "Celebrity Y" } },
                    { "match": { "associated_entities": "Celebrity Z" } }
                ]
                }
            }
            }
            3. Advanced Strategies: Real-Time Computation with Vector Search
            If you don’t want to rely on precomputed relationships, another option is to compute relatedness in real-time using:

            Vector Embeddings: Create vector embeddings for each celebrity based on textual or contextual data (e.g., names, biographies, related keywords). These embeddings capture semantic similarities.
            Vector Search in Elasticsearch: Perform nearest-neighbor search (using cosine similarity or dot product) to find similar profiles directly.
            Advantages:

            No need for manual precomputation.
            Highly scalable and adaptable for real-time scenarios.
            How to Use This in Elasticsearch:

            Index vector representations of each celebrity’s profile.
            During search, compute the vector of the query (e.g., “Celebrity X”) and find the closest vectors (most similar profiles).
            Example:

            json
            Copy code
            {
            "query": {
                "knn": {
                "field": "profile_vector",
                "query_vector": [0.123, -0.456, 0.789],
                "k": 5
                }
            }
            }
            Conclusion
            You can effectively compute and manage related celebrities outside of Elasticsearch using graph databases, collaborative filtering, or even vector embeddings. These external systems can be queried in real-time to supply context to your Elasticsearch queries. The hybrid approach offers the best of both worlds, allowing you to keep core recommendation logic outside Elasticsearch while still leveraging Elasticsearch for efficient search and ranking.



    Harmans Thoughts:

    Clarifying questions: 

        How realtime does this have to be? 
        How fresh does the suggestions need to be? Do we need to refresh our trie as soon as other people search topics


        Lets add these searches as dcouments to elastic search.
        Then we can support similartiy search by using elastic search to ... -> we may have to reindex data as we count up how many times each query is being performed. 
        This reindexing job by elastic search will also need to be done when offline job recomputes each document in elastic search inde. 
    


    this looks like an autocomplete type problem... 
    People also searched for... 
    We would need to reverse find what people are search  for that is similar to what you searched. 
    This could require machine learnign and using a recommendation system to identify what topics are close to the topic you searched for.
    You can reverse look up the topic for a search and then use that to find the top k searches within a topic. 
    You would need to aggregrate what people are searching for. 


    
------------------------------------------------------------------------------------------------

todo: food delivery service: 


----------------------------------------------------------------

Design a service to calculate the top k listened songs in past 24 hours

Invoke a service to get top K listed songs
It could be continuous stream of data, so we can use a time window based solution, our window length is past 24 hours. Current time can be the server time (We can confirm we are building this based on server time)
We can calculate this window every hour depending on our requirement. Hourly or very half hour, we can confirm with the user.
Streaming solution can be used, we can stream and store this data in some messaging solution like Apache Kafka and using Apache Spark like cluster computing framework we can run calculation per window every hour or so. Each window will be like an RDD of data on which we can do aggregation.
Once the data is aggregated the aggregation result cna be pushed to a distributed cache for pulling it fast from the front end.
After an hour when the job again runs it will again calculate top k and update in cache (again update to cache needs to be checked). As this is readonly solution a small hit of showing stale data while updating the cache might not be that much of a problem
Old data beyond the past 24 hour window can be thrown away via another batch job as it is not require


----------------------------------------------------------------

System Design | GoFundMe (todo)



Was recently asked to design a website like GoFundMe.

How will you design the payment system, user login. What features will you support? How will you handle payment failures etc.


----------------------------------------------------------------

Design an auction system on instagram (todo)

On the flip side, one should also perhaps consider allowing users to create auction listings for items they wish to sell? You're not always bidding but may also want to sell/auction.
Other add-ons could be:

Notifications should be sent when users are outbid
Auto-bid up to a limit




Auction system:: 

Bidders bid for an item. 
SSE -> because most bidders arent participating in auction but want to see the latest price for an item. 



Assumptions: some product is being auctioned off for money. Users need to authorize payment in advance to make sure that whoever wins the auction pays. A single auction can have 1 million participants, most won't bid but they all want to see real-time updates of the current best bid for the hype.

Some ideas:

security: need login and security tokens to make sure that users are bidding as themselves (no fraud) and have already authorized payment (no sandbagging)
payment: need a payment system and provider - check balances, payment types, etc. Have to verify cards for users to participate in bidding
very high read:write load, most want "streaming" updates of the price. Most front ends (like LC) implement this as a recurring web request like .../v1/auctionPrice?auctionID=387asd8x

high user load: we want to "bounce" most bids at or below the current best bid as early as possible. Utilize caching to store the current best, and update that cache VERY frequently (order of seconds). Use fanout if necessary, such as using redis or memcached, so e.g. 1000 web servers hit 10 cache servers, and thus only the 10 cache servers contact the auction server every 5 seconds for an updated price
consistency: the first bid at each new best price should be the winner, so a single server ultimately should track the best bid and current winner.

fault tolerance: that single server for consistency is a liability. New bids and prices should be immediately committed to a DB, or written to a writeahead log. On failure, another instance should be able to take over quickly - you just need the last entry of the writeahead log to know the current winner

even more fault tolerance: you can run a master-standby config where the standby just sits there. Upon failure of the master, the standby reads the last entry in the writeahead log and takes over until the master is back up
timing: what timestamp do you take as the truth for auctions
client time: really bad idea, clients can manipulate their clocks and web requests
web server time: somewhat bad idea, servers' time synchronizations vary on the order of 1-100 ms which can matter for fairness
auction server's time: decent, but beware edge cases where a clock sync results in naughty edge cases:
real time t, server's clock is fast and reads t+100. you record a new bid b at t+d
real time t+1, server synchronizes its time and now reads t+1
real time t+2, a bid for b arrives. The server sees a new best bid b at t+2. If you're not careful, the server can consider this the new best bid even though it arrived later
you can mitigate this by only using the bid values, and just letting the nature of single-threaded execution break ties
you can also use an "absolute clock" like Java's System.nanoTime() instead of a "synchronized calendar clock" like System.currentTimeMillis(); the former isn't adjusted by synchronization
also beware of other common time gotchas such as bidders in different time zones, and auctions that take place around DST transitions
Some other features and stretch goals would include

    setting a minimum auction price
    auctioning off 10 of something: Dutch auction (see also: US treasury and bond auctions)
    setting an end time for the auction: last best bid before the end time wins
    ending the auction based on activity: no new best bid in 5 minutes ends the auction and the last person wins



----------------------------------------------------------------
Wallet Service Integration

Context: The company has a Bike Rent System paid per hour, and the bikes are always shared.

Problem: Create a wallet system that integrates with the company's bike rent product.

        My Approach:

        I used the button-up approach to solve this problem. And started by understanding the entities and having an overlook of the database (really high level).

        Basically, you need a financial entity (to store information related to the author of the transactions and tax proposal); you need a transaction table (with all the transactions a wallet has); a wallet (that has on financial entity, many transactions).

        On the logic, we must never remove anything from the transactions. That is basically account rules (so if there is any problem or dispute) we must always add a new transaction.

        The wallet also holds the balance of the transactions, so that it is easier to check and it doesn't demand any calculation to check the balance; plus that information is the most accessed.

        When a user uses a bike, we deduct an amount from it's wallet. And from time to time we deduct another amount until a limit. After that limit (zero balance), the user can't rent a new bike.

        In terms of interfaces, we must have a summary endpoint (that shows the balance of the main screen), that endpoint is usually cached and must handle a huge amount of requests. An endpoint to get all the transactions (that retrieves that information paged) and an endpoint to rechange (add more money) that triggers integrations with banks etc... The endpoint used by the other integrations (the core of the Bike Rent System) uses a gRPC interface, for performance reasons.

        I'll not extend on how the services were defined, but the nicest parts of this interview were adding the functionality of transferring money among users and allowing your friends to use your balance to rent the bike. Also, keep in mind that collection is part of this system. Block a user to rent a new bike, and we must always think about possible in-which fraud can happen. So as exercises drow the whole system =)



Soln 2:

        Thanks for posting, was this a real interview?

        I just did the mock interview on my own, was a kind of neat problem. You spent more time on the details of the queries than I did, maybe I should do the same in interviews coming up. Good idea to log all the transactions, that's a great requirement to infer.

        Not sure about caching for the summary endpoints though or gRPC for performance. The cache of any one user's balance will mostly only be used by that one user, and any rental will change the balance so the value would be frequently evicted. 50M DAU with an average of 1 rental per day is about 500 tx/second on average, maybe 1,500/second peak. Not too bad for a system with few API servers and a master-standby DB for reliability anyway. Hard to argue with gRPC though, even a small benefit is a benefit.

        Did you wrangle with making sure transactions are only done once? For example maybe your server that posts a credit card charge to a payment provider crashes right after requesting the payment. This is the main part I struggle with in problems like "Design a Payment Service" or "Design Ticketmaster," etc. Maybe

        1. you record a pending transaction in a DB, like "I'm charging customer A for bike rental B for C dollars, with transaction number D. Status pending.". Basically as a writeahead log
        2. you run the transaction against the payment provider; I'm assuming payment providers let you supply a custom bit of info like the transaction number D
        3. you get an answer from the payment provider, either an accept with metadata or a decline and reasons
        4. update the transaction info in (1), setting the status to success or failed
        5. you post the result of the transaction to the tx log
        
        So if a crash occurs you can handle it:

        between 1-2: there is a record in the DB, but they payment provider's API has no record. Retry the transaction.
        between 2-3: there's a record in the DB, AND the payment provider's API has a record of that transaction number. Proceed from there
        after 3: same as prior point
        Which means recovering from a crash involves reading the DB for all pending transactions, and following up on them.

        Don't know if that's the best way to do it though.


----------------------------------------------------------------



Hello leetcoders,
    One design question in interviews especially with Autonomous Driving companies (e.g. Waymo, Uber ATG, etc) could be the following.
    Design a simulation platform backed by distributed cloud that renders collected datasets all the way from SDC vehicles to the user browser.
    To get a better sense of how a simulation environment looks like you can visit Uber ATG post and Waymo blog post.
    As you may know, the challenge of designing a seamless rendering platform for (500-1000) users is the extremely large datasets. In this problem we are dealing with several high-res RGB cameras as well as the data from radar and lidar sensors. The whole dataset may contain over several millions of driving miles.

    Here are some specifications:

    In-vehicle data:
    5x 720p streams of RGB cameras. (We may compress)
    1x stream of lidar sensor (raw point cloud)
    5x stream of low-res radar
    GPS readings
    Operator data
    Driving data (gas, streering angle, brake, etc from CAN protocol)
    
    External data:
    High definition map of the venue. (high level map of traffic signs, roads, facades, etc)
    After each complete route by the vehicle, the data will be uploaded somewhere to enable visualization.
    Traffic: We expect to have maximum of 1000 users working with the system.
    Duration: On average each dataset contains 1 hour of driving. So each visualization task takes ~1hour.
    
    High durability is a must.
    Realtime and seamless rendering experience is required.
    
    The visualization environment must process the data and render 3D or 2D objects in the map as defined by the dataset.
    Fast-forward, backward, replay mode is required.
    
    
    Bonus features:
    Ability to annotate the dataset and save for later use.
    Ability to save sessions and continue the process at later time.
    Interaction with other services such as (Pedestrian detection, obstacle detection, etc)
    Since I did not find a similar design over the internet, I thought I would create this topic to get feedback from the community.
    Thanks,



Harmans design:

    high res RGB cameras. 


    FR:
        rendering platform for 500-1000 users

        Render it after the drive is completed and we have the data somewhere like S3

        Render the high definition map as well as the vehicle on the map, would need the dimensions of the vehicle. 

        Dataset has 1 hour of driving content. 


----------------------------------------------------------------

    Credit card transactions aggregation system

        Hello,

        Recently I interviewed a company where I was asked to design a system to aggregate credit card transactions from multiple credit card companies.
         What database would you think is the best for such a sytem?

        The data is structured (Company name, Date, Transaction Description, Amount). This can be stored in RDBMS as tables. 
        One table for user data (which provides a user id), and then transactions in a separate table with above data and user id. 
        A third table stores user id and list of companies to get.

        The above could also be stored in a document DB just as easily. At least we could store user info and their list of companies in one doc, 
        since it is nice to have this information together while downloading data periodically. The actual transactions could go in separate doc.
         However, this doc would keep increasing in size. Do we need to trim it periodically? That is, retain data for 1 or 2 years?

        Would columnar data be better for the transaction data? It is just a store and not really being queried on any parameter (at least not in the initial stages).

        Lastly, what is the secure way to store passwords or session tokens for the credit card company.

        During the interview, no matter what I said, the interviewer was unhappy. I am left confused as to what should I have actually said. I didnt get the offer.

        Thank you all!




        I worked in one of the financial data aggregation company. We used SQL database(Oracle). This is the best way as less joins needed in this case and also schema is not changed/ less changes expected. Also We need ACID compliant databases in this scenario. People may tempt to use NoSQL but it will be not necessary. SQL will solve 99% of the designs.



        The SQL vs NoSQL and OLTP vs OLAP depends solely on requirements. So make sure you ask next time. Do you need transactional joins? Do you need to scale horizontally? What's the expected load? What's the expected query patterns? SQL is usually the simplest solution in 99% of cases. OSS SQL databases are time tested workhorses.

        As for storing passwords and session tokens, store the hash in the database. For secret data that you need to be able to access in the future (SSN, credit card numers, etc), store a token (GUID, etc) in the main database and store the secret itself in a hardware encrypted secret store (KMS, Vault, etc). Use the token for all comparisons and only access the secret data when required (processing the transaction with an external party)
                    




----------------------------------------------------------------

Detection and response system (todo):


Setup: you have a daemon that collects data about a machine can upload data. You also have an ML model that takes in aforementioned data and can detect anomalies and produce responses, either OK/NOT OK. The ML model can also output actions to take when NOT_OK is outputted, e.g. SHUTDOWN, and these actions should be sent back to the daemon and actuated on its own.

Scale:

100K machines, scalable up to 10M
every machine uploads every 10 min, and payloads are 10mb
Data retention of 1 year
I have a rough idea of how to do this but not sure on specifics. Basically combining a metric collection system and notification system.

Clients call an upload service to upload data. Data upload could be bursty. 100K * 10mb = 1TB every 10 min. We can scale horizontally in upload servers to distributed storage, but I'm not sure how to make this more efficient. What if an upload fails? Should the data be chunked?
Upload service calls evaluation service running the ML models. We need to do 100K evaluations every 10 min. We can build a specialized cluster capable of running many parallel evalutions. We use a distributed message queue to make sure evaluation requests are not lost.
evaluation service calls notification service that notifies clients. Once evaluations are done, those processes post to a notification service that forwards results to clients. This can also scale horizontally.
Some follow ups:

We can use the retained data to retrain and develop our model. How do we deploy this and notify clients?





----------------------------------------------------------------

    Design a system to aggregate metrics from large cluster(800+) of web servers

    Design a system to aggregate metrics from large cluster(800+) of web servers. System should be near realtime.

        Low Level Design of Food delivery app - Zomato, Swiggy, UberEats

        Requirements

        Restaurant can register themselves.
        User can create, update, delete, get their profiles.
        User can search for the restaurant using restaurant name, city name.
        Restaurant can add, update foodmenu.
        User can see the foodmenu. User can get the food items based on Meal type or Cuisine type.
        User can add/remove items to/from the cart. User can get all the items of the cart.
        User can place or cancel the order. User can get all the orders ordered by him/her.
        User can apply the coupons. User can get the detailed bill containing tax details.
        User can made a payment using different modes of payment - credit card, wallet, etc.
        Delivery boy can get all the deliveries made by him using his Id.
        User can get the order status anytime. Success, Out for Delivery, Delivered, etc.
        Services

        RestaurantService
        UserService
        FoodMenuService
        CartService
        OrderService
        PricingService
        PaymentService
        DeliveryService
        I have published a story on medium about the working of all the services. I also have added the Data Model required. Please find the link here .
    https://medium.com/@mayankbansal933/food-delivery-app-lld-c1409ef49266




----------------------------------------------------------------

Design a system which can report frequently occurring exceptions on a dashboard (todo)

    You are given a log file generated from a distributed service in which each line is either a service event or exception.
    Design a system which can report frequently occurring exceptions on a dashboard. In below file, most commonly occuring exception is Exception1.

    User of the system should be able to fetch results for say top 10 or 50 exceptions. Assume that, a given event as well as exception take only one line in the log file.

    event 1
    exception 1
    exception 2
    event 2
    exception 3
    exception 1
    exception 3
    exception 1
    Entries should be sorted by occurance, most frequent appearing at the top.
    Top 2 exceptions for above file would be.

    exception 1  
    exception 3
    Now, what if the operation needs to be performed for a given time range (Assume that each log entry has timestamp as well)?


clarifying questions:

    How many instances of the service are running on how many machines?
    Does the dashboard need to be updated in real time or just when the user clicks refresh?
    How many simultaneous users will there be for the dashboard?
    Do we have the ability to ask the service owners to modify their code?
    How often are the log files rolled and/or deleted?
    If a node goes down or the service blows up, what is the state of the log file(s) for service(s) running on that node?
    How is the name and location of the log file determined (config file, calling a service, etc.)?
    Can the file system housing the log file be accessed remotely, or does a log file capture require a service running on the same machine as the service that is doing the logging?
    Is the dashboard just for this service, or is it part of a larger dashboard system that displays information about other components and/or summaries and roll-ups?
    What are the requirements for consistency, accessibility, and partitionability of the dashboard system?
    What is acceptable latency from the time an exception occurs until it appears on the dashboard?
    
    If a log file capture service of some sort is running against the same file system as the log file, is it allowed to write a file locally?
    Some exceptions may occur frequently, but may be trivial. Is there a need to weigh the importance of the exceptions reported besides just their frequency? Might this become a requirement in the future?

    How long does the dashboard system need to retain data?
    What is the expected volume of exceptions per day, week, or month per instance of the service?
    
    What does it mean for an exception to be "frequently occurring"? In a day? In the last hour? Per week? Per the time window specified by the user? Or simply the most frequently occurring in one particular log file (i.e. from one instance of the service) from the start of the file to the last entry?
    If multiple occurrences of the service run, possibly on different machines, does the identification of "frequently occurring" exceptions need to be based on the total for all instances of the service, by machine, or by individual instance of the service? (It's OK to say all or any of the above - this will give me more information about how much data I need to store, for how long, and what kinds of queries need to be available.)  

Is this system design splunk?


    ok soooo...












----------------------------------------------------------------

Github : Design search feature in Github scale code repository


Design search feature in Github scale code repository

    Requirements:

    Search with words in code repository
    Global scale
    Private code repository search.
    Preference to code owned by self or user's team.


    CHAT GPT SOLN:


    For designing a large-scale code search feature in a GitHub-like repository, here is a solution that focuses on key components needed to meet the requirements:

        Requirements Recap:
        Search with keywords in a code repository: Users should be able to search for specific words across all repositories.
        Global scale: The system should be designed to handle millions of repositories and billions of lines of code across the globe.
        Private code repository search: Users should be able to search within their private repositories.
        Preference to code owned by self or user's team: The system should prioritize search results from the user's repositories or their team’s repositories.
        System Design Overview
        
        1. High-Level Architecture
        Indexing Service: Continuously indexes the codebase. This includes both public and private repositories.
        
        Search Service: Handles user queries, leveraging the pre-built index.
        
        Ranking & Personalization Service: Prioritizes search results based on the user's repositories or their team's code.
        
        Data Storage: Efficient storage for index data, repository metadata, and user preferences.
        
        Security Layer: Handles authorization, ensuring that users can only search within repositories they have access to.
        
        
        2. Components
        Code Ingestion & Indexing

        Repository Cloning: Regularly clone repositories for indexing (public and private).
        Parsing & Tokenization: Break down code into searchable tokens (keywords, identifiers, comments).
        Index Building: Use an inverted index (e.g., Apache Lucene) to map keywords to file locations within repositories.
        Delta Indexing: Continuously update the index with changes to repositories (e.g., commits, branches).
        
        Distributed Storage

        Index Sharding: Split the index across multiple nodes for scalability and performance. Shards can be based on repository size or ownership.
        
        Replica Management: Store replicas of index shards across data centers globally for high availability and fault tolerance.
        Search Query Handling

        Query Parsing: Analyze the search query (e.g., keyword search, exact match, regex) and route it to relevant index shards.
        Search Engine (Elasticsearch/Solr): Utilize a distributed search engine to run queries in parallel against multiple index shards.
        Result Ranking & Personalization

        Personalization: Enhance ranking by boosting results from repositories the user owns or has starred.
        Team-Based Preferences: Incorporate team or organization-based preferences by using metadata (team ownership) when ranking results.
        Private Repository Access Control

        Authorization: Check repository permissions (using OAuth tokens or access control lists) before returning search results.
        Data Isolation: Ensure private repositories have separate indexing pipelines with strict access control.
        Global Scale & Performance Optimization

        Edge Caching: Cache popular queries and frequently accessed results at edge locations (CDNs) to reduce latency.
        
        Load Balancing: Use intelligent routing based on data center load and user location to direct search requests.
        Throttling & Rate Limiting: Implement API rate limiting to ensure fair use and prevent abuse.
        
        Monitoring & Analytics

        Search Metrics: Track query performance, ranking effectiveness, and index freshness.
        
        User Feedback Loop: Use click-through data to improve ranking algorithms and adjust relevance scoring.
        
        Technical Considerations
        Index Size: Optimize index compression to handle large-scale data efficiently (e.g., using delta encoding, skip lists).
        
        Data Consistency: Employ eventual consistency models, especially for rapidly updated repositories.
        Latency: Aim for sub-second search results by optimizing shard queries and caching.
        Scalability: Plan for horizontal scaling with auto-scaling for both indexing and search nodes.
        
        Workflow Example
        A user enters a search query for a keyword like "foo" in the GitHub UI.
        The search query is parsed and matched against the user’s repositories and global code repositories.
        The query is routed to the relevant shards based on repository metadata.
        The search service ranks the results by prioritizing the user’s and team’s repositories.
        The results are displayed, ensuring that the user only sees results from repositories they are authorized to view.
        Trade-Offs & Challenges
        
        Freshness vs. Latency: Keeping the index up-to-date without affecting query performance.
        Global Consistency: Handling the challenge of globally distributed data while minimizing cross-region latency.
        Security: Strictly isolating private repository data during indexing and search phases.
        Conclusion
        This design balances scalability, performance, and security while focusing on user preferences and efficient global distribution.


--------------------------------

Questions to ask the Interviewer
Candidate
What are the primary use cases of the system?
Do live commenting service support only text?
Are the clients distributed across the globe?
What is the amount of Daily Active Users (DAU)?
What is the system’s total number of daily live videos?
What is the average amount of live comments on a live video?
What is the anticipated read: write ratio of live comments?
What is the peak amount of concurrent users watching the same live video?
Should the comments on streamed videos be archived to save storage?
Interviewer
Clients can interact with each other in real-time over live comments on a Facebook live video
Yes
Yes
100 million DAU
200 million daily live videos
10
100: 1
80 million
Yes, there is no need to support the replay of live comments

The pull-based subscription model is the optimal choice for the delivery of live comments to the receivers for the following reasons 6:

the receiver only needs to view the live comments when the receiver is online (connected or subscribed to the live video)
the receiver can ignore the comments when disconnected or unsubscribed
non-trivial to predict the total count of clients that will be subscribed to watching a live video
the receivers will be geographically distributed across the globe
the connection from the receiver might be unpredictable




Figure 2: REST compared with SSE
The client creates a regular HTTP long poll connection with the server with server-sent events (SSE). The server can push a continuous stream of data to the client on the same connection as events occur. The client doesn’t need to perform subsequent requests 7.

The only difference for SSE from a regular HTTP request is that the Accept header on the HTTP request holds the value text/event-stream. The EventSource interface is used by the client to receive and process server-sent events independently in text/event-stream format without closing the connection. All modern web browsers support the EventSource interface natively. The EventSource interface can be implemented on iOS and Android platforms with lightweight libraries 7, 8.



SSE works over traditional HTTP
SSE streams chunks of data over the same open HTTP connection
The predefined fields of an SSE connection are the following 6:

Field	Description
event	the event type defined by the server
data	the payload of the event
id	ID for each event
retry	the client attempts to reconnect with the server after a specific timeframe if the connection was closed



Apache Cassandra is not optimized for read operations due to the nature of the LSM-based storage engine. An in-memory database such as Redis can be used in combination with Apache Cassandra to improve the read performance and make the data storage layer scalable and performant for live comments. The geo-replication enabled Redis cache with a time to live (TTL) of 1 second can be added as a cache layer on top of Apache Cassandra to improve the read performance. In addition, live comments published on an extremely popular live video can be kept in the cache on network edges to improve the latency 10.

The sets data type in Redis can be used to efficiently store the live comments. The native deduplication logic of sets data type ensures that live comments are stored in memory without having an additional logic to prevent repeated live comments. The sorted set data type in Redis can be used for data integrity by maintaining the reverse chronological ordering of live comments. The sorted set data type can make use of the timestamp on live comments for sorting the live comments without implementing a custom sorting algorithm. The metadata of the publisher of a live comment can be stored in Redis hash data type for quick retrieval 4, 11, 5.

The receivers who are geographically located closer to the publisher of the live comment will see the live comment instantly while the receivers who are located on a different continent might see the live comment with a slight delay (lower than 250 ms) to favor availability and partition tolerance in the CAP theorem 10.

----------------------------------------------------------------


Copy coredump files from millions of system to single Storage server like S3

There are millions of systems which generate coredump at any point. When ever coredump is created, file should be 
automatically copied into Central server s3. Problem statement is more concentrated around how to automate the 
secure connection from systems to the central storage server.


If you are working in Amazon technologies, you use IAM roles to securely authenticate the box to push directly to S3 for specific keys. 
You don't have to maintain an API server or anything. All that needs to be added is a 
simple aws s3 cp coredump s3://bucket/core_dumps/machine_id/app_name/yyyymmdd.dump. You can add the logic in systemd hooks.

----------------------------------------------------------------

I was asked to answer a question in a phone interview for Amazon.
The question is as follow:
Image there is a website generates user access log in Terabytes scale every day. Each log contains many different information such as 
username, visiting time, location, page visited, and etc.
what's your solution if we want to only store certain info(for example, user, login time, location) to a sql database. How to optimize the process.

This is an open question. So I believe there is no correct answer. what is your thought?


    Few constraints to consider:

    Asynchronous : you do not want user-experience to get blocked on log processing
    Lossless (as much as possible) : for instance provide 99.9 guarantees
    Latency : Are these logs being used for real-time analysis and alerting for site-down? If so < 1min latency. Is this is used for offline reporting? 20min latency might be doable.
    Security : Are you sending PII data-logs for the user. If so, they must be encrypted during transfer and every service must have the right keys to de-crypt before processing
    Services involved.

    Log-receiving service : This service can receive logs and perform basic validations - size, data-types, required fields like user-id etc. It then batches these logs in-memory (say: 30k or 3min) and stores this batch into a store say an S3 bucket file. (Depending on how fault-tolerant you need to be you can store each log too, since in-memory batches can be lost if the server crashes before a persist). This initial store ensures that you persist a raw copy of the log-data. (This is similar to the Producer service referred in Kafka answers)
    This service can also be preceded by a queue like SQS, to decouple the log-sending service/client from this log-receiving service. Scale-handling.
    Log-Processing service/ worker : This can reads S3 file contents, transforms into the data-model needed for SQL and writes required rows into SQL. You can also drop this transformed data into another queue so you can decouple and scale writes-to-SQL indepdent of log transformation. SQL query engine will take care of updating indexes and atomic/consistent writes.
    Partitioning is something to discuss : you can either have timestamp be the partition key ( which likely means you'll have hot-partitions for recent logs but good range querying) or you can have user-id be partition key(less likelihood of hot parititions for an amazon user). Or have a hash of either ts /userid be partitionkey (good distribution poor range querying).



Its a good example of streaming and distributed computing.

We can push generated log event into Kafka.
From Kafka, we can have spark streaming solution that will consume Kafka Message and create Spark RDD stream, those RDD will be 
mapped to required data (required attributes) and store into required DB like SQL ,Cassandra etc.


Another solution:

    break the problem down to 3 main parts:

    Collection
    Transport
    Storage
    Discuss and nail down the specs:
    Discussion (with white board drawing) could be something like:
    Many things to clarify that affects the system design:

    Scale requirement is in terabytes/per day but:

    do we have a estimate on the peak load? Peak load defines the boundary for our system, we need to take them in to account.
    Availability requirements - what would be the acceptable uptime?

    Security - should we be concerned about security?

    at collection side
    at transport
    storage
    Reliability requirements

    Collection failures
    Transport failures
    Storage failures
    Any acceptable data loss?
    Redundancy / cost trade off
    Realtime requirements, how much lag is acceptable end-to-end?

    Regional reqs ( wordwide deployments) - just to be clear that system does not need different specs for world wide deployments, if yes, we need to account for that.

    Proceed to design components:
    Collection discussion:

    Will the collection agent parse the data and send it in a consumable form? (json or similar)

    parsing at source is a nice way to distributed the processing, trade off -> using memory and CPU on application host. This way of distributing parsing load can be used catch offenders (those that spam logs)
    parsing at a common backend location would lead to bottlenecks at large scales.
    Transport discussion:

    Kafka or other existing distributed queuing system should generally work
    peak load factor is important here.
    message size matters here, each message in kbs vs mbs. Gbs
    Storage:

    Ask how data in the DB is going to be queried?
    -Discuss trade offs using a full SQL DB vs pseudo-sql or no sql DB for the use case.
    Propose aggregating data to some precision before storing:
    -user, login time, location
    What is time scale at? seconds, milliseconds? nano seconds is going to be very hard
    SQL db wont scale for seconds or milliseconds.
    Propose aggregating at minute level - trade off some granularity
    Ask what the user would do with the data, if they are only interested in higher level stats, aggregate up.
    Propose aggregating geo location to 1 decimal place - trade off in accuracy over miles.
    Discuss TTL of fine grained data storage - trade off db storage performance vs ability to go back in time
    propose, 30, 60, 90 day TTL and move to compressed archival.
    Does that data need additional processing?
    filtering
    date conversions/standardizations
    Design stream processing for aggregation to DB based on aggregation reqs.
    discuss some stream processing algorithm cost/trade off like running median etc
    Discuss Db scaling
    know internal workings of typical SQL , no SQL dbs

Third soln:

    Clarification:

    Is the log comes from multiple servers? If yes, we need to consider collecting data from different machines
    What's the purpose of storing the data to SQL? is it for analytical purpose? If yes, does it need to be real time?
    What's the size of the log if it only store certain info? I need this information to estimate the storage needed for the SQL and the number of records. In real situation, I will ask for example log file to see the structure of a single log.
    How long does the data in the SQL need to be keep? A month of a week? I may need to house keep the data periodically
    Is there an non-peak hour for the website to perform data collection task? since this may consume certain resources of the server
    Assumption:

    The logs come from 100 web servers behind a load balancer
    It is for daily analytical purpose. Collecting it end of day is acceptable
    The log structure is user_id, visiting_timestamp, visiting_url, the storage required for one single log is around 8 + 8 + 8 = 24 bytes
    After the analytics process completed. The SQL data can be delete. If the data scientist need the data again, we may provide a mechanism to allow him to load specific date range of data.
    The log are stored in multiple files. for each file, separate by file size, 500 MB each
    3:00 am to 6:00 am is the non-peak hour of the web site
    The daily analytical process can be completed in 1 hour.
    Components:

    Storage for log files
    SQL Server(s)
    Process to load data into SQL Server
    cron job to trigger load data process
    Process to house keep the old data in SQL Server
    Design:

    Adjust the config of the web site to store the log file into a SAN storage. So that the processing power / bandwidth during loading the data to SQL server can be separate from that of original web servers
    Write a program to read and batch upload the data from log file to SQL Server. 10 instance of the program will trigger by cron everyday 3:00 am. Each of them responsible for 10 web servers' log files running in parallel, so that the loading time can be shorten to 1/10.
    Inside the program we use bulk insert, instead of running insert statement one by one, in order to save upload time
    Although the data size seems very large, e.g. 4 TB = 4,000 GB = 4,000,000 MB = 4,000,000,000 Byte, the record count is about 160,000,000 (24 bytes per row in assumption) and I think single SQL Server can handled it, if we have house keeping the old data.
    A stored procedure to truncate the table is provided for the analytical process to use to house keep the database after analysis completed.
    A health check stored procedure will be run on 7:00 am everyday to check the data load completed or not. If not, it will send email to system admin to alert him. A completed date time will be write to the database to indicate job completed. The health check procedure will check against this data.


----------------------------------------------------------------
Design a system to store and retrieve logs for all of eBay

Sol1:

    Basic Idea:
    (From my experience in an actual enterprise application)

    Micro-services run in different servers in distributed system. Each server we can install an agent which will monitor any logs that are written.
    The App writes logs to the defined destination(e.g. logs to console) and then the agent in background picks it and sends it to centralize server.
    The centralize server and the app server are registered to use the agent and hence the authentication would be taken care in that process.

    AppServer(Log Agent) ------------------------> Log Server

    One example of such in market is Splunk. In simple terms, app server installs an agent, registers it with splunk server and with predefined log appender, it logs data.

    Now there is another way. Where the agent streams the new logs to a Kafka stream or any other queue. Then we can have a log subscriber to process the stream data. Advantage is its more flexibility. Applications can write a lot of other subscribers like error alerts, Security data in logs etc.

    Now the next thing is how the relation between different logs of same workflow is maintained. For e.g service A runs in server1, it calls service B that runs in server 2 and then another service C that runs in server 3. How to correlate all the 3 logs for 1 single workflow.

    There is a well known concept called Correlation ID. Some systems like SharePoint provide it with each call by default. Its basically a unique ID like GUID that you can attach to 1 log workflow across microservices. Simple. Now any issue debugging we can use this guid and then fetch corelated log files from log server.

    From here we can go into stuffs like splunk, elastic search etc. Then how to make it scale, what if agent fails etc.
    We can talk of how to store it in log server if we are not using any Market product like splunk. How to shard if log size is increasing etc.


Sol2:

    I'm surprised that no one mentioned about how Kibana and Splunk does it today. We can do something similar for this. Log aggregation.

    Each of the linux server where the logs are written can be configured with the filebeats and syslogger which will pull the logs from the destination directory and syslogger will transform it to the format you want. file beats knows where its current pointer is in the file system so, in case of crashes it can continue where it left off. It constantly polls the log files for information and send it to the target system via kafka. We need kafka for this because there are a thousands of messages written to he disc every second, only high scalable messaging system can keep up to distrubute the load.

    What is the target system? we can configure a document based db(cassandra) which is distributed based on the partition key. Also if you like we can create multiple keyspaces(Cassandra) for each of the ebay team. So the logs are separate for each of the team. What is the partition key? we can use the timestamp of the log as a partition key, even better if we use an epoch time for partition key because people use the range of dates to filter the information so it helps greatly. Since we are using epoch time, range of dates or specific date is mandatory for the search.

    How is the log line structured? if we have a consistant pattern of log across the teams we can use syslogger to parse the content using regex and create a document in the cassandra. So each row in the cassandra has parsed out information from the log. Since each of the log has several fields, we can configure the table columns to map each of the field in the log. Like timestamp, file name, filepath, app-name. etc, will be columns. These columns will be come secondary indexes on the table since cassandra will only allow search on the primary key and secondary indexes.

    Cassandra doesn't support fuzzy search out of the box. We need to add some plugin to make the fuzzy search work.

    Great design. Instead of Cassandra, how about using Elastic Search. We can get FuzzySearch on all the structured logs. Do you see any downside to using it?










---------------------------------------------------------------- 
Youtube system design system design interview book:

    Back of the envelope estimation
    The following estimations are based on many assumptions, so it is important to communicate with the interviewer to make sure she is on the same page.
    • Assume the product has 5 million daily active users (DAU). • Users watch 5 videos per day.
    • 10% of users upload 1 video per day.
    • Assume the average video size is 300 MB.
    • Total daily storage space needed: 5 million * 10% * 300 MB = 150TB
    • CDN cost.
    • When cloud CDN serves a video, you are charged for data transferred out of the CDN.
    • Let us use Amazon’s CDN CloudFront for cost estimation (Figure 14-2) [3]. Assume 100% of traffic is served from the United States. The average cost per GB is $0.02. For simplicity, we only calculate the cost of video streaming.
    • 5 million * 5 videos * 0.3GB * $0.02 = $150,000 per day.
    From the rough cost estimation, we know serving videos from the CDN costs lots of money. Even though cloud providers are willing to lower the CDN costs significantly for big customers, the cost is still substantial. We will discuss ways to reduce CDN costs in deep dive.


    More practically i think youtube has 300 million daily active users 

Before we discuss video streaming flow, let us look at an important concept: streaming protocol. This is a standardized way to control data transfer for video streaming. Popular streaming protocols are:
• MPEG–DASH. MPEG stands for “Moving Picture Experts Group” and DASH stands for "Dynamic Adaptive Streaming over HTTP".
• Apple HLS. HLS stands for “HTTP Live Streaming”. • Microsoft Smooth Streaming.
• Adobe HTTP Dynamic Streaming (HDS).

You do not need to fully understand or even remember those streaming protocol names as they are low-level details that require specific domain knowledge. The important thing here is to understand that different streaming protocols support different video encodings and playback players. When we design a video streaming service, we have to choose the right streaming protocol to support our use cases. To learn more about streaming protocols, here is an excellent article [7].
Videos are streamed from CDN directly. 

The edge server closest to you will deliver the video. Thus, there is very little latency. Figure 14-7 shows a high level of design for video streaming.

Video transcoding
When you record a video, the device (usually a phone or camera) gives the video file a certain format. If you want the video to be played smoothly on other devices, the video must be encoded into compatible bitrates and formats. Bitrate is the rate at which bits are processed over time. A higher bitrate generally means higher video quality. High bitrate streams need more processing power and fast internet speed.
Video transcoding is important for the following reasons:
• Raw video consumes large amounts of storage space. An hour-long high definition video recorded at 60 frames per second can take up a few hundred GB of space.
• Many devices and browsers only support certain types of video formats. Thus, it is important to encode a video to different formats for compatibility reasons.
• To ensure users watch high-quality videos while maintaining smooth playback, it is a good idea to deliver higher resolution video to users who have high network bandwidth and lower resolution video to users who have low bandwidth.
• Network conditions can change, especially on mobile devices. To ensure a video is played continuously, switching video quality automatically or manually based on network conditions is essential for smooth user experience.
Many types of encoding formats are available; however, most of them contain two parts:

• Container: This is like a basket that contains the video file, audio, and metadata. You can tell the container format by the file extension, such as .avi, .mov, or .mp4.

• Codecs: These are compression and decompression algorithms aim to reduce the video size while preserving the video quality. The most used video codecs are H.264, VP9, and HEVC.

In Figure 14-8, the original video is split into video, audio, and metadata. Here are some of the tasks that can be applied on a video file:
• Inspection: Make sure videos have good quality and are not malformed.
• Video encodings: Videos are converted to support different resolutions, codec, bitrates, etc. Figure 14-9 shows an example of video encoded files.
• Thumbnail. Thumbnails can either be uploaded by a user or automatically generated by the system.
• Watermark: An image overlay on top of your video contains identifying information about your video.


Video transcoding architecture
The proposed video transcoding architecture that leverages the cloud services, is shown in Figure 14-10.

The architecture has six main components: preprocessor, DAG scheduler, resource manager, task workers, temporary storage, and encoded video as the output. Let us take a close look at each component.


The preprocessor has 4 responsibilities:
1. Video splitting. Video stream is split or further split into smaller Group of Pictures (GOP) alignment. GOP is a group/chunk of frames arranged in a specific order. Each chunk is an independently playable unit, usually a few seconds in length.
2. Some old mobile devices or browsers might not support video splitting. Preprocessor split videos by GOP alignment for old clients.
3. DAG generation. The processor generates DAG based on configuration files client programmers write. Figure 14-12 is a simplified DAG representation which has 2 nodes and 1 edge:


The upload flow is updated as follows:
1. The client makes a HTTP request to API servers to fetch the pre-signed URL, which gives the access permission to the object identified in the URL. The term pre-signed URL is used by uploading files to Amazon S3. Other cloud service providers might use a different name. For instance, Microsoft Azure blob storage supports the same feature, but call it “Shared Access Signature” [10].
2. API servers respond with a pre-signed URL.
3. Once the client receives the response, it uploads the video using the pre-signed URL.


Cost-saving optimization
CDN is a crucial component of our system. It ensures fast video delivery on a global scale. However, from the back of the envelope calculation, we know CDN is expensive, especially when the data size is large. How can we reduce the cost?
Previous research shows that YouTube video streams follow long-tail distribution [11] [12]. It means a few popular videos are accessed frequently but many others have few or no viewers. Based on this observation, we implement a few optimizations:
1. Only serve the most popular videos from CDN and other videos from our high capacity
storage video servers (Figure 14-28).


---------------------------------------------------------------- Design Google drive

• Moving infrequently used data to cold storage. Cold data is the data that has not been active for months or years. Cold storage like Amazon S3 glacier [11] is much cheaper than S3.


Notification service
To maintain file consistency, any mutation of a file performed locally needs to be informed to other clients to reduce conflicts. Notification service is built to serve this purpose. At the high-level, notification service allows data to be transferred to clients as events happen. Here are a few options:
• Long polling. Dropbox uses long polling [10].
• WebSocket. WebSocket provides a persistent connection between the client and the server. Communication is bi-directional.
Even though both options work well, we opt for long polling for the following two reasons:
• Communication for notification service is not bi-directional. The server sends information about file changes to the client, but not vice versa.
• WebSocket is suited for real-time bi-directional communication such as a chat app. For
 
Google Drive, notifications are sent infrequently with no burst of data.
With long polling, each client establishes a long poll connection to the notification service. If changes to a file are detected, the client will close the long poll connection. Closing the connection means a client must connect to the metadata server to download the latest changes. After a response is received or connection timeout is reached, a client immediately sends a new request to keep the connection open.
Save storage space
To support file version history and ensure reliability, multiple versions of the same file are stored across multiple data centers. Storage space can be filled up quickly with frequent backups of all file revisions. Three techniques are proposed to reduce storage costs:
• De-duplicate data blocks. Eliminating redundant blocks at the account level is an easy way to save space. Two blocks are identical if they have the same hash value.
• Adopt an intelligent data backup strategy. Two optimization strategies can be applied:
• Set a limit: We can set a limit for the number of versions to store. If the limit is reached, the oldest version will be replaced with the new version.
• Keep valuable versions only: Some files might be edited frequently. For example, saving every edited version for a heavily modified document could mean the file is saved over 1000 times within a short period. To avoid unnecessary copies, we could limit the number of saved versions. We give more weight to recent versions. Experimentation is helpful to figure out the optimal number of versions to save.
• Moving infrequently used data to cold storage. Cold data is the data that has not been active for months or years. Cold storage like Amazon S3 glacier [11] is much cheaper than S3.


Failure Handling
Failures can occur in a large-scale system and we must adopt design strategies to address these failures. Your interviewer might be interested in hearing about how you handle the following system failures:
• Load balancer failure: If a load balancer fails, the secondary would become active and pick up the traffic. Load balancers usually monitor each other using a heartbeat, a periodic signal sent between load balancers. A load balancer is considered as failed if it has not sent a heartbeat for some time.
• Block server failure: If a block server fails, other servers pick up unfinished or pending jobs.
• Cloud storage failure: S3 buckets are replicated multiple times in different regions. If files are not available in one region, they can be fetched from different regions.
• API server failure: It is a stateless service. If an API server fails, the traffic is redirected to other API servers by a load balancer.
• Metadata cache failure: Metadata cache servers are replicated multiple times. If one node goes down, you can still access other nodes to fetch data. We will bring up a new cache server to replace the failed one.
• Metadata DB failure.
• Master down: If the master is down, promote one of the slaves to act as a new master and bring up a new slave node.
• Slave down: If a slave is down, you can use another slave for read operations and
bring another database server to replace the failed one.
• Notification service failure: Every online user keeps a long poll connection with the notification server. Thus, each notification server is connected with many users. According to the Dropbox talk in 2012 [6], over 1 million connections are open per machine. If a server goes down, all the long poll connections are lost so clients must reconnect to a different server. Even though one server can keep many open connections, it cannot reconnect all the lost connections at once. Reconnecting with all the lost clients is a relatively slow process.
• Offline backup queue failure: Queues are replicated multiple times. If one queue fails, consumers of the queue may need to re-subscribe to the backup queue.

Step 4 - Wrap up
In this chapter, we proposed a system design to support Google Drive. The combination of strong consistency, low network bandwidth and fast sync make the design interesting. Our design contains two flows: manage file metadata and file sync. Notification service is another important component of the system. It uses long polling to keep clients up to date with file changes.
Like any system design interview questions, there is no perfect solution. Every company has its unique constraints and you must design a system to fit those constraints. Knowing the tradeoffs of your design and technology choices are important. If there are a few minutes left, you can talk about different design choices.
For example, we can upload files directly to cloud storage from the client instead of going through block servers. The advantage of this approach is that it makes file upload faster because a file only needs to be transferred once to the cloud storage. In our design, a file is transferred to block servers first, and then to the cloud storage. However, the new approach has a few drawbacks:
• First, the same chunking, compression, and encryption logic must be implemented on different platforms (iOS, Android, Web). It is error-prone and requires a lot of engineering effort. In our design, all those logics are implemented in a centralized place: block servers.
• Second, as a client can easily be hacked or manipulated, implementing encrypting logic on the client side is not ideal.
Another interesting evolution of the system is moving online/offline logic to a separate service. Let us call it presence service. By moving presence service out of notification servers, online/offline functionality can easily be integrated by other services.
Congratulations on getting this far! Now give yourself a pat on the back. Good job!




---------------------------------------------------------------- Virus Scanner: 

Design a system where client can upload a file and viruses need to be scanned

    Create a system like:
    https://support.virustotal.com/hc/en-us/articles/115002126889-How-it-works
    a example could be : https://www.virustotal.com/gui/file/0bf01094f5c699046d8228a8f5d5754ea454e5e58b4e6151fef37f32c83f6497/detection

    so basically apis where a user uploads a file and is scanned by multiple virus vendors. Some of the requirements are:

    Ability to run scripts and virus scanners on the files to extract information
    what kind of workflow and scanning engine we need? how do we detect failures? how is data stored
    Addition of new scripts on the fly
    what kind of metrics we need for healthy monitoring
    non functional

    system should be real time
    system should be scalable for millions of users
    file size can be between 50 kb to 2 gb
    scanning service can be runned on different Operating systems. Not all scanning service are the same

Soln1:

    One way to approach this is to use a publisher/subscriber model. UI can upload the file(to any object storage like S3) and send a message to a queue(like kafka) 
    and all the virus scanners can be running on separate containers(that way the operating system constraint is met) and subscribe to the topic.
    Once they are ready with a result all of them can update to a DB(or use another topic if needed). Can use different APIs each to upload file, 
    gather results from all metrics and aggregate, fetch existing results based on file Hash
    The various metrics we could collect could be end-end time taken to return response to user, number of Virus scanner failures, 
    overall failures, response time for each scanner based on file size etc..

    (Upload will be dirrectly to S3 with presigned url. )

    S3 notification to a server generates kafka event -> containerized jobs -> Update results of scan in db directly 
    another offline job can check results of all the virus scanners and set the status of the file to is_clean. 

    (Review S3 uploads again + payments)

----------------------------------------------------------------

Github : Design search feature in Github scale code repository (todo)

Design search feature in Github scale code repository

    Requirements:

    Search with words in code repository
    Global scale
    Private code repository search.
    Preference to code owned by self or user's team.

---------------------------------------------------------------- Google Maps Sys design:

Read sys design vol 2 


    Routes are stored in raw GPS data.
    Shortestes route can be found by dijkstra algorithm.
    All road information and real time traffics should be represened in graph form.

    Images are represented and stored in at different zooms, to represent the same area 
    So zooom all the way out and earth is just 1 rasterized image, but zoom in and we can 
    render and rasterize each image, and stich them to form the view. 


    Having never worked for Google/MapQuest/etc, and having never really looked into it, I assume the way their shortest route finding software works is as follows:

    Partition the globe into large geographic features (states, provinces, regions, etc):
    Build a graph for each one of those

    For each graph, build graphs for smaller and smaller locations.

    This way, a local route can be calculated using Djikstra's on a relatively small graph, and a long trip can have its route found by using Djikstra's on a large in geographic area, but still small in nodes graph.


---------------------------------------------------------------- 
How to implement nearest location kind of functionality in a google map type application (TODO)

    I was asked this question in a start-up's interview:

    When we search in Google map, we type a string in the search bar and it takes to the location. In the map we can see places near by: like schools, colleges, hospital, stations etc.

    How can we implement such a functionality?

    Followup:

    You are given a file with millions of lines. Each line consists of a tuple:
    <string placename, double longitude, double latitude>

    How will you implement a function to search by a particular place (given its name in string)? I was asked not to use a database.

    I suggested using a quad tree for the first question where we divide the planet into uniform size grids. Each grid will have list of locations nearby.

    For the follow up question I asked if the data can come in memory or not. In case it can come in memory, then I suggested creating a 
    trie where each terminating node of a trie can store the list of nearby places.

    Any better answers or suggestions to approach this question..



----------------------------------------------------------------
Design a store locator that will return top N nearest stores from your current location (TODO):


    A quad tree is a 4-ary tree where each node has 4 children. A 2-d grid can be in the form a quad tree by taking the centre point as the root and assigning the 4 quadrants as its children. By recursively doing so we cover the entire grid based on the granularity we desire.

    Once such information is mapped, depending on the user's location, we can expand the quadrants until we reach a store. This can be done by storing the store ids in the quadrants. In order to make it efficient, if a quadrant has 0 or 1 store, we do not divide it further. In this way any person living in or on the edge of that quadrant will be directed to that store. Quadrants with more than 1 store can be divided further. This makes it easy to get the first nearby store in O(1). The overall runtime would be O(4*log4(N)).


    OR GEOHASH with the 9 squares idea. Redis has geohash, as well as ES and Sql. 

    Review this problem again .


----------------------------------------------------------------
    Design Alexa/Siri/Google Home Architecture (TODO):



----------------------------------------------------------------
    Design a Data Experimentation platform (TODO):

    Scope:
    Q: What does the experimentation platform do?
    A: You can use it run different versions of a program for clients to see, and collect data from their actions
    Q: What programs should it support? Experimenting on different programs probably have very different requirements to handle
    A: Many different types, as much as possible. Be flexible
    Q: In what format will experiment results be displayed as?
    A: Graphs, text, charts
    Q:

    Workflow:
    Starting with small scale: few users, testing a single version of experiment
    Database to store experiment results. There will be a lot of data that's collected will be reused in different experiments, so can use MySql.
    Server: to send client the version of the software you want them to use. Messages sent can be done using REST and Json containing the parameters for the experiment version.
    Client side: Add code that listens for experiment parameters, and to use them. Also add code to collect experiment results, like user time spent online with x version
    The UI for the platform: MVC architecture, model is db files, control is .js, view is html, css files

    At scale:
    Multiple servers with load balancing, each handling versions and experiment data for different programs and users
---------------------------------------------------------------- (todo);





---------------------------------------------------------------- Payment gateway: (todo)

Design your own Payment Gateway / Payment wallet / Payment ecosystem [ even you can assume you are the Bank but not mandatory ]
There can be many aspects and variations of payment systems. A Payment System is very broad , you can play a role of gateway/3rd Party/ provider/merchant...etc. Cook up your requirements and choose your approach

Note:

Choose your role and MVP. Drag to your conclusive system design. Your design should be Technical ( not on UI ) and around following topics :
a. Functional Requirements(in short) - description of the service , activities each user role can perform , features the system supports
b. Non-Functional Requirements (in short) - Performance / Availability / Scalability / Security Encoding / Maintainability / Extensibility
c. Any Extended Requirements you can think of is essential(in short) - Double spend / transaction timeout / Fraud detection
d. Design consideration -
Iimpose size limit on any data ?
Read and Write volumes and based on latency throughput , any trade-off you might choose to build your system
Authentication models best fitting your system and why ? How your system prevents anti-money laundering ?
e. Capaity , Traffic & Storage Estimation
f. Exposed and Internal APIs + Parameters
g. Entities & their attributes - e.g User: userId, name, email, address, dob, creationDate, lastLogin, etc.
h. Database Schema + description of essential relationship
i. Choice of your storage and tradeoffs - Metadata storage / File Storage / Cloud Storage, Transient Data Storage, Object Storage
j. Any special Data Structure or Algorithm needed for any specific purpose ?
k. Any specific service we need as a - BackGround/Off-line/Pipeline Services for Synchronization OR Message-durability-scalability-delivery
l. Component Interaction diagram
m. Data Partitioning , Sharding, Replication - Range Based / Hash Based / or any custom key scheme
n. Pull/Push/Hybrid the Notification
o. what you would Cache ? Justify your caching strategy ?
p. Reason you choices for Encodings , Permissions and Securing your aplication - Double spend / transaction timeout / Fraud detection

Related blogs / topics / architecture diagrams are enncouraged
----------------------------------------------------------------

autocomplete 2. 
Read system desing vol 1 


~24,000 query per second (QPS) = 10,000,000 users * 10 queries / day * 20 characters / 24 hours / 3600 seconds.
• Peak QPS = QPS * 2 = ~48,000

• Assume 20% of the daily queries are new. 10 million * 10 queries / day * 20 byte per query * 20% = 0.4 GB. This means 0.4GB of new data is added to storage daily.

Data gathering service
Let us use a simplified example to see how data gathering service works. Assume we have a frequency table that stores the query string and its frequency as shown in Figure 13-2. In the beginning, the frequency table is empty. Later, users enter queries “twitch”, “twitter”, “twitter,” and “twillo” sequentially. Figure 13-2 shows how the frequency table is updated.


To get top 5 frequently searched queries, execute the following SQL query:
  This is an acceptable solution when the data set is small. When it is large, accessing the database becomes a bottleneck. We will explore optimizations in deep dive.

The time complexity of this algorithm is the sum of time spent on each step mentioned above:
O(p) + O(c) + O(clogc)
The above algorithm is straightforward. However, it is too slow because we need to traverse the entire trie to get top k results in the worst-case scenario. Below are two optimizations:

1. Limit the max length of a prefix
2. Cache top search queries at each node Let us look at these optimizations one by one.


Limit the max length of a prefix
Users rarely type a long search query into the search box. Thus, it is safe to say p is a small integer number, say 50. If we limit the length of a prefix, the time complexity for “Find the prefix” can be reduced from O(p) to O(small constant), aka O(1).

Cache top search queries at each node
To avoid traversing the whole trie, we store top k most frequently used queries at each node. Since 5 to 10 autocomplete suggestions are enough for users, k is a relatively small number. In our specific case, only the top 5 search queries are cached.
By caching top search queries at every node, we significantly reduce the time complexity to retrieve the top 5 queries. However, this design requires a lot of space to store top queries at every node. Trading space for time is well worth it as fast response time is very important.
Figure 13-8 shows the updated trie data structure. Top 5 queries are stored on each node. For example, the node with prefix “be” stores the following: [best: 35, bet: 29, bee: 20, be: 15, beer: 10].

Let us revisit the time complexity of the algorithm after applying those two optimizations: 1. Find the prefix node. Time complexity: O(1)
2. Return top k. Since top k queries are cached, the time complexity for this step is O(1).
As the time complexity for each of the steps is reduced to O(1), our algorithm takes only O(1) to fetch top k queries.

Data gathering service
In our previous design, whenever a user types a search query, data is updated in real-time. This approach is not practical for the following two reasons:
• Users may enter billions of queries per day. Updating the trie on every query significantly slows down the query service.
• Top suggestions may not change much once the trie is built. Thus, it is unnecessary to update the trie frequently.
To design a scalable data gathering service, we examine where data comes from and how data is used. Real-time applications like Twitter require up to date autocomplete suggestions. However, autocomplete suggestions for many Google keywords might not change much on a daily basis.
Despite the differences in use cases, the underlying foundation for data gathering service remains the same because data used to build the trie is usually from analytics or logging services.
Figure 13-9 shows the redesigned data gathering service. Each component is examined one by one.

Analytics Logs. It stores raw data about search queries. Logs are append-only and are not indexed. Table 13-3 shows an example of the log file.

Aggregators. The size of analytics logs is usually very large, and data is not in the right format. We need to aggregate data so it can be easily processed by our system.
Depending on the use case, we may aggregate data differently. For real-time applications such as Twitter, we aggregate data in a shorter time interval as real-time results are important. On the other hand, aggregating data less frequently, say once per week, might be good enough for many use cases. During an interview session, verify whether real-time results are important. We assume trie is rebuilt weekly.

Aggregated Data.
Table 13-4 shows an example of aggregated weekly data. “time” field represents the start time of a week. “frequency” field is the sum of the occurrences for the corresponding query in that week.

Workers. Workers are a set of servers that perform asynchronous jobs at regular intervals. They build the trie data structure and store it in Trie DB.
Trie Cache. Trie Cache is a distributed cache system that keeps trie in memory for fast read. It takes a weekly snapshot of the DB.

Trie DB. Trie DB is the persistent storage. Two options are available to store the data:
1. Document store: Since a new trie is built weekly, we can periodically take a snapshot of it, serialize it, and store the serialized data in the database. Document stores like MongoDB [4] are good fits for serialized data.

2. Key-value store: A trie can be represented in a hash table form [4] by applying the following logic:
• Every prefix in the trie is mapped to a key in a hash table.
• Data on each trie node is mapped to a value in a hash table. Figure 13-10 shows the mapping between the trie and hash table.


 A search query is sent to the load balancer.
2. The load balancer routes the request to API servers.
3. API servers get trie data from Trie Cache and construct autocomplete suggestions for the client.
4. In case the data is not in Trie Cache, we replenish data back to the cache. This way, all subsequent requests for the same prefix are returned from the cache. A cache miss can happen when a cache server is out of memory or offline.


Trie operations
Trie is a core component of the autocomplete system. Let us look at how trie operations (create, update, and delete) work.
Create
Trie is created by workers using aggregated data. The source of data is from Analytics Log/DB.
Update
There are two ways to update the trie.
Option 1: Update the trie weekly. Once a new trie is created, the new trie replaces the old one.

Option 2: Update individual trie node directly. We try to avoid this operation because it is slow. However, if the size of the trie is small, it is an acceptable solution. When we update a trie node, its ancestors all the way up to the root must be updated because ancestors store top queries of children. Figure 13-13 shows an example of how the update operation works. On the left side, the search query “beer” has the original value 10. On the right side, it is updated to 30. As you can see, the node and its ancestors have the “beer” value updated to 30.

Delete
We have to remove hateful, violent, sexually explicit, or dangerous autocomplete suggestions. We add a filter layer (Figure 13-14) in front of the Trie Cache to filter out unwanted suggestions. Having a filter layer gives us the flexibility of removing results based on different filter rules. Unwanted suggestions are removed physically from the database asynchronically so the correct data set will be used to build trie in the next update cycle.


Scale the storage
Now that we have developed a system to bring autocomplete queries to users, it is time to solve the scalability issue when the trie grows too large to fit in one server.
Since English is the only supported language, a naive way to shard is based on the first character. Here are some examples.
• If we need two servers for storage, we can store queries starting with ‘a’ to ‘m’ on the first server, and ‘n’ to ‘z’ on the second server.
• If we need three servers, we can split queries into ‘a’ to ‘i’, ‘j’ to ‘r’ and ‘s’ to ‘z’.
Following this logic, we can split queries up to 26 servers because there are 26 alphabetic characters in English. Let us define sharding based on the first character as first level sharding. To store data beyond 26 servers, we can shard on the second or even at the third level. For example, data queries that start with ‘a’ can be split into 4 servers: ‘aa-ag’, ‘ah- an’, ‘ao-au’, and ‘av-az’.
At the first glance this approach seems reasonable, until you realize that there are a lot more words that start with the letter ‘c’ than ‘x’. This creates uneven distribution.


To mitigate the data imbalance problem, we analyze historical data distribution pattern and apply smarter sharding logic as shown in Figure 13-15. The shard map manager maintains a lookup database for identifying where rows should be stored. For example, if there are a similar number of historical queries for ‘s’ and for ‘u’, ‘v’, ‘w’, ‘x’, ‘y’ and ‘z’ combined, we can maintain two shards: one for ‘s’ and one for ‘u’ to ‘z’.

Interviewer: How can we support the trending (real-time) search queries?
Assuming a news event breaks out, a search query suddenly becomes popular. Our original design will not work because:
• Offline workers are not scheduled to update the trie yet because this is scheduled to run on weekly basis.
• Even if it is scheduled, it takes too long to build the trie.
Building a real-time search autocomplete is complicated and is beyond the scope of this book so we will only give a few ideas:
• Reduce the working data set by sharding.
• Change the ranking model and assign more weight to recent search queries.
• Data may come as streams, so we do not have access to all the data at once. Streaming data means data is generated continuously. Stream processing requires a different set of systems: Apache Hadoop MapReduce [6], Apache Spark Streaming [7], Apache Storm [8], Apache Kafka [9], etc. Because all those topics require specific domain knowledge, we are not going into detail here.
Congratulations on getting this far! Now give yourself a pat on the back. Good job!

















---------------------------------------------------------------- AUTOCOMPLETE VERSION 1 - ZOOKEEPER MAX USAGE. 

its easier to just read it the format is bad:
https://lopespm.com/2020/08/03/implementation-autocomplete-system-design.html



Requirements
The design has to accommodate a Google-like scale of about 5 billion daily searches, which translates to about 58 thousand queries per second. We can expect 20% of these searches to be unique, this is, 1 billion queries per day.

If we choose to index 1 billion queries, with 15 characters on average per query2 and 2 bytes per character (we will only support the english locale), then we will need about 30GB of storage to host these queries.

Functional Requirements
Get a list of top phrase suggestions based on the user input (a prefix)
Suggestions ordered by weighting the frequency and recency of a given phrase/query3
The main two APIs will be:

top-phrases(prefix): returns the list of top phrases for a given prefix
collect-phrase(phrase): submits the searched phrase to the system. This phrase will later be used by the assembler to build a data structure which maps a prefix to a list of top phrases
Non-Functional Requirements
Highly Available
Performant - response time for the top phrases should be quicker than the user’s type speed (< 200ms)
Scalable - the system should accommodate a large number of requests, while still maintaining its performance
Durable - previously searched phrases (for a given timespan) should be available, even if there is a hardware fault or crash
Design & Implementation


The two main sub-systems are:

the distributor, which handles the user’s requests for the top phrases of a given prefix
the assembler, which collects user searches and assembles them into a data structure that will later be used by the distributor


This implementation uses off-the-shelf components like kafka (message broker), hadoop (map reduce and distributed file system), redis (distributed cache) and nginx (load balancing, gateway, reverse proxy), but also has custom services built in python, namely the trie distribution and building services. The trie data structure is custom made as well.

The backend services in this implementation are built to be self sustainable and don’t require much orchestration. For example, if an active backend host stops responding, it’s corresponding ephemeral znode registry eventually disappears, and another standby backend node takes its place by attempting to claim the position via an ephemeral sequential znode on zookeeper.


Trie: the bedrock data structure
The data structure used by, and provided to the distributor is a trie, with each of its prefix nodes having a list of top phrases. The top phrases are referenced using the flyweight pattern, meaning that the string literal of a phrase is stored only once. Each prefix node has a list of top phrases, which are a list of references to string literals.

As we’ve seen before, we will need about 30GB to index 1 billion queries, which is about the memory we would need for the above mentioned trie to store 1 billion queries. Since we want to keep the trie in memory to enable fast lookup times for a given query, we are going to partition the trie into multiple tries, each one on a different machine. This relieves the memory load on any given machine.

For increased availability, the services hosting these tries will also have multiple replicas. For increased durability, the serialized version of the tries will be available in a distributed file system (HDFS), and these can be rebuilt via map reduce tasks in a predictable, deterministic way.

Information Flow
Assembler: collect data and assemble the tries
Client submits the searched phrase to the gateway via http://localhost:80/search?phrase="a user query"
Since the search service is outside the scope of this implementation, the gateway directly sends the searched phrase to the collector’s load balancer via http://assembler.collector-load-balancer:6000/collect-phrase?phrase="a user query"
The collector’s load balancer forwards the request to one of the collector backends via http://assembler.collector:7000/collect-phrase?phrase="a user query"

The collector backend sends a message to the phrases topic to the message broker (kafka). The key and value are the phrase itself 4
The Kafka Connect HDFS Connector assembler.kafka-connect dumps the messages from the phrases topic into the /phrases/1_sink/phrases/{30 minute window timestamp}5 folder 6
Map Reduce jobs are triggered 7: they will reduce the searched phrases into a single HDFS file, by weighting the recency and frequency of each phrase 8

A TARGET_ID is generated, according to the current time, for example TARGET_ID=20200807_1517
The first map reduce job is executed for the K 9 most recent /phrases/1_sink/phrases/{30 minute window timestamp} folders, and attributes a base weight for each of these (the more recent, the higher the base weight). This job will also sum up the weights for the same phrase in a given folder. The resulting files will be stored in the /phrases/2_with_weight/2_with_weight/{TARGET_ID} HDFS folder

The second map reduce job will sum up all the weights of a given phrase from /phrases/2_with_weight/2_with_weight/{TARGET_ID} into /phrases/3_with_weight_merged/{TARGET_ID}
The third map reduce job will order the entries by descending weight, and pass them through a single reducer, in order to produce a single file. This file is placed in /phrases/4_with_weight_ordered/{TARGET_ID}
The zookeper znode /phrases/assembler/last_built_target is set to the TARGET_ID
The Trie Builder service, which was listening to changes in the /phrases/assembler/last_built_target znode, builds a trie for each partition10, based on the /phrases/4_with_weight_ordered/{TARGET_ID} file. For example, one trie may cover the prefixes until mod, another from mod to racke, and another from racke onwards.
Each trie is serialized and written into the /phrases/5_tries/{TARGET_ID}/{PARTITION} HDFS file (e.g. /phrases/5_tries/20200807_1517/mod\|racke), and the zookeeper znode /phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/trie_data_hdfs_path is set to the previously mentioned HDFS file path.
The service sets the zookeper znode /phrases/distributor/next_target to the TARGET_ID


Transfering the tries to the Distributor sub-system
The distributor backend nodes can either be in active mode (serving requests) or standby mode. The nodes in standby mode will fetch the most recent tries, load them into memory, and mark themselves as ready to take over the active position. In detail:
The standby nodes, while listening to changes to the znode /phrases/distributor/next_target, detect its modification and create an ephemeral sequential znode, for each partition, one at a time, inside the /phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/ znode. If the created znode is one of the first R znodes (R being the number of replica nodes per partition 11), proceed to the next step. Otherwise, remove the znode from this partition and try to join the next partition.
The standby backend node fetches the serialized trie file from /phrases/5_tries/{TARGET_ID}/{PARTITION}, and starts loading the trie into memory.

When the trie is loaded into memory, the standby backend node marks itself as ready by setting the /phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/{CREATED_ZNODE} znode to the backend’s hostname.

The Trie Backend Applier service polls the /phrases/distributor/{TARGET_ID}/ sub-znodes (the TARGET_ID is the one defined in /phrases/distributor/next_target), and checks if all the nodes in all partitions are marked as ready

If all of them are ready for this next TARGET_ID, the service, in a single transaction, changes the value of the /phrases/distributor/next_target znode to empty, and sets the /phrases/distributor/current_target znode to the new TARGET_ID. With this single step, all of the standby backend nodes which were marked as ready will now be active, and will be used for the following Distributor requests.

Distributor: handling the requests for Top Phrases
With the distributor’s backend nodes active and loaded with their respective tries, we can start serving top phrases requests for a given prefix:

Client requests the gateway for the top phrases for a given prefix via http://localhost:80/top-phrases?prefix="some prefix"
The gateway sends this request to the distributor’s load balancer via http://distributor.load-balancer:5000/top-phrases?prefix="some prefix"
The load balancer forwards the request to one of the frontends via http://distributor.frontend:8000/top-phrases?prefix="some prefix"
The frontend service handles the request:
The frontend service checks if the distributed cache (redis) has an entry for this prefix 12. If it does, return these cached top phrases. Otherwise, continue to next step
The frontend service gets the partitions for the current TARGET_ID from zookeeper (/phrases/distributor/{TARGET_ID}/partitions/ znode), and picks the one that matches the provided prefix
The frontend service chooses a random znode from the /phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/ znode, and gets its hostname
The frontend service requests the top phrases, for the given prefix, from the selected backend via http://{BACKEND_HOSTNAME}:8001/top-phrases="some prefix"
The backend returns the list of top phrases for the given prefix, using its corresponding loaded trie
The frontend service inserts the list of top phrases into the distributed cache (cache aside pattern), and returns the top phrases
The top phrases response bubbles up to the client
Zookeper Znodes structure
Note: Execute the shell command docker exec -it zookeeper ./bin/zkCli.sh while the system is running to explore the current zookeeper’s znodes.

phrases
distributor
assembler
last_built_target - set to a TARGET_ID
distributor
current_target - set to a TARGET_ID
next_target - set to a TARGET_ID
{TARGET_ID} - e.g. 20200728_2241
partitions
|{partition 1 end}
trie_data_hdfs_path - HDFS path where the serialized trie is saved
nodes
0000000000
0000000001
0000000002
…
{partition 2 start}|{partition 2 end}
…
{partition 3 start}|
…
HDFS folder structure
Note: Access http://localhost:9870/explorer.html in your browse while the system is running to browse the current HDFS files and folders.

phrases
1_sink *- the searched phrases are dumped here, partitioned into 30min time blocks, *
{e.g 20200728_2230}
{e.g 20200728_2300}
2_with_weight - phrases with their initial weight applied, divided by time block
{TARGET_ID}
3_with_weight_merged - consolidation of all the time blocks: phrases with their final weight
{TARGET_ID}
4_with_weight_ordered - single file of phrases ordered by descending weight
{TARGET_ID}
5_tries - storage of serialized tries
{TARGET_ID}
|{partition 1 end}
{partition 2 start}|{partition 2 end}
{partition 3 start}|



----------------------------------------------------------------

Facebook | System Design Onsite | Compute Percentile Metrics Over Time-Series (TODO)



Design a metric-collection system (e.g. Datadog) which allows clients to query for the different percentiles (50th, 90th, 99th) 
over different periods of time - 1 minute, 5 minutes, 30 minutes and so on.

    

    Metrics are pulled from endpoint that is installed in each service. /metrics and service installs our library such as prometheus metrics.. 
    Then we can use promql with a timeseries database. 

    


    Store metrics in C* sorted by timestamp clustering key, and partition keyis the unqiue metric name/id

    To query data, we can keep some of these views hot in redis.. for the last minte, 5 mins, 30 mins, ... agg to get longer views. 
    Precompute for each opeation -> Max latency/mid/etc... 
    To get the percentiles.. we only provide a few views, 50, 90, 99.. -> keep in seperate redis stores. 









----------------------------------------------------------------


Given a log file with HTTP requests to a server. The log file is very huge 100 TB, you have to filter the 
requests which has 200 success status. How can we approach this question?

The log file has timestamp, request type and status. You can assume that it will look like the apache server log.


Soln 1:

    Assumption:

    Each request has an id, URL, request timestamp and response timestamp and status code
    The log is ordered by timestamp.
    Clarify question:

    The above information is in the same or different line?(assume answer is latter)
    What about requests with the same URL? Are they required to be stored separately?(assume answer is yes)
    Do we need to store URL separately to save space?(assume answer is yes, shorten URL)
    Is it guaranteed there is always status code for the request?(assume answer is no).
    Follow up with 4 if is answer no, what's the max timeout? (How to cleanup the caching while streaming.)
    How many different requests are there? (assume it's with query string so a lot)

    Idea:
    Streaming to parsing log file and distributed data storage. Or use existing distributed database: Cassandra

    Approaching:

    Parsing:
    a. Paring the log file line by line, maintain a map with request id as key.
    b. If the map size is too big, trigger cleanup to kick out expired requests.
    c. Once get response code, remove out of map and store it.
    d. Use Kafka for streaming, one producer paring the file, use multi consumers subscribe by sharding request id to deal with storage task.

    Store URL
    a. Generate a shorter URL. Based on how many different URLs, decide what's URL id length is. One example is MD5 encode URL, cut to the length and convert with 64 characters [0-9][A-Z][a-z][-.]. Add one character to resolve collision.
    b. URL is sharded by URL id
    c. Since URL id is consistent as X, load starts from X[0] to X[.] until there is no URL associated to the id. If URL already exists, use it. Otherwise use first empty one as new id.

    Store request info
    a. Request is sharded by request id hashing
    b. request id, url id, request timestamp, response timestamp and status code are stored there

Many ways can be used. ELK; HDFS/HBase/Hive/Spark; NiFi etc. If you prefer to coding and craft way, any language to implement something like Lex. Of course you need to consider of smart indexing hierarchy on time and type of log records.





Will a simpler Map Reduce do the job for us?

I am assuming that the file is on the server and the user will be sending the message to filter. Hence I divide the file into smaller chunks and pass to it multiple machine. (Map part) Job of each machine is to find the page_no/file_name/line_no, timestamp where the status appears. (Reduce part). Aggregate and sort the results of the mapper and return then to the user. Since the file is so large, I do not expect the user to look for results immediately.

If we want to reduce the latency of the operation, we can process the first few pages separately and return them to the user, process the remaining results using the above approach in the background.

I have worked on something very similar. You can use ELK stack and feed your large log data into ElasticSearch using filebeat and Logstash. Then use ElasticSearch python library for log parsing directly from ES. Look at the code here: https://github.com/ritwik12/Satellite-Log-Data-Analysis

Also, have a look at this blog if you don't want to use ELK: https://ritwik12.github.io/Blog/Optimization/







SPARK TUTORIAL:

        Spark job -> filter operation. 

        employee_data = [("1","John",28,5000),
                        ("2","Smith",30,6000),
                        ("3","Adam",35,4000),
                        ("4","Henry",40,7000)]
                        
        employee_rdd = spark.sparkContext.parallelize(employee_data)
        filtered_rdd = employee_rdd.filter(lambda x: x[3] > 5000)
        print(filtered_rdd.collect())  # Output: [('2', 'Smith', 30, 6000), ('4', 'Henry', 40, 7000)]

        employee_df = spark.createDataFrame(employee_data, ["ID", "Name", "Age", "Salary"])
        filtered_df = employee_df.filter(employee_df.Salary > 5000)
        filtered_df.show()

        mapped_rdd = employee_rdd.map(lambda x: (x[0], x[1].upper(), x[2], x[3]))
        print(mapped_rdd.collect())  # Output: [('1', 'JOHN', 28, 5000), ('2', 'SMITH', 30, 6000), ...]

        # Assuming df is your DataFrame and it has four columns: ID, Name, Age, Salary
        # Convert DataFrame to RDD
        rdd = df.rdd
        # Perform the operation
        mapped_rdd = rdd.map(lambda x: (x[0], x[1].upper(), x[2], x[3]))
        # Convert RDD back to DataFrame
        # Assuming you know the schema of your DataFrame as ("ID", "Name", "Age", "Salary")
        df_upper = spark.createDataFrame(mapped_rdd, ["ID", "Name", "Age", "Salary"])
        # Collect results
        df_upper.show()

        total_salary = employee_rdd.map(lambda x: x[3]).reduce(lambda a, b: a + b)
        print(total_salary)  # Output: 22000

        df total_salary_df = employee_df.agg({"Salary": "sum"}).collect()[0][0]
        print(total_salary_df)  # Output: 22000

        from pyspark.sql import functions as F
        # Assuming employee_df is your DataFrame and "Salary" is the column you want to sum
        total_salary = employee_df.agg(F.sum("Salary")).collect()[0][0]
        print(total_salary)  # Output: The sum of all salaries


        Here are some of the key takeaways:

        Filtering: This process involves selecting specific elements from a dataset based on defined criteria. In PySpark, this can be achieved using the filter() transformation on both RDDs and DataFrames.
        Mapping: Mapping involves applying a function to each element in a dataset to create a new one with the results. PySpark supports this operation using the map() transformation, but only on RDDs. For DataFrames, one would need to convert the DataFrame to an RDD, perform the mapping, and convert it back to a DataFrame. Alternatively, and preferred method is, one can use PySpark functions to achieve the same functionality without having to convert DataFrames into RDDs.
        Reducing: This process aggregates the elements of a dataset down to a single value. In PySpark, this can be achieved using the reduce() action on RDDs. For DataFrames, one would use aggregation functions to achieve similar results.





----------------------------------------------------------------


Amazon | System Design | Web Crawler Detector

 was asked this question during a system design interview at Amazon Bangalore.

Imagine you have a website operating at the scale of Amazon or flipkart. We have to design a system that helps detect the bots or crawlers crawling our website. How can we do this?

This could be a huge discussion overall. So, we can discuss about it one by one. I would 1st of all, we need to break the problem into 2 half: 1) a set of rules to identify whether a series of requests from one IP address or multiple follows a suspicious pattern. 2) Whether we are talking about a passive detection or an active instrumentation. Obviously. the set of patterns used and the scale of the problem itself will dictate the 2nd choice.

1st) As far as patterns are concerned, there are multiple things to considers. a) A crawler will very likely to be a distributed crawler. These crawlers exists that operate in a clustered fashion to allow the sites gateways to not automatically detect the bot. b) A crawler will very likely use a bunch of proxies to route the requests to be not detected. c) A crawler will very likely use a different engine for indexing the pages and different for crawling to introduce a randomness to the crawl. d) A crawler might distribute the crawl into small chunks of manageable jobs to be executed at different times to allow large scale crawl.

2nd) An active instrumentation might only be bothered about one IP address on one gateway. It can also extend to a set of local gateways but cannot go beyond that. The tradeoff can be defined between a potential false positive, speed of detection and overall impact on performance. A passive instrumentation might be offloading the access metrics to a separate system for identifying the potential patterns. These patterns can also be potentially using some machine learning algorithms from the previous patterns or it could be merely based on the frequency of visits from a series of random IP addresses.

Note that some of the Application gateways have features like rate limiting, throttling, back-pressure, DDNS detection etc, which also follows similar approaches.



I guess I would answer this along the lines of a rate limiter.

I would suggest a simple rule such as "100+ requests from a single IP within a minute" results in an additional verification step to make sure requests are coming from a human. I dont think it is necessary to device anything more complex than that within the 30 minutes you have.

So question becomes a problem of integrating an existing system with a distributed lookup table for "IP -> # of requests from that IP over last 60 secs". If the lookup succeeds, request is dispatched within the system as usual. Otherwise, it is forwarded to a service that performs human verification steps (whose details I would say is outside the scope of the interview). If verification fails, you notify your initial entry point of the system to black list the IP for a while so subsequent requests get discarded immediately.


This is a classic anomaly detection problem.
Step 1: Track the behavior and journey breadcrumbs of actual users on the website. These would be known users like testers, business users, actual clients visiting the website.
Step 2: Once you have enough data to construct a model representing user persona, you can deploy that model to check anomalous behavior.

A good way to track anomalous behavior is time between requests, request headers, CTAs, Duration between requests, etc This forms the basis for your model/models
Step 3: Track "änomalous" requests over time and create a model for that as well. We need to make sure we dont reject valid requests (false positivies).
Step 4: For each request, evaluate the behavior with both models to determine anomalous behavior and then make the decision to backlist.
----------------------------------------------------------------

RATE LIMITER:
 -> Review this in inteview cheat sheet system desing or alex xu he has good explanatiions


The token bucket algorithm can be used for this purpose. A rough description follows:

Associated to each clientId, assume a bucket of tokens, that has a capacity of 100 tokens and a continuous inflow of 100 tokens per second (or 1 token every 0.01s). Each request from a client takes a token from the bucket with that clientId. If that bucket is empty, the request should not be allowed.

----

Build distributed database:

3. Stateless applications
I previously wrote about the local state being a cloud anti-pattern. This is even more true for multi-region architecture. When clients interact with an application they do so in a series of interactions called a session.

In a stateless architecture, the server must treat all client requests independently of prior requests or sessions, and should not store any session information locally. So given the same input, a stateless application should provide the same response to any end-user.

Stateless applications can scale horizontally, since any request can be handled by any available computing resources (e.g. instances, containers or functions).


In 2013, Netflix famously talked and wrote about testing Cassandra in multi-region setup, writing 1 million records in one region of a multi-region cluster, followed by a read, 500ms later, in another region, while keeping a production level of load on the cluster — all that without any data loss.

4. Use local resources and avoid cross-regional calls
As mentioned previously, preventing increased latency is critical for applications. Therefore, it's important to avoid synchronous cross-region calls and always make sure resources are locally available for the application to use, thus optimising latency.

For example, objects stored in an Amazon S3 bucket should be replicated in multiple regions to allow for local access from any region. Luckily, Amazon implemented the feature cross-region replication for Amazon S3. Cross-region replication is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.


Separating the writes from the reads across multiple regions will improve your disaster recovery capabilities, but it will also let you scale read operations into a region that is closer to your users — and make it easier to migrate from one region to another.

The main restriction with this pattern is that all critical writes traffic must go to one single master, in the region of origin.


----------------------------------------------------------------




----------------------------------------------------------------

Facebook | System Design | A web crawler that will crawl Wikipedia


We need to deploy the same software on each node. We have 10,000 nodes, the software can know about all the nodes. We have to minimize communication and make sure each node does equal amount of work.

The tough part is to equally distribute workload without centralized components. I think Consistent Hashing is a good example.
Basically, group all nodes into a ring. Randomly distribute them onto that ring. Each node is only responsible for its left hand arc. For any url, hash it onto the ring as well, so that only one node will process it. This way we don't need a centralized hash table to avoid repeating. Also, the distribution is roughly balanced over each node. When a node visits a page, for each wiki link, hash it onto the ring so that another node will process it.

Yes, this is a consensus problem. I remember the question has a limitation that we can not have any sort of centralization.

You have a point about a node dying in between, its work can possibly be duplicated, but maybe that is desirable? The problem didn't mention anything about that.

Finally, i think each system design problem has countless problems, At the end of the day, an interview is about the scope we have to tackle, and the trade offs of each solution. We just need to balance them out as much as possible :)


ANOTHER SOLUTION:

A Web crawler system design has 2 main components:

The Crawler (Write path)
The Indexer (Read path)
Begin with these high level components, then break them down into smaller sub-components, then connect these to form a coherent whole.
Then talk about caching of Search results to speed up querying, ranking of results, detection of cycles/handling of broken links/handling of sites that are temporarily down when crawling, Robots.txt, throttling when crawling a web site (if you're Google, you don't want your gazillion crawlers to drown a website by attempting to crawl it in parallel) etc. Those are the web crawler challenges to solve.
BTW: Your solution mentioned Zookeeper. By your own (warped) logic, if you were the interviewer you'd have asked the poor candidate to design Zookeeper as well. I wouldn't. I'd just ask him the purpose of Zookeeper (only if I wasn't aware of it). I would want the candidate to focus on the above problems. They're no way related to "consistent hashing" or "L4/L7 load balancers" etc.

Did the interviewer specifically mention that you can't use a messaging queue like SQS? Not sure why exotic solutions like consistent hashing are being mentioned here again and again.

In a System design question, understand the scope of the problem and stay true to the original problem.
The scope was to design a web crawler using available distributed system constructs and NOT to design a distributed database or a distributed cache.

A Web crawler system design has 2 main components:

The Crawler (Write path)
The Indexer (Read path)
Make sure you ask about expected number of URLs to crawl (Write QPS) and expected number of Query API calls (Read QPS).
Make sure you ask about the SLA for the Query API. If its in tens of milliseconds at say 90 percentile, you'll probably need to cache the query results.

Begin with these high level components, then break them down into smaller sub-components, then connect these to form a coherent whole.

Crawler

For crawling you need a "seed of URLs" to begin the crawling. You'd want to put the URLs in a queue.
The queue workers would work on one URL at a time. Each queue worker, given a URL has to:
**Extract text from the URL and send it to a Document Indexing Service .
**Insert any links found in the page back into the queue. Before inserting, the links are looked up (and stored) in a Global NoSql store, to ensure they weren't already crawled. We use a NoSql store (and not a SQL Database) because we're doing lookup operations only and don't require expensive joins.
Eventually the queue will become empty. At this point, the "seeder" will reseed the queue with seed URLs and the whole process restarts.

Scaling up the crawler (only if the websites to crawl are in billions):
Your queue could be a distributed message queue (such as SQS or Azure ServiceBus).
Your NoSql store could be DynamoDB.
The interviewer would most likely know that both message queues and NoSql stores maintain replicas (typically master-slave replication) for fault tolerance and re-partition themselves via an algorithm like consistent hashing for scalability.
All distributed queues have a Visibility Timeout i.e. when an element is dequeued, it still remains in the queue but is made invisible to other dequeue requests till Visibility Timeout seconds have elapsed. A worker that is handling the dequeued element must explicitly delete it from the queue before Visibility Timeout seconds.

Challenges for Crawler:
How would you handle throttling from your NoSql store (say because you have too many crawlers attempting to lookup and write URLS to it)? If you try an exponential retry algorithm in your worker, your message queue may release the URL being already crawled to another worker.
How would you handle dead links? You'd probably want to maintain a blacklist of links that returned 404 so that your workers don't put these in crawler queue the next time around.
How do you handle crawling of temporarily offline websites. Assume that your worker connected to the website but it took 40 seconds to respond with 503 but by that time, your message queue already released the same URL to another worker who'd attempt to reconnect to the same website and suffer from the same fate.
How would you handle websites that failed to get crawled? You'd probably want to store them in a Dead Letter Queue to be inspected later.
Would you respect Robots.txt files while crawling? Maybe you could store the domain name + /* of such sites in the Blacklist NoSql Store.
How would you throttle requests from your crawlers running in parallel to different pages on a single website (like Wikipedia). Maybe a message queue is not a right fit in this design? You could probably use a Streaming queue (like Kafka/Kinesis Streams/ Azure EventHub) where the domain name of the URL is the partition key. This means that all sub-URLs within a domain will be handled by one worker only. But this leads to obvious load balancing issues. Alternatively, you could invest in a Rate Limiter that ensures that one worker does not open more than n connections to a single website. But what is a good value of n? Wikipedia can probably handle thousands of concurrent connections but a small company's website could cave in. So the value on n depends on the domain being crawled and will need tweaking via trial and error. Which means you'll need another NoSql store that stores domain names and n which the RateLimiter will need to cache when doing the rate limiting. Next question: what should the worker do if the Rate Limiter disallowed it from accessing the URL? Should it keep retrying? If yes, what if the message queue releases the same URL to another worker? It makes sense for the worker to drop it and go for the next URL in the queue. This will cause the message queue to release this current URL to another worker after Visibility Timeout seconds, and that worker might have a higher chance of succeeding. But what if the next URL is also of the same domain?
All in all, you should also discuss what logs/metrics you'd emit and how these are analyzed to make the crawling better. Some metrics to emit would be how many times a worker was rate limited, latency of every operation (such as reading entire contents a URL, time taken for ranking the text, indexing it etc.). For websites that caved in or were unresponsive etc, apart from the metrics, you'd also write them in special log files which are then machine learned to produce the Blacklist NoSql store or to recompute the number of connections for a domain to be used by the Rate Limiter.

Indexer

You'll need a Document Indexing Service that does 2 things on its write path:
** Insert the URL and its text into another NoSql store. This will be used for showing cached copies in case the original URLis unavailable or is dead. But what if the text is huge? In that case, it makes sense to store the text instead in an Object/Blob Store like S3/Azure Blob Storage under a key which is the hash of the URL.

** Maintain a Reverse Index that maps keywords/phrases in the text back to the original URL. You can use a Reverse Index database like Elastic Search.
The Document Indexing Service also exposes a query API that takes as input, a phrase + number of results (i.e. page size) to return. It would then break the phrase into keywords, remove "Stop words" (words like the, a, an etc), correct spelling mistakes in the keywords, add synonyms of certain keywords and then call into Elastic Search to return the URLs that contain these keywords. Elastic Search has plugins which already do spelling corrections, stop word removal etc. The URLs could then be fed into a Ranking service that ranks the URL before returning. If your Read QPS is higher than Write QPS, a better approach would be to do the ranking via an hourly offline process and store the rank in Elastic Search. That would speed up the querying path as you'd skip ranking and instead ask Elastic Search to return URLs sorted by rank. This also makes it easy to paginate as the first page will contain the top n results, the next page will contain the next n results etc. The hourly offline ranker would need what are called as "Clickstream logs" to figure out which links are being clicked more often so as to rank them higher.

The query API must also return a pagination token to allow the caller to continue retrieving more pages

Challenges for Indexer:
Can we speed up the read path by maintaining a cache for most commonly queried phrases? The 80/20 rule states that 80% of users query the same 20% phrases. If so, what caching strategy to use? LRU/TTL? Should we go with a write through cache so that the crawler directly updates the cache? Or a cache-aside strategy where the cache is bypassed by the crawlers when they write to the Document Service? In that case, what TTL value would be appropriate for the cache?
What kind of distributed cache would we use? Redis/memcached? Only if asked, you can mention about consistent hashing here.
That's about it...No consistent hashing/zookeeper etc required.


You're on the right track regarding "Clickstream" logs. Essentially search engines like Google log which links from the search page you clicked. There are techniques for them to find out the "dwell time" (i.e. how long you stayed on the link you clicked) as well. The "dwell time" allows Google to figure out how relevant the link is to the keywords that were searched. If the link was clicked by mistake or is irrelevant you're likely to close the web page quickly and return to the Search page. Otherwise you'll dwell on that page for quite some time.

This information (keywords you typed + links you clicked + dwell time) is then fed into the *Ranker *. If the Ranker notices that many users are clicking the third link in the Search Query result for a set of keywords, and the "dwell time" is high, it will rank it higher. Eventually the most "relevant" link would be on top. At least that's the hope. Of course, Google's ranking looks at various other signals (many of them are proprietary secrets) but that's the gist. I'm guessing they also use your Google user name (if you're logged in) or your IP address to make the search results more of your taste ("personalization"). Ranking + personalization is a phenomenally complex problem and apart from ML algorithms, requires a large amount of user clickstream data to constantly train the AI to keep making it better. This large amount of user data gives Google the edge over other search engines like Bing, specially for unusual queries.

I've already listed several bottlenecks above.










--------------------------------
Hi All, i want to write Algorithm for upload Large file (Ex: 1G) like google drive.

Another optimization along with previous answers you can do is, when you break the file into chunks, create a metadata of chunks and send it to the server as the first step. Doing this would allow to only upload the failed chunks in case uploading stops. Also, it would allow you to concurrently upload the chunks and merge them using metadata. This would also give you the flexibility of a very large file being uploaded to multiple servers at the same time.

Also rmbr to checksum for corruption + COMPRESSION 

You basically want to divide the file into chunks of size that is acceptable in the memory where the file is being uploaded and also can be transfered via network.
So you create a stream between source and destination and chunk the file into smaller size say 25 MB, and keep sending this chunks untill everything is transfered.

P.S You might wonder, why leetcode has a question along these lines : Read N Characters Given Read4

presigned urls:
YOu write the file stream directly to dfs/s3/ etc.

At the client side where user select the file to upload, calculate the file size and divide the file in multiple chunk.
calculate the checksum as well to make sure at the end when file created successfully, checksum can be matched.
Every chunk will go as a service call to the server where the file being processed and stored to a filesystem.
use octet-stream rather than multipart.

Split up and compress. The storage industry has dubbed the compression algorithm deduplication. Most folks here advised you to split up the file into smaller chunks, which is fine. Though, you should create a worker that finds mathematical patterns in those chunks and creates a hash table to decrypt the breaking down the files even more when you compress them. If you don't want to reinvent the wheel, you may consider system commands and old utilities like zip for this. 

What are the other parameters? Is it a service uploading to one machine? or multiple machines? IN that case, P2P is your friend. Break down your file. 1G is not a large file for most production environments. It should be large enough to choke up the network bandwidth. that is when things will get interesting ( from an interview point of view ). Also you might wanna clarify the details of type of file.

----------------------------------------------------------------

Design a High Availability system for a HTTP proxy. Need to keep in mind the properties and requirments of http.






----------------------------------------------------------------

How would you design a system for video pause functionality for multiple devices? For eg: If you are logged in your mbile and watching a video and you paused it and later you used your laptop to log in and watch the video. Your video should start resuming from where you paused off.

    HLD:
    Use redis/memcache to store the session of the user as list with (Video Id: Time_Seen) when ever user exits application, whenever a user clicks a video check if the video id is in redis, if yes then skip to the particular time by appending it to the video stream request else video will play as usual. Redis can have an eviction limit which can be agreed up on by SLAs


    Another soln: 


    Requirements
    Functional
    1. Instantaneous Pausing and Resuming
    Sub-millisecond Response: The feature should allow users to pause and resume content almost instantaneously upon command, ensuring a seamless viewing experience.

    2. Consistent Playback State
    Platform Synchronization: Whether switching devices or browsers, the feature must maintain the playback state, allowing users to resume precisely where they left off.

    Non Functional
    1. Reliability and Availability
    Continuous Service Availability: The pause and resume function should be available consistently, ensuring users can access it whenever needed without downtime.

    2. Performance
    Minimal Latency: Ensuring minimal delay between user action (pause/resume) and system response to maintain a seamless viewing experience.

    3. Scalability
    Elasticity: The system should effortlessly accommodate the growing user base and increased traffic without compromising performance.

    Data Model
    There are 3 entities, out of which 2 of them are supporting entities along with 1 entity that captures are functional requirements

    User Entity

    Fields

    UserId (10 bytes)

    DeviceId (10 bytes)

    Total ~ 20 bytes

    Content Entity

    Fields

    ContentId (10 bytes)

    Title ( 50 bytes)

    Description (200 bytes)

    S3Url (100 bytes)

    Total ~ 360 bytes

    Playback Entity (Many-to-Many relation)

    Fields

    UserId (10 bytes)

    ContentId (10 bytes)

    PlaybackPosition (Integer 4 bytes) - Stores the video frame stamp which will be used by the video service.

    PlabackStatus (Enum 1 byte) - status being Playing, Paused, Stopped etc

    TimeStampOfInteraction (DateTime 8 bytes)

    Total ~ 33 bytes

    Back Of The Envelop Estimates
    Users & Traffic

    Assume 10,000 concurrent users during peak hours

    It is a write heavy system since update for playback point will be more frequent than resume. Assume read to write ration of 1:100

    Each user performs 3 interactions (pauses, resumes) per hour on average

    Total Requests = 10,000 * 3 = 30,000 requests per hour

    Queries Per Second

    The number of requests per second = 30000/3600 ~ 10 TPS

    Storage

    We only estimate storage for playback entity as that is pertaining to our feature

    30000 * 33 bytes ~ 990Kb per hour ~ 712Mb per month

    Bandwidth

    These bandwidth estimates ignores metadata, headers and request/response messages

    The number of requests per second (read+write) * size of the playback entity = 10*33bytes ~ 330 Bytes per second

    Memory

    An MRU cache that stores the playback entity for one hour will be of memory ~ 330 per second * 3600 second ~ 2MB

    From the estimation this is clear that it is not a heavy workload system, but we will still design for scalability to handle big releases and unexpected traffic.

    High Level Design

    This is an API based synchronous system design, the diagram above shows some basic design components (Load Balancer, Webserver etc.) along with feature specific service. The basic components will never be covered in my posts and can be googled for better understanding.

    PlayBack Service - A playback point is updated with a request to the write path of playback service, whereas last known frame is retrieved by using the read path of the service. Read more about this service in low level design.

    Video Service - This service is out of scope of this feature and is only shown in the diagram to complete the flow, this service is responsible for managing content and video in the cdn.

    Database - The choice of database here is not really hard. Since ACID properties are not required and eventual consistency is fine one can go with a NoSQL database (AWS Dynamo DB), on the other hand since the scalability requirement is not much one can also opt for a SQL db (AWS RDS). For the purpose of the interview/post I will opt for relational DB.

    Cache - A redis mem cache will suffice here.

    Low Level Design
    The workflow of the pause and resume feature is crucial. Here's a breakdown of the workflow:

    Event Trigger - Pause/Resume:

    When a user initiates a pause or resume action while streaming content, an event is generated on the user interface (UI) or device application. This would work in a vanilla case, but what if the user’s device dies before any event is generated, in that case we will use something called as heartbeat. At regular intervals the device will generate an event and will send it to the playback service.

    Event Transmission:

    The event data, specifying the type (pause or resume) and the content being viewed, is transmitted to the backend servers.

    Playback Service Processing:

    Upon receiving the event, the service (e.g., through read/write APIs or services) process this data, identifying the user, the content.

    Data Update and Storage:

    The system updates the playback status or position associated with the specific user and content, storing this updated information in the database or a designated storage mechanism.

    Resumption Protocol:

    For resume events, the service fetches the last known playback position from the database and resumes playback from that point onwards. The playback service provides the best last known position, for eg in the event of device failure, we would have the last heartbeat we got from the device, therefore the choice of our heartbeat frequency will decide the accuracy of the resumption.

    Acknowledgment to User Interface:

    Once the update is complete, the backend sends an acknowledgment or response to the user interface or device, confirming the successful processing of the pause or resume event.

    API Model

    namespace playback

    // Definition of a unique identifier for users and content
    string UserIdentifier
    string ContentIdentifier

    // Structure for the timestamp
    timestamp PlaybackTimestamp

    // Structure representing a request to pause content
    structure PauseContentRequest {
        user_id: UserIdentifier,
        content_id: ContentIdentifier,
        timestamp: PlaybackTimestamp,
        frame: Frame
    }

    // Structure representing a request to resume content
    structure ResumeContentRequest {
        user_id: UserIdentifier,
        content_id: ContentIdentifier,
        timestamp: PlaybackTimestamp
    }

    // API definitions
    service PlaybackService {
        version: "2023-12-25",

        operations: {
            PauseContent: {
                input: PauseContentRequest,
                output: PauseContentResponse
            },
            ResumeContent: {
                input: ResumeContentRequest,
                output: ResumeContentResponse
            }
        }
    }

    // Response structure for pausing content
    structure PauseContentResponse {
        user_id: UserIdentifier,
        content_id: ContentIdentifier,
        timestamp: PlaybackTimestamp,
        frame: Frame
    }

    // Response structure for resuming content
    structure ResumeContentResponse {
        user_id: UserIdentifier,
        content_id: ContentIdentifier,
        timestamp: PlaybackTimestamp,
        frame: Frame
    }


    Design Alternatives
    In the dynamic realm of streaming platforms, exploring diverse design approaches is akin to unlocking a multitude of possibilities. Designing the architecture for pause and resume functionalities beckons consideration of various alternatives, each offering unique advantages in shaping user experiences. Whether leveraging asynchronous event-driven paradigms, synchronous processing, or hybrid models blending both worlds, the quest lies in crafting a system that seamlessly caters to user actions while maintaining scalability, responsiveness, and fault tolerance. Until now we saw a synchronous approach (API based), now let's delve deeper into these design alternatives, dissecting their intricacies to unravel the optimal approach for enhancing the streaming experience.


    In shifting from the previous synchronous model to this innovative hybrid approach, a fundamental evolution emerges in how pause and resume functionalities are orchestrated within the streaming platform. While the prior synchronous system necessitated immediate processing for all actions, the new design strategically differentiates between pause and resume workflows. Here, the asynchronous architecture (using Amazon SQS) for pause actions liberates the system from immediate processing constraints, fostering scalability and fault tolerance. Simultaneously, the synchronous handling of resume actions prioritizes immediacy, significantly reducing latency and enhancing user interaction for crucial playback resumptions. This distinct hybrid model intricately balances efficiency and responsiveness, catering to the intricacies of pause and resume behaviors while fortifying the platform's performance under varying workloads.

    Comparative Analysis: Hybrid (Async for Pause, Sync for Resume) vs. All-Synchronous Approach

    Latency Optimization:

    Hybrid Approach: Reduces latency for pause actions by employing asynchronous processing and the response will be generated as soon as event s pushed to SQS, ensuring a more immediate response to user requests.

    All-Synchronous Approach: May experience increased latency for all actions due to synchronous processing, potentially impacting user experience, especially for pause actions.

    Scalability and Performance:

    Hybrid Approach: Utilizes asynchronous processing for pause actions, enhancing scalability and fault tolerance in handling high volumes of pause requests.

    All-Synchronous Approach: Might encounter scalability challenges under heavy loads, as synchronous systems handle all actions immediately, potentially leading to performance bottlenecks.

    Decoupling and Flexibility:

    Hybrid Approach: Decouples pause actions with asynchronous processing, offering flexibility and independence in handling pause events, enabling fault isolation and scalability.

    All-Synchronous Approach: Inherently more tightly coupled, potentially impacting the ability to scale independently and leading to interconnected dependencies between components.

    Fault Isolation and Resilience:

    Hybrid Approach: Enhances fault isolation as pause actions are decoupled and processed asynchronously, reducing the risk of cascading failures.

    All-Synchronous Approach: Might pose challenges in fault isolation, as immediate synchronous processing could propagate failures across interconnected components.

    Potential Bottlenecks
    Bottleneck: Surge in Traffic due to Celebrity-Endorsed Content:

    Scenario: A newly released video, heavily promoted and endorsed by a popular celebrity, garners unprecedented attention, leading to an influx of users attempting to access and watch the content simultaneously.

    Impact: The sudden spike in traffic results in an overwhelming load on the pause and resume functionalities of the streaming platform, causing performance degradation, increased latency, and potential service disruptions.

    Challenge: The system struggles to handle the sudden surge in pause and resume requests, impacting the overall user experience for accessing the endorsed content.

    Solution:

    Distribute Traffic: Utilize load balancers to evenly distribute incoming traffic across multiple servers or instances. This prevents any single component from being overwhelmed by the surge in requests, balancing the workload.

    Predictive Scaling: Utilize predictive analytics or historical data to anticipate traffic spikes. Preemptively scale up resources before the surge occurs, ensuring readiness to handle increased loads.

    Bottleneck: Concurrent User Requests for Resume at Specific Playback Positions:

    Scenario: A widely popular TV series releases its newest season on the streaming platform, attracting a massive audience. Concurrently, a significant number of users attempt to resume watching the series from the exact playback position where they left off.

    Impact: The surge in users requesting to resume from specific playback positions simultaneously strains the synchronous system's capability to handle multiple concurrent requests.

    Challenge: The synchronous processing for these resume actions, especially at precise timestamps or positions within the content, leads to increased contention and potential bottlenecks in accessing and updating these granular playback states in real-time.

    Effects: Elevated latency and potential service degradation occur as the system struggles to fulfill numerous precise resume requests promptly.

    Solution: Employing optimizations such as implementing efficient indexing in the database for playback positions, optimizing read and write operations, or introducing caching mechanisms specifically for frequently accessed playback positions can alleviate this bottleneck. Additionally, considering a hybrid model where resume actions are handled synchronously while less critical operations like pause are processed asynchronously could enhance the system's responsiveness.

----------------------------------------------------------------




----------------------------------------------------------------


ELASTIC SEARCH TUTORIA:

    Elasticsearch is the distributed search and analytics engine at the heart of the Elastic Stack. Logstash and Beats facilitate collecting, aggregating, and enriching your data and storing it in Elasticsearch. Kibana enables you to interactively explore, visualize, and share insights into your data and manage and monitor the stack. Elasticsearch is where the indexing, search, and analysis magic happens.

    Elasticsearch provides near real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it.

    While not every problem is a search problem, Elasticsearch offers speed and flexibility to handle data in a wide variety of use cases:

    Add a search box to an app or website
    Store and analyze logs, metrics, and security event data
    Use machine learning to automatically model the behavior of your data in real time
    Use Elasticsearch as a vector database to create, store, and search vector embeddings
    Automate business workflows using Elasticsearch as a storage engine
    Manage, integrate, and analyze spatial information using Elasticsearch as a geographic information system (GIS)
    Store and process genetic data using Elasticsearch as a bioinformatics research tool


    Data in: documents and indices
    edit
    Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents. When you have multiple Elasticsearch nodes in a cluster, stored documents are distributed across the cluster and can be accessed immediately from any node.

    When a document is stored, it is indexed and fully searchable in near real-time--within 1 second. Elasticsearch uses a data structure called an inverted index that supports very fast full-text searches. An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.

    An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data. By default, Elasticsearch indexes all data in every field and each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees. The ability to use the per-field data structures to assemble and return search results is what makes Elasticsearch so fast.

    Elasticsearch also has the ability to be schema-less, which means that documents can be indexed without explicitly specifying how to handle each of the different fields that might occur in a document. When dynamic mapping is enabled, Elasticsearch automatically detects and adds new fields to the index. This default behavior makes it easy to index and explore your data—​just start indexing documents and Elasticsearch will detect and map booleans, floating point and integer values, dates, and strings to the appropriate Elasticsearch data types.

    Ultimately, however, you know more about your data and how you want to use it than Elasticsearch can. You can define rules to control dynamic mapping and explicitly define mappings to take full control of how fields are stored and indexed.

    Defining your own mappings enables you to:

    Distinguish between full-text string fields and exact value string fields
    Perform language-specific text analysis
    Optimize fields for partial matching
    Use custom date formats
    Use data types such as geo_point and geo_shape that cannot be automatically detected
    It’s often useful to index the same field in different ways for different purposes. For example, you might want to index a string field as both a text field for full-text search and as a keyword field for sorting or aggregating your data. Or, you might choose to use more than one language analyzer to process the contents of a string field that contains user input.

    The analysis chain that is applied to a full-text field during indexing is also used at search time. When you query a full-text field, the query text undergoes the same analysis before the terms are looked up in the index.
    
    Elasticsearch provides a simple, coherent REST API for managing your cluster and indexing and searching your data. 


    Searching your dataedit
    The Elasticsearch REST APIs support structured queries, full text queries, and complex queries that combine the two. Structured queries are similar to the types of queries you can construct in SQL. For example, you could search the gender and age fields in your employee index and sort the matches by the hire_date field. Full-text queries find all documents that match the query string and return them sorted by relevance—how good a match they are for your search terms.

    In addition to searching for individual terms, you can perform phrase searches, similarity searches, and prefix searches, and get autocomplete suggestions.

    Have geospatial or other numerical data that you want to search? Elasticsearch indexes non-textual data in optimized data structures that support high-performance geo and numerical queries.

    You can access all of these search capabilities using Elasticsearch’s comprehensive JSON-style query language (Query DSL). You can also construct SQL-style queries to search and aggregate data natively inside Elasticsearch, and JDBC and ODBC drivers enable a broad range of third-party applications to interact with Elasticsearch via SQL.

    Analyzing your dataedit
    Elasticsearch aggregations enable you to build complex summaries of your data and gain insight into key metrics, patterns, and trends. Instead of just finding the proverbial “needle in a haystack”, aggregations enable you to answer questions like:

    How many needles are in the haystack?
    What is the average length of the needles?
    What is the median length of the needles, broken down by manufacturer?
    How many needles were added to the haystack in each of the last six months?
    You can also use aggregations to answer more subtle questions, such as:

    What are your most popular needle manufacturers?
    Are there any unusual or anomalous clumps of needles?
    Because aggregations leverage the same data-structures used for search, they are also very fast. This enables you to analyze and visualize your data in real time. Your reports and dashboards update as your data changes so you can take action based on the latest information.

    What’s more, aggregations operate alongside search requests. You can search documents, filter results, and perform analytics at the same time, on the same data, in a single request. And because aggregations are calculated in the context of a particular search, you’re not just displaying a count of all size 70 needles, you’re displaying a count of the size 70 needles that match your users' search criteria—​for example, all size 70 non-stick embroidery needles.


    Anomaly detection

    But wait, there’s more
    Want to automate the analysis of your time series data? You can use machine learning features to create accurate baselines of normal behavior in your data and identify anomalous patterns. With machine learning, you can detect:

    Anomalies related to temporal deviations in values, counts, or frequencies
    Statistical rarity
    Unusual behaviors for a member of a population

    ILM: Manage the index lifecycle
    edit
    You can configure index lifecycle management (ILM) policies to automatically manage indices according to your performance, resiliency, and retention requirements. For example, you could use ILM to:

    Spin up a new index when an index reaches a certain size or number of documents
    Create a new index each day, week, or month and archive previous ones
    Delete stale indices to enforce data retention standards
    You can create and manage index lifecycle policies through Kibana Management or the ILM APIs. Default index lifecycle management policies are created automatically when you use Elastic Agent, Beats, or the Logstash Elasticsearch output plugin to send data to the Elastic Stack.


ELASTIC SEARCH SCALABILITY:

    Scalability and resilience: clusters, nodes, and shards
    edit
    Elasticsearch is built to be always available and to scale with your needs. It does this by being distributed by nature. You can add servers (nodes) to a cluster to increase capacity and Elasticsearch automatically distributes your data and query load across all of the available nodes. No need to overhaul your application, Elasticsearch knows how to balance multi-node clusters to provide scale and high availability. The more nodes, the merrier.

    How does this work? Under the covers, an Elasticsearch index is really just a logical grouping of one or more physical shards, where each shard is actually a self-contained index. By distributing the documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy, which both protects against hardware failures and increases query capacity as nodes are added to a cluster. As the cluster grows (or shrinks), Elasticsearch automatically migrates shards to rebalance the cluster.

    There are two types of shards: primaries and replicas. Each document in an index belongs to one primary shard. A replica shard is a copy of a primary shard. Replicas provide redundant copies of your data to protect against hardware failure and increase capacity to serve read requests like searching or retrieving a document.

    The number of primary shards in an index is fixed at the time that an index is created, but the number of replica shards can be changed at any time, without interrupting indexing or query operations.

    It depends…​edit
    There are a number of performance considerations and trade offs with respect to shard size and the number of primary shards configured for an index. The more shards, the more overhead there is simply in maintaining those indices. The larger the shard size, the longer it takes to move shards around when Elasticsearch needs to rebalance a cluster.

    Querying lots of small shards makes the processing per shard faster, but more queries means more overhead, so querying a smaller number of larger shards might be faster. In short…​it depends.

    As a starting point:

    Aim to keep the average shard size between a few GB and a few tens of GB. For use cases with time-based data, it is common to see shards in the 20GB to 40GB range.
    Avoid the gazillion shards problem. The number of shards a node can hold is proportional to the available heap space. As a general rule, the number of shards per GB of heap space should be less than 20.
    The best way to determine the optimal configuration for your use case is through testing with your own data and queries.

    In case of disasteredit
    A cluster’s nodes need good, reliable connections to each other. To provide better connections, you typically co-locate the nodes in the same data center or nearby data centers. However, to maintain high availability, you also need to avoid any single point of failure. In the event of a major outage in one location, servers in another location need to be able to take over. The answer? Cross-cluster replication (CCR).

    CCR provides a way to automatically synchronize indices from your primary cluster to a secondary remote cluster that can serve as a hot backup. If the primary cluster fails, the secondary cluster can take over. You can also use CCR to create secondary clusters to serve read requests in geo-proximity to your users.

    Cross-cluster replication is active-passive. The index on the primary cluster is the active leader index and handles all write requests. Indices replicated to secondary clusters are read-only followers.

    Care and feedingedit
    As with any enterprise system, you need tools to secure, manage, and monitor your Elasticsearch clusters. Security, monitoring, and administrative features that are integrated into Elasticsearch enable you to use Kibana as a control center for managing a cluster. Features like downsampling and index lifecycle management help you intelligently manage your data over time.


    





    Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.

    Here is the simple architecture of ELK stack


    Logs: Server logs that need to be analyzed are identified
    Logstash: Collect logs and events data. It even parses and transforms data
    ElasticSearch: The transformed data from Logstash is Store, Search, and indexed.
    Kibana: Kibana uses Elasticsearch DB to Explore, Visualize, and Share
    However, one more component is needed or Data collection called Beats. This led Elastic to rename ELK as the Elastic Stack

    While dealing with very large amounts of data, you may need Kafka, RabbitMQ for buffering and resilience. For security, nginx can be used.

    How Does it Work?
    Logstash has over 160 connector and transform tools to get logs from inconsistent and strange formats, and over different protocols, over the network or wherever they reside and originate over your network. Once this is done, Logstash gets these logs into a system: Elasticsearch. Elasticsearch allows you to perform real time search and analysis of your data. Kibana then provides a visualization and exploration system on top of Logstash and Elasticsearch so you can easily understand your data in terms of charts and graphs.

    Elasticsearch Plugins
    Plugin	Description
    Elasticsearch-HQ	A user interface that provides monitoring, management, and querying capabilities for Elasticsearch clusters.
    Search Guard	Offers comprehensive security features for Elasticsearch, including encryption, authentication, authorization, and audit logging.
    Dejavu	A web UI for Elasticsearch that allows you to browse, search, and manage data.

    Logstash Plugins
    Plugin	Description
    Logstash Input Plugins (e.g., file, syslog, beats, http, jdbc)	These plugins allow Logstash to read data from various sources.
    Logstash Filter Plugins (e.g., grok, mutate, date, geoip)	Used for transforming and enriching data before it is sent to its destination.
    Logstash Output Plugins (e.g., elasticsearch, email, file, http)	These plugins define how Logstash sends data to various outputs like Elasticsearch, email, files, or over HTTP.

    Kibana Plugins
    Plugin	Description
    Canvas	Allows users to create custom dynamic infographics based on Elasticsearch data.
    Timelion	A time series data visualizer that enables users to combine fully expressive, Elasticsearch-driven queries with simple syntax.
    Elastic Maps	Enables users to visualize geospatial data in Kibana with maps.



